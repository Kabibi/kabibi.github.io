<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本文为 Andrew NG 的深度学习课程四《卷积神经网络》第2周的对应学习笔记。">
<meta name="keywords" content="Andrew Ng deep learning,CNN">
<meta property="og:type" content="website">
<meta property="og:title" content="深度学习之四《卷积神经网络》笔记(2)">
<meta property="og:url" content="http://kabibi.github.io/drafts/old_posts/深度学习之四《卷积神经网络》笔记-2.html">
<meta property="og:site_name" content="∇ &gt; 0">
<meta property="og:description" content="本文为 Andrew NG 的深度学习课程四《卷积神经网络》第2周的对应学习笔记。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_LeNet.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_AlexNet.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_VGG16.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_residual_block.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_residual_network.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_residual_network_results.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_ResNets.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2_CNN_ResNet.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L5_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L5_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L5_3.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L6_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L6_3.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L6_4.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L7_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L7_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/pre_train.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C4W2L11_1.jpg">
<meta property="og:updated_time" content="2019-04-03T01:59:47.375Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习之四《卷积神经网络》笔记(2)">
<meta name="twitter:description" content="本文为 Andrew NG 的深度学习课程四《卷积神经网络》第2周的对应学习笔记。">
<meta name="twitter:image" content="http://kabibi.github.io/images/post_images/C4W2_LeNet.jpg">





  
  
  <link rel="canonical" href="http://kabibi.github.io/drafts/old_posts/深度学习之四《卷积神经网络》笔记-2">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深度学习之四《卷积神经网络》笔记(2) | ∇ > 0</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">∇ > 0</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Reinforcement Learning</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>

      
      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/kabibi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">深度学习之四《卷积神经网络》笔记(2)

</h1>

<div class="post-meta">
  
  


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>深度学习之四《卷积神经网络》笔记-2</li>
          
        
      
    
  </ul>


</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <p>本文为 Andrew NG 的深度学习课程四《卷积神经网络》第2周的对应学习笔记。</p>
<a id="more"></a>
<h1 id="Why-look-at-case-studies"><a href="#Why-look-at-case-studies" class="headerlink" title="Why look at case studies"></a>Why look at case studies</h1><p>本周课程将主要介绍几个典型的CNN案例。通过对具体CNN模型及案例的研究，来帮助我们理解知识并训练实际的模型。</p>
<p>典型的CNN模型包括：</p>
<ul>
<li>LeNet-5</li>
<li>AlexNet</li>
<li>VGG<br>除了这些性能良好的CNN模型之外，我们还会介绍 Residual Network（ResNet）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。</li>
</ul>
<p>另外，还会介绍Inception Neural Network。</p>
<h1 id="Classic-Network"><a href="#Classic-Network" class="headerlink" title="Classic Network"></a>Classic Network</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>LeNet-5模型是第一个成功应用于数字识别问题的卷积神经网络。在MNIST数据中，它的准确率达到大约99.2%。典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer，即 $$ \hat y $$。<br>下图所示的是一个数字识别的LeNet-5的模型结构：</p>
<p><img src="/images/post_images/C4W2_LeNet.jpg" alt="LeNet-5"></p>
<p>该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>AlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：</p>
<p><img src="/images/post_images/C4W2_AlexNet.jpg" alt="AlexNet"></p>
<p>AlexNet模型与LeNet-5模型类似，只是要复杂一些，总共包含了大约6千万个参数。同样可以根据实际情况使用激活函数ReLU。原作者还提到了一种优化技巧，叫做<strong>Local Response Normalization(LRN)</strong>。 而在实际应用中，LRN的效果并不突出。</p>
<h2 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h2><p>VGG-16模型更加复杂一些，一般情况下，其CONV layer和POOL layer设置如下：</p>
<ul>
<li>CONV = 3x3 filters, s = 1, same</li>
<li>MAX-POOL = 2x2, s = 2</li>
</ul>
<p>VGG-16结构如下所示:</p>
<p><img src="/images/post_images/C4W2_VGG16.jpg" alt="VGG16"></p>
<p>VGG-16的参数多达1亿3千万。</p>
<h1 id="Residual-Networks（ResNets）"><a href="#Residual-Networks（ResNets）" class="headerlink" title="Residual Networks（ResNets）"></a>Residual Networks（ResNets）</h1><h2 id="What’s-ResNets"><a href="#What’s-ResNets" class="headerlink" title="What’s ResNets?"></a>What’s ResNets?</h2><p>我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为<strong>Residual Networks(ResNets)</strong>。Residual Networks由许多隔层相连的神经元子模块组成，我们称之为Residual block。单个Residual block的结构如下图所示：</p>
<p><img src="/images/post_images/C4W2_residual_block.jpg" alt="Residual Block"></p>
<p>上图中红色部分就是skip connection，直接建立 $$ a^{[l]} $$ 与 $$ a^{[l+2]} $$ 之间的隔层联系。相应的表达式如下：</p>
<p>$$$ a^{[l+2]} = g(z^{[l+2]}+a^{[l]}) $$$</p>
<p>$a^{[l]}$直接隔层与下一层的线性输出相连，与 $$ z^{[l+2]} $$ 共同通过激活函数（ReLU）输出$$ a^{[l+2]}$$ 。</p>
<p>该模型由Kaiming He, Xiangyu Zhang, Shaoqing Ren和Jian Sun共同提出。由多个Residual block组成的神经网络就是<strong>Residual Network</strong>。实验表明，这种模型结构对于训练非常深的神经网络，效果很好。另外，为了便于区分，我们把非Residual Networks称为Plain Network。</p>
<p><img src="/images/post_images/C4W2_residual_network.jpg" alt="Residual Network"></p>
<p>Residual Network的结构如上图所示。</p>
<h2 id="Residual-Network-VS-Plain-Network"><a href="#Residual-Network-VS-Plain-Network" class="headerlink" title="Residual Network VS. Plain Network"></a>Residual Network VS. Plain Network</h2><p>与Plain Network相比，Residual Network能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸。从下面两张图的对比中可以看出，随着神经网络层数增加，Plain Network实际性能会变差，training error甚至会变大。然而，Residual Network的训练效果却很好，training error一直呈下降趋势。</p>
<p><img src="/images/post_images/C4W2_residual_network_results.jpg" alt="Residual Network"></p>
<h1 id="Why-ResNets-Works"><a href="#Why-ResNets-Works" class="headerlink" title="Why ResNets Works?"></a>Why ResNets Works?</h1><h2 id="Why-work"><a href="#Why-work" class="headerlink" title="Why work?"></a>Why work?</h2><p>下面用个例子来解释为什么ResNets能够训练更深层的神经网络。</p>
<p><img src="/images/post_images/C4W2_ResNets.jpg" alt="ResNet"></p>
<p>如上图所示，输入x经过很多层神经网络后输出 $$ a^{[l]} $$， $$a^{[l]}$$ 经过一个Residual block输出 $$a^{[l+2]} $$。$$a^{[l+2]} $$的表达式为：</p>
<p>$$a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})$$</p>
<p>输入x经过Big NN后，若 $$ W^{[l+2]}\approx0$$，$$ b^{[l+2]}\approx0$$，则有：</p>
<p>$$ a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}\ \ \ \ when\ a^{[l]}\geq0 $$</p>
<p>可以看出，<strong>即使发生了梯度消失，$$ W^{[l+2]}\approx0$$，$$ b^{[l+2]}\approx0$$，也能直接建立 $$a^{[l+2]} $$与$$a^{[l]} $$的线性关系</strong>，且$$ a^{[l+2]}=a^{[l]}$$ ，这其实就是identity function。$$ a^{[l]}$$ 直接连到 $$ a^{[l+2]}$$ ，从效果来说，相当于直接忽略了 $$a^{[l]}$$ 之后的这两层神经层。这样，<strong> 看似很深的神经网络，其实由于许多Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。</strong>而且从性能上来说，这两层额外的Residual blocks也不会降低Big NN的性能。</p>
<p>当然，如果Residual blocks确实能训练得到非线性关系，那么也会忽略short cut，跟Plain Network起到同样的效果。</p>
<h2 id="维度不同如何解决？"><a href="#维度不同如何解决？" class="headerlink" title="维度不同如何解决？"></a>维度不同如何解决？</h2><p>有一点需要注意的是，如果Residual blocks中 $$ a^{[l]}$$ 和 $$ a^{[l+2]} $$的维度不同，通常可以引入矩阵$$W_s$$，与 $$ a^{[l]}$$ 相乘，使得 $$ W_s*a^{[l]} $$ 的维度与 $$ a^{[l+2]} $$ 一致。参数矩阵 $$ W_s $$ 有来两种方法得到：</p>
<ul>
<li>一种是将 $$ W_s $$ 作为学习参数，通过模型训练得到；</li>
<li>另一种是固定 $$ W_s $$ 值（类似单位矩阵），不需要训练， $$ W_s $$ 与 $$ a^{[l]} $$ 的乘积仅仅使得 $$ a^{[l]} $$ 截断或者补零。</li>
</ul>
<p>这两种方法都可行。</p>
<p>下图所示的是CNN中ResNets的结构：</p>
<p><img src="/images/post_images/C4W2_CNN_ResNet.jpg" alt="ResNet"></p>
<p>ResNets同类型层之间，例如CONV layers，大多使用same类型，保持维度相同。如果是不同类型层之间的连接，例如CONV layer与POOL layer之间，如果维度不同，则引入矩阵 $$ W_s$$。</p>
<h1 id="Network-in-Network-and-1×1-convolutions"><a href="#Network-in-Network-and-1×1-convolutions" class="headerlink" title="Network in Network and 1×1 convolutions"></a>Network in Network and 1×1 convolutions</h1><p>Min Lin, Qiang Chen等人提出了一种新的CNN结构，即1x1 Convolutions，也称Networks in Networks。这种结构的特点是滤波器算子filter的维度为1x1。对于单个filter，1x1的维度，意味着卷积操作等同于乘积操作。</p>
<p><img src="/images/post_images/C4W2L5_1.jpg" alt="C4W2L5_1"></p>
<p>那么，对于多个filters，1x1 Convolutions的作用实际上类似全连接层的神经网络结构。效果等同于Plain Network中 $$ a^{[l]} $$ 到 $$ a^{[l+1]} $$的过程。这点还是比较好理解的。</p>
<p><img src="/images/post_images/C4W2L5_2.jpg" alt="C4W2L5_2"></p>
<p>1x1 Convolutions可以用来缩减输入图片的通道数目。方法如下图所示：</p>
<p><img src="/images/post_images/C4W2L5_3.jpg" alt="C4W2L5_3"></p>
<h1 id="Inception-Network-Motivation"><a href="#Inception-Network-Motivation" class="headerlink" title="Inception Network Motivation"></a>Inception Network Motivation</h1><h2 id="What’s-Inception-Network"><a href="#What’s-Inception-Network" class="headerlink" title="What’s Inception Network?"></a>What’s Inception Network?</h2><p>如下图所示:</p>
<p><img src="/images/post_images/C4W2L6_1.jpg" alt="C4W2L6_1"></p>
<p>我们可以使用不同尺寸的filters进行same convolutions，或者进行max-pool。只要保证输出是28×28即可, 然后将不同layer的结果堆叠起来，得到最后的结果。</p>
<ul>
<li>Convolution layer: 不同filter size的filters，比如1×1的filter(不需要padding), 或者3×3的fileter（需要padding）等等。</li>
<li>Max-Pool Layer: 我们甚至可以使用MAX-POOL layer.</li>
</ul>
<h2 id="Inception-Network-的优势"><a href="#Inception-Network-的优势" class="headerlink" title="Inception Network 的优势"></a>Inception Network 的优势</h2><p>使用Inception Network而非传统的单一尺寸和功能的filter不同，Inception Network使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块, 可以大大地提高性能。</p>
<h2 id="The-Problem-of-Computational-Cost"><a href="#The-Problem-of-Computational-Cost" class="headerlink" title="The Problem of Computational Cost"></a>The Problem of Computational Cost</h2><p>Inception Network在提升性能的同时，会带来计算量大的问题。例如下面这个例子：</p>
<p><img src="/images/post_images/C4W2L6_3.jpg" alt="C4W2L6_3"></p>
<p>此CONV layer需要的计算量为：28x28x32x5x5x192=120 million。可以看出但这一层的计算量都是很大的。为此，我们可以引入1x1 Convolutions来减少其计算量，结构如下图所示：</p>
<p><img src="/images/post_images/C4W2L6_4.jpg" alt="C4W2L6_4"></p>
<p>通常我们把该1x1 Convolution称为“瓶颈层”（bottleneck layer）。引入bottleneck layer之后，总共需要的计算量为：28x28x16x192+28x28x32x5x5x16=12.4 million。明显地，虽然多引入了1x1 Convolution层，但是总共的计算量减少了近90%，效果还是非常明显的。由此可见，1x1 Convolutions还可以有效减少CONV layer的计算量。</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总结一下：在构建神经网络的时候，不想决定Pooling layer的filter size大小，那么inception module就是最好的选择，我们可以使用各种类型的filters，只需要把输出连接起来。之后我们讲到计算代价问题，我们可以使用1×1 ConV Layer构建bottleneck layer，从而大大降低计算量，通常，只要构建合理的bottleneck layer，既可以显著缩小表示层的规模，又不会降低网络的性能。</p>
<h1 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h1><p>上一节我们使用1x1 Convolution来减少Inception Network计算量大的问题。引入1x1 Convolution后的Inception module如下图所示：</p>
<p><img src="/images/post_images/C4W2L7_1.jpg" alt="C4W2L7_1"></p>
<p>多个Inception modules组成Inception Network，效果如下图所示：</p>
<p><img src="/images/post_images/C4W2L7_2.jpg" alt="C4W2L7_2"></p>
<p>上述Inception Network除了由许多Inception modules组成之外，值得一提的是网络中间隐藏层也可以作为输出层Softmax，有利于防止发生过拟合。</p>
<h1 id="Using-Open-Source-Implementation"><a href="#Using-Open-Source-Implementation" class="headerlink" title="Using Open-Source Implementation"></a>Using Open-Source Implementation</h1><p>介绍Github的使用，略。</p>
<h1 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h1><h2 id="什么是transfer-learning？"><a href="#什么是transfer-learning？" class="headerlink" title="什么是transfer learning？"></a>什么是transfer learning？</h2><p><code>Transfer learning</code>: 深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如我们已经训练好一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学习方法被称为迁移学习（Transfer Learning）。</p>
<p>如果需要构建新模型的样本数量较少，可以只训练输出层的权重系数 $ W^{[L]} $, $ b^{[L]} $ 而保持其他层的权重参数保持不变。这种方法的优点在于做法比较简单，而且不需要很多的数据。而如果训练样本很多的话，那么也可以重新训练网络的权重系数，这样可以使得模型更加精确。采用那种方法通常根据数据量而定。</p>
<p>如果重新训练所有权重系数，初始 $ W^{[l]} $, $ b^{[l]} $ 由之前的模型训练得到，这一过程称为<code>pre-training</code>。</p>
<p><img src="/images/post_images/pre_train.jpg" alt="pre-train"></p>
<p>之后，不断调试、优化 $ W^{[l]} $,$b^{[l]}$ 的过程称为<code>fine-tuning</code>。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头。</p>
<h2 id="transfer-learning-的应用场合"><a href="#transfer-learning-的应用场合" class="headerlink" title="transfer learning 的应用场合"></a>transfer learning 的应用场合</h2><p>总的来说，transfer learning的应用场合主要为：</p>
<ul>
<li>Task A and B have the same input x.</li>
<li>You have a lot more data for Task A than Task B.</li>
<li>Low level features from A could be helpful for learning B.</li>
</ul>
<h2 id="Why-transfer-learning-works？"><a href="#Why-transfer-learning-works？" class="headerlink" title="Why transfer learning works？"></a>Why transfer learning works？</h2><p>一个从Quora上摘抄的回答：<br>You have learned to differentiate between rotten potato and fresh potato. But now with your learning experience, you have to differentiate between rotten tomato and fresh tomato. During learning of rotten potato, you have extracted knowledge like- if any vegetable is rotten, liquid will come out from there body. So, if you can use this knowledge in your job of rotten tomato identification, you are using transfer learning technique.</p>
<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p>有几种data augmentation的方法：</p>
<ul>
<li>常用的Data Augmentation方法是对已有的样本集进行<code>Mirroring</code>和<code>Random Cropping</code>。</li>
<li>另一种Data Augmentation的方法是<code>color shifting</code>。color shifting就是对图片的RGB通道数值进行随意增加或者减少，改变图片色调。</li>
<li>除了随意改变RGB通道数值外，还可以更有针对性地对图片的RGB通道进行<code>PCA color augmentation</code>，也就是对图片颜色进行主成分分析，对主要的通道颜色进行增加或减少，可以采用高斯扰动做法。这样也能增加有效的样本数量。具体的PCA color augmentation做法可以查阅AlexNet的相关论文。</li>
</ul>
<p>最后提一下，在构建大型神经网络的时候，data augmentation和training可以由两个不同的线程来进行。</p>
<h1 id="State-of-Computer-Vision"><a href="#State-of-Computer-Vision" class="headerlink" title="State of Computer Vision"></a>State of Computer Vision</h1><p>神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的hand-engineering，对已有data进行处理，比如上一节介绍的data augmentation。模型算法也会相对要复杂一些。如果data很多，可以构建深层神经网络，不需要太多的hand-engineering，模型算法也就相对简单一些。</p>
<p><img src="/images/post_images/C4W2L11_1.jpg" alt="C4W2L11_1"></p>
<p>值得一提的是hand-engineering是一项非常重要也比较困难的工作。很多时候，hand-engineering对模型训练效果影响很大，特别是在数据量不多的情况下。</p>
<p>在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：</p>
<ul>
<li><code>Ensembling</code>: Train several networks independently and average their outputs.</li>
<li><code>Multi-crop at test time</code>: Run classifier on multiple versions of test images and average results.</li>
</ul>
<p>最后，我们还要灵活使用开源代码：</p>
<ul>
<li>Use archittectures of networks published in the literature</li>
<li>Use open source implementations if possible</li>
<li>Use pretrained models and fine-tune on your dataset</li>
</ul>
<h1 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h1><p>学习具体的CNN模型有利于我们理解并训练实际的模型，我们首先介绍了三种经典的CNN模型结构，分别是：LeNet-5，AlexNet以及VGG-16。</p>
<p>接着，我们介绍了一种能够训练很深的网络的方法——<code>ResNets</code>。隔层相连的结构有利于我们训练更深的神经网络。我们需要理解：</p>
<ol>
<li>ResNets 原理是什么？</li>
<li>ResNets 为什么能够让我们训练更深的神经网络？</li>
<li>ResNets 中隔层相连，如果这两层维度不同如何解决？</li>
</ol>
<p>然后我们介绍了 <code>1×1 convolutions</code>，这种方法可以缩减输入图片的通道数以及有利于在 Inception Network 中减少计算量。</p>
<p>紧接着我们介绍了 <code>Inception Network</code>，在之前的 Conv 层，我们采用多个 filters，不过这几个 filters 的 filter size都是相同的，而在 Inception Network 中，这几个 filters 的 filter size 可以是不同的，但是要通过 padding 保证输出结果的维度是相同的。这样做可以大大地提高性能，但是引出了另一个问题：计算量过大。计算量过大可以通过之前提到的 1×1 convolutions 解决，通过引入瓶颈层，可以减少90%的计算量。</p>
<p>最后还介绍了 <code>Transfer learning</code>， <code>Data augmentation</code>以及计算机视觉目前的发展。</p>

        
      </div>
      
      
      
    </div>
    


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>深度学习之四《卷积神经网络》笔记-2</li>
          
        
      
    
  </ul>


    
    
    
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/kenan.jpeg" alt="Aaron">
            
              <p class="site-author-name" itemprop="name">Aaron</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Why-look-at-case-studies"><span class="nav-number">1.</span> <span class="nav-text">Why look at case studies</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Classic-Network"><span class="nav-number">2.</span> <span class="nav-text">Classic Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5"><span class="nav-number">2.1.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">2.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG-16"><span class="nav-number">2.3.</span> <span class="nav-text">VGG-16</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Residual-Networks（ResNets）"><span class="nav-number">3.</span> <span class="nav-text">Residual Networks（ResNets）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#What’s-ResNets"><span class="nav-number">3.1.</span> <span class="nav-text">What’s ResNets?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Residual-Network-VS-Plain-Network"><span class="nav-number">3.2.</span> <span class="nav-text">Residual Network VS. Plain Network</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Why-ResNets-Works"><span class="nav-number">4.</span> <span class="nav-text">Why ResNets Works?</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-work"><span class="nav-number">4.1.</span> <span class="nav-text">Why work?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#维度不同如何解决？"><span class="nav-number">4.2.</span> <span class="nav-text">维度不同如何解决？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-in-Network-and-1×1-convolutions"><span class="nav-number">5.</span> <span class="nav-text">Network in Network and 1×1 convolutions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Inception-Network-Motivation"><span class="nav-number">6.</span> <span class="nav-text">Inception Network Motivation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#What’s-Inception-Network"><span class="nav-number">6.1.</span> <span class="nav-text">What’s Inception Network?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception-Network-的优势"><span class="nav-number">6.2.</span> <span class="nav-text">Inception Network 的优势</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Problem-of-Computational-Cost"><span class="nav-number">6.3.</span> <span class="nav-text">The Problem of Computational Cost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary"><span class="nav-number">6.4.</span> <span class="nav-text">Summary</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Inception-Network"><span class="nav-number">7.</span> <span class="nav-text">Inception Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Using-Open-Source-Implementation"><span class="nav-number">8.</span> <span class="nav-text">Using Open-Source Implementation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Transfer-learning"><span class="nav-number">9.</span> <span class="nav-text">Transfer learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是transfer-learning？"><span class="nav-number">9.1.</span> <span class="nav-text">什么是transfer learning？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transfer-learning-的应用场合"><span class="nav-number">9.2.</span> <span class="nav-text">transfer learning 的应用场合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-transfer-learning-works？"><span class="nav-number">9.3.</span> <span class="nav-text">Why transfer learning works？</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Augmentation"><span class="nav-number">10.</span> <span class="nav-text">Data Augmentation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#State-of-Computer-Vision"><span class="nav-number">11.</span> <span class="nav-text">State of Computer Vision</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Summary-1"><span class="nav-number">12.</span> <span class="nav-text">Summary</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Aaron</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: 'fTeB9V5sfVjFhtRO94F2DNCL-gzGzoHsz',
    appKey: 'FygR9J8SjMjPb7QRF9Ld54NH',
    placeholder: '说点什么～',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: 'zh-cn' || 'zh-cn'
  });
</script>




  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
  


  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.loadBookmark();
  
  </script>


  

  

  

</body>
</html>
