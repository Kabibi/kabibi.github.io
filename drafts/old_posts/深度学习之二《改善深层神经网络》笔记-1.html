<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本文为 Andrew NG 的深度学习课程二第一周的对应学习笔记。">
<meta name="keywords" content="Andrew Ng deep learning">
<meta property="og:type" content="website">
<meta property="og:title" content="深度学习之二《改善深层神经网络》笔记(1)-训练集&#x2F;验证集&#x2F;测试集的设置、偏差和方差、正则化、归一化、梯度检查">
<meta property="og:url" content="http://kabibi.github.io/drafts/old_posts/深度学习之二《改善深层神经网络》笔记-1.html">
<meta property="og:site_name" content="∇ &gt; 0">
<meta property="og:description" content="本文为 Andrew NG 的深度学习课程二第一周的对应学习笔记。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L1_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L2_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L2_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L5_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L5_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L6_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L7_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L8_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L8_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L9_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L9_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L10_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L11_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W1L12_1.jpg">
<meta property="og:updated_time" content="2019-04-03T01:59:47.375Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习之二《改善深层神经网络》笔记(1)-训练集&#x2F;验证集&#x2F;测试集的设置、偏差和方差、正则化、归一化、梯度检查">
<meta name="twitter:description" content="本文为 Andrew NG 的深度学习课程二第一周的对应学习笔记。">
<meta name="twitter:image" content="http://kabibi.github.io/images/post_images/C2W1L1_1.jpg">





  
  
  <link rel="canonical" href="http://kabibi.github.io/drafts/old_posts/深度学习之二《改善深层神经网络》笔记-1">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深度学习之二《改善深层神经网络》笔记(1)-训练集/验证集/测试集的设置、偏差和方差、正则化、归一化、梯度检查 | ∇ > 0</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">∇ > 0</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Reinforcement Learning</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>

      
      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/kabibi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">深度学习之二《改善深层神经网络》笔记(1)-训练集/验证集/测试集的设置、偏差和方差、正则化、归一化、梯度检查

</h1>

<div class="post-meta">
  
  


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>深度学习之二《改善深层神经网络》笔记-1</li>
          
        
      
    
  </ul>


</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <p>本文为 Andrew NG 的深度学习课程二第一周的对应学习笔记。</p>
<a id="more"></a>
<h1 id="Train-Dev-Test-sets"><a href="#Train-Dev-Test-sets" class="headerlink" title="Train/Dev/Test sets"></a>Train/Dev/Test sets</h1><h2 id="选择合适的参数"><a href="#选择合适的参数" class="headerlink" title="选择合适的参数"></a>选择合适的参数</h2><p>在构建一个神经网络的时候，我们需要设置许多参数，例如神经网络的层数、每个隐藏层包含的神经元个数、学习因子（学习速率）、激活函数的选择等等。实际上很难在第一次设置的时候就选择到这些最佳的参数，而是需要通过不断地迭代更新来获得。这个<strong>循环迭代的过程</strong>是这样的：我们先有个想法Idea，先选择初始的参数值，构建神经网络模型结构；然后通过代码Code的形式，实现这个神经网络；最后，通过实验Experiment验证这些参数对应的神经网络的表现性能。根据验证结果，我们对参数进行适当的调整优化，再进行下一次的Idea-&gt;Code-&gt;Experiment循环。通过很多次的循环，不断调整参数，选定最佳的参数值，从而让神经网络性能最优化</p>
<p><img src="/images/post_images/C2W1L1_1.jpg" alt></p>
<p>深度学习已经应用于许多领域中，比如NLP，CV，Speech Recognition等等。通常来说，<strong>最适合某个领域的深度学习网络往往不能直接应用在其它问题上</strong>。解决不同问题的最佳选择是根据样本数量、输入特征数量和电脑配置信息（GPU或者CPU）等，来选择最合适的模型。即使是最有经验的深度学习专家也很难第一次就找到最合适的参数。因此，<strong>应用深度学习是一个反复迭代的过程</strong>，需要通过反复多次的循环训练得到最优化参数。决定整个训练过程快慢的关键在于单次循环所花费的时间，单次循环越快，训练过程越快。</p>
<h2 id="选择合适的-Train-Dev-Test-sets"><a href="#选择合适的-Train-Dev-Test-sets" class="headerlink" title="选择合适的 Train/Dev/Test sets"></a>选择合适的 Train/Dev/Test sets</h2><p>而<strong>设置合适的Train/Dev/Test sets</strong>，能有效提高训练效率。一般地，我们将所有的样本数据分成三个部分：Train/Dev/Test sets。</p>
<ul>
<li>Train sets用来训练你的算法模型；</li>
<li>Dev sets用来验证不同算法的表现情况，从中选择最好的算法模型；</li>
<li>Test sets用来测试最好算法的实际表现，作为该算法的无偏估计。</li>
</ul>
<p><strong>通常设置Train sets和Test sets的数量比例为70%和30%。如果有Dev sets，则设置比例为60%、20%、20%</strong>，分别对应Train/Dev/Test sets。这种比例分配在样本数量不是很大的情况下，例如100,1000,10000，是比较科学的。但是如果数据量很大的时候，例如100万，Train/Dev/Test sets的比例通常可以设置为98%/1%/1%，或者99%/0.5%/0.5%。样本数据量越大，相应的Dev/Test sets的比例应该越低一些。</p>
<p>如果<strong>没有Test sets也是没有问题的</strong>。Test sets的目标主要是进行无偏估计。我们可以通过Train sets训练不同的算法模型，然后分别在Dev sets上进行验证，根据结果选择最好的算法模型。这样也是可以的，不需要再进行无偏估计了。如果只有Train sets和Dev sets，通常也有人把这里的Dev sets称为Test sets，我们要注意加以区别。</p>
<p>现代深度学习还有个重要的问题就是<font color="red">训练样本和测试样本分布上不匹配</font>，解决这一问题的比较科学的办法是尽量保证Dev sets和Test sets来自于同一分布。值得一提的是，训练样本非常重要，通常我们可以将现有的训练样本做一些处理，例如图片的翻转、假如随机噪声等，来扩大训练样本的数量，从而让该模型更加强大。即使Train sets和Dev/Test sets不来自同一分布，使用这些技巧也能提高模型性能。</p>
<h1 id="Bias-Variance"><a href="#Bias-Variance" class="headerlink" title="Bias/Variance"></a>Bias/Variance</h1><p>偏差（Bias）和方差（Variance）是机器学习领域非常重要的两个概念和需要解决的问题。在传统的机器学习算法中，Bias和Variance是对立的，分别对应着欠拟合和过拟合，我们常常需要在Bias和Variance之间进行权衡。而在深度学习中，我们可以同时减小Bias和Variance，构建最佳神经网络模型。</p>
<p>如下图所示，显示了二维平面上，high bias，just right，high variance的例子。可见，high bias对应着欠拟合，而high variance对应着过拟合。</p>
<p><img src="/images/post_images/C2W1L2_1.jpg" alt></p>
<p>上图这个例子中输入特征是二维的，high bias和high variance可以直接从图中分类线看出来。而<font color="red">对于输入特征是高维的情况，如何来判断是否出现了high bias或者high variance呢？</font></p>
<p>例如猫识别问题，输入是一幅图像，其特征维度很大。这种情况下，我们可以通过两个数值 Train set error 和 Dev set error 来理解 bias 和 variance。</p>
<ul>
<li>假设 Train set error 为 1%，而 Dev set error 为 11%，即该算法模型对训练样本的识别很好，但是对验证集的识别却不太好。这说明了该模型对训练样本可能存在过拟合，模型泛化能力不强，导致验证集识别率低，这恰恰是high variance的表现。</li>
<li>假设 Train set error 为 15%，而 Dev set error 为 16%，虽然二者 error 接近，即该算法模型对训练样本和验证集的识别都不是太好。这说明了该模型对训练样本存在欠拟合。这恰恰是high bias的表现。</li>
<li>假设 Train set error 为 15%，而 Dev set error 为 30%，说明了该模型既存在 high bias也存在 high variance（深度学习中最坏的情况）。</li>
<li>假设 Train set error 为 0.5%，而 Dev set error 为 1%，即 low bias 和 low variance，是最好的情况。</li>
</ul>
<p>值得一提的是，<strong>以上的这些假设都是建立在base error是0的基础上，即人类都能正确识别所有猫类图片</strong>。base error不同，相应的Train set error和Dev set error会有所变化，但没有相对变化。</p>
<p><strong>一般来说，Train set error 体现了是否出现 bias，Dev set error 体现了是否出现 variance</strong>（正确地说，应该是Dev set error与Train set error的相对差值）。</p>
<p>我们已经通过二维平面展示了high bias或者high variance的模型，下图展示了high bias and high variance的模型：</p>
<center><img src="/images/post_images/C2W1L2_2.jpg" width="300"></center>

<p>模型既存在high bias也存在high variance，可以理解成某段区域是欠拟合的，某段区域是过拟合的。</p>
<p>如何避免出现 high bias 和 high variance?</p>
<table>
<thead>
<tr>
<th>减少 bias</th>
<th>减少 variance</th>
</tr>
</thead>
<tbody>
<tr>
<td>选择更复杂的NN模型(增加神经网络的隐藏层个数、神经元个数), 延长训练时间等。在 base error 不高的情况下，一般都能通过这些方式有效降低和避免 high bias，至少在训练集上表现良好。</td>
<td>增加训练样本数据，Regularization 等</td>
</tr>
</tbody>
</table>
<p>这里有几点需要注意的:</p>
<ul>
<li>第一，解决 high bias 和 high variance 的方法是不同的。实际应用中通过 Train set error 和 Dev set error 判断是否出现了 high bias 或者 high variance，然后再选择针对性的方法解决问题。</li>
<li>第二，Bias 和 Variance 的折中 tradeoff。传统机器学习算法中，Bias  和 Variance 通常是对立的，减小 Bias 会增加 Variance，减小 Variance 会增加 Bias。而在现在的深度学习中，通过使用更复杂的神经网络和海量的训练样本，一般能够同时有效减小Bias和Variance。这也是深度学习之所以如此强大的原因之一。</li>
</ul>
<h1 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h1><h2 id="L1-regularization-and-L2-regularization"><a href="#L1-regularization-and-L2-regularization" class="headerlink" title="L1 regularization and L2 regularization"></a>L1 regularization and L2 regularization</h2><p>如果出现了过拟合，即 high variance，则需要采用正则化 regularization 来解决。虽然扩大训练样本数量也是减小 high variance 的一种方法，但是通常获得更多训练样本的成本太高。所以，更可行有效的办法就是使用 regularization。</p>
<p>我们先来回顾一下之前介绍的 Logistic regression。采用 <code>L2 regularization</code>，其表达式为：</p>
<p>$$J(w,b)=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}{\Vert w \Vert}_2^2$$<br>$${\Vert w \Vert}<em>2^2=\sum</em>{j=1}^{n_x}w_j^2=w^Tw$$</p>
<p>这里有个问题：<font color="red">为什么只对 w 进行正则化而不对 b 进行正则化呢？</font>其实也可以对 b 进行正则化。但是一般 w 的维度很大，而 b 只是一个常数。相比较来说，参数很大程度上由w决定，改变b值对整体模型影响较小。所以，一般为了简便，就忽略对b的正则化了。</p>
<p>除了 L2 regularization 之外，还有另外一只正则化方法：<code>L1 regularization</code>。其表达式为：<br>$$$<br>\begin{align}<br>J(w,b)&amp;=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}{\Vert w \Vert}_1 \<br>{\Vert w \Vert}<em>1&amp;=\sum</em>{j=1}^{n_x}{\vert w_j \vert} \<br>\end{align}<br>$$$</p>
<p>与 L2 regularization 相比，<strong>L1 regularization 得到的 w 更加稀疏，即很多 w 为零值</strong>。其优点是节约存储空间，因为大部分 w 为 0。然而，实际上 L1 regularization 在解决 high variance 方面比 L2 regularization 并不更具优势。而且，<strong>L1的在微分求导方面比较复杂</strong>。所以，一般 L2 regularization 更加常用。</p>
<p>L1、L2 regularization中的 $$\lambda$$ 就是正则化参数（超参数的一种）。可以设置 $$\lambda$$ 为不同的值，在 Dev set 中进行验证，选择最佳的 $$\lambda$$ 。</p>
<p>在深度学习模型中，L2 regularization的表达式为：<br>$$$<br>\begin{align}<br>J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})&amp;=\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L{\Vert w^{[l]} \Vert}^2 \<br>{ \Vert w^{[l]} \Vert }^2 &amp;=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2  \<br>\end{align}<br>$$$</p>
<p>通常，我们把 $${ \Vert w^{[l]} \Vert}^2$$ 称为 <code>Frobenius范数</code>，记为 $${\Vert w^{[l]} \Vert}_F^2$$ 。一个矩阵的 Frobenius 范数就是计算所有元素平方和再开方，如下所示：</p>
<p>$$$<br>||A||<em>F=\sqrt {\sum</em>{i=1}^m\sum_{j=1}^n|a_{ij}|^2}<br>$$$</p>
<p>值得注意的是，由于加入了正则化项，梯度下降算法中的 $$dw^{[l]}$$ 计算表达式需要做如下修改：</p>
<p>$$$<br>dw^{[l]}=dw^{[l]}_{before}+\frac{\lambda}{m}w^{[l]} \<br>w^{[l]}:=w^{[l]}-\alpha\cdot dw^{[l]}<br>$$$</p>
<p>L2 regularization 也被称做 <code>weight decay</code>。这是因为，由于加上了正则项， $$dw^{[l]}$$ 有个增量，在更新 $$w^{[l]}$$ 的时候，会多减去这个增量，使得 $$w^{[l]}$$ 比没有正则项的值要小一些。不断迭代更新，不断地减小。<br>$$\begin{eqnarray}w^{[l]} &amp;:=&amp;w^{[l]}-\alpha\cdot dw^{[l]}\ &amp;=&amp;w^{[l]}-\alpha\cdot(dw^{[l]}<em>{before}+\frac{\lambda}{m}w^{[l]})\ &amp;=&amp;(1-\alpha\frac{\lambda}{m})w^{[l]}-\alpha\cdot dw^{[l]}</em>{before} \end{eqnarray}$$</p>
<p>其中，$$ (1-\alpha\frac{\lambda}{m})&lt;1 $$。</p>
<h2 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting"></a>Why regularization reduces overfitting</h2><p>为什么正则化能够有效避免high variance，防止过拟合呢？下面我们通过几个例子说明。还是之前那张图，从左到右，分别表示了欠拟合，刚好拟合，过拟合三种情况。</p>
<p><img src="/images/post_images/C2W1L5_1.jpg" alt></p>
<p>假如我们选择了非常复杂的神经网络模型，如上图左上角所示。在未使用正则化的情况下，我们得到的分类超平面可能是类似上图右侧的过拟合。但是，如果使用L2 regularization，<strong>当 $$\lambda$$ 很大时， $$w^{[l]}\approx0$$ 。 $$w^{[l]}$$ 近似为零，意味着该神经网络模型中的某些神经元实际的作用很小，可以忽略。从效果上来看，其实是将某些神经元给忽略掉了。这样原本过于复杂的神经网络模型就变得不那么复杂了，而变得非常简单化了</strong>。如下图所示，整个简化的神经网络模型变成了一个逻辑回归模型。问题就从high variance变成了high bias了。</p>
<center><img src="/images/post_images/C2W1L5_2.jpg" width="300"></center>

<p>因此，选择合适大小的 $$\lambda$$ 值，就能够同时避免high bias和high variance，得到最佳模型。</p>
<p>还有另外一个直观的例子来解释为什么正则化能够避免发生过拟合。假设激活函数是 tanh 函数。tanh 函数的特点是在z接近零的区域，函数近似是线性的，而当 |z| 很大的时候，函数非线性且变化缓慢。当使用正则化， $$\lambda$$ 较大，即对权重 $$w^{[l]}$$ 的惩罚较大， $$w^{[l]}$$ 减小。因为 $$z^{[l]}=w^{[l]}a^{[l]}+b^{[l]}$$ 。当 $$w^{[l]}$$ 减小的时候， $$z^{[l]}$$ 也会减小。则此时的 $$z^{[l]}$$ 分布在 tanh 函数的近似线性区域。那么这个神经元起的作用就相当于是 linear regression。<strong>如果每个神经元对应的权重 $$w^{[l]}$$ 都比较小，那么整个神经网络模型相当于是多个linear regression的组合，即可看成一个 linear network。得到的分类超平面就会比较简单，不会出现过拟合现象</strong>。</p>
<h2 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h2><p>除了 L2 regularization 之外，还有另外一种防止过拟合的有效方法：Dropout。<strong>Dropout 是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</strong></p>
<center><img src="/images/post_images/C2W1L6_1.jpg" width="300"></center>

<p>Dropout 有不同的实现方法，接下来介绍一种常用的方法：<code>Inverted dropout</code>。假设对于第 l 层神经元，设定保留神经元比例概率 keep_prob=0.8，即该层有 20% 的神经元停止工作。 dl 为 dropout 向量，设置 dl 为随机 vector，其中 80% 的元素为 1，20% 的元素为 0。在 python 中可以使用如下语句生成 dropout vector：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>],al.shape[<span class="number">1</span>]) &lt; keep_prob</span><br></pre></td></tr></table></figure>
<p>然后，第 l 层经过 dropout，随机删减 20% 的神经元，只保留 80% 的神经元，其输出为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">al = np.multiply(al,dl)</span><br></pre></td></tr></table></figure>
<p>最后，还要对 al 进行 scale up 处理，即：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">al /= keep_prob</span><br></pre></td></tr></table></figure>
<p><strong>之所以要对 al 进行 scale up, 是为了保证在经过 dropout 后， al 作为下一层神经元的输入值尽量保持不变</strong>。假设第 l 层有 50 个神经元，经过 dropout 后，有 10 个神经元停止工作，这样只有 40 神经元有作用。那么得到的 al 只相当于原来的 80%。scale up 后，能够尽可能保持 al 的期望值相比之前没有大的变化。</p>
<p>Inverted dropout 的另外一个好处就是在对该 dropout 后的神经网络进行测试时能够减少 scaling 问题。因为在训练时，使用 scale up 保证 al 的期望值没有大的变化，测试时就不需要再对样本数据进行类似的尺度伸缩操作了。</p>
<p><strong>对于 m 个样本，单次迭代训练时，随机删除掉隐藏层一定数量的神经元；然后，在删除后的剩下的神经元上正向和反向更新权重 w 和常数项 b；接着，下一次迭代中，再恢复之前删除的神经元，重新随机删除一定数量的神经元，进行正向和反向更新 w 和 b。不断重复上述过程，直至迭代训练完成。</strong></p>
<p>值得注意的是，<strong>使用 dropout 训练结束后，在测试和实际应用模型时，不需要进行 dropout 和随机删减神经元，所有的神经元都在工作</strong>。</p>
<h2 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a>Understanding Dropout</h2><p>Dropout 通过每次迭代训练时，随机选择不同的神经元，相当于每次都在不同的神经网络上进行训练，类似机器学习中 Bagging 的方法（三个臭皮匠，赛过诸葛亮），能够防止过拟合。</p>
<p>除此之外，还可以从权重 w 的角度来解释为什么 dropout 能够有效防止过拟合。对于某个神经元来说，某次训练时，它的某些输入在 dropout 的作用被过滤了。而在下一次训练时，又有不同的某些输入被过滤。经过多次训练后，某些输入被过滤，某些输入被保留。这样，该神经元就不会受某个输入非常大的影响，<strong>影响被均匀化了</strong>。也就是说，对应的权重w不会很大。这从效果上来说，与 L2 regularization 是类似的，都是对权重 w 进行“惩罚”，减小了 w 的值。</p>
<center><img src="/images/post_images/C2W1L7_1.jpg" width="400"></center>

<p>总结一下，<strong>对于同一组训练数据，利用不同的神经网络训练之后，求其输出的平均值可以减少 overfitting</strong>。Dropout 就是利用这个原理，<strong>每次丢掉一定数量的隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加健壮的特征</strong>。</p>
<p>在使用 dropout 的时候，有几点需要注意</p>
<ul>
<li>首先，<strong>不同隐藏层的 dropout 系数 keep_prob 可以不同</strong>。一般来说，神经元越多的隐藏层，keep_out 可以设置得小一些，例如 0.5；神经元越少的隐藏层，keep_out 可以设置的大一些，例如 0.8。</li>
<li>另外，实际应用中，<strong>不建议对输入层进行 dropout</strong>，如果输入层维度很大，例如图片，那么可以设置 dropout，但 keep_out 应设置的大一些，例如 0.8，0.9。</li>
<li>总体来说，就是<strong>越容易出现 overfitting 的隐藏层，其 keep_prob 就设置的相对小一些</strong>。没有准确固定的做法，通常可以根据 validation 进行选择。</li>
</ul>
<p>Dropout 在 CV 领域应用比较广泛，因为输入层维度较大，而且没有足够多的样本数量。值得注意的是 dropout 是一种 regularization 技巧，用来防止过拟合的，最好只在需要 regularization 的时候使用 dropout。</p>
<p>使用 dropout 的时候，可以通过绘制 cost function 来进行 debug，看看 dropout 是否正确执行。一般做法是，将所有层的 keep_prob 全设置为 1，再绘制 cost function，即涵盖所有神经元，看 J 是否单调下降。下一次迭代训练时，再将 keep_prob设置为其它值。</p>
<h2 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h2><p>除了 L2 regularization 和 dropout regularization 之外，还有其它减少过拟合的方法。</p>
<p>一种方法是<strong>增加训练样本数量</strong>。但是通常成本较高，难以获得额外的训练样本。但是，<strong>我们可以对已有的训练样本进行一些处理来“制造”出更多的样本</strong>，称为 <code>data augmentation</code>。例如图片识别问题中，可以对已有的图片进行水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。如下图所示，这些处理都能“制造”出新的训练样本。虽然这些是基于原有样本的，但是对增大训练样本数量还是有很有帮助的，不需要增加额外成本，却能起到防止过拟合的效果。</p>
<p><img src="/images/post_images/C2W1L8_1.jpg" alt></p>
<p>在数字识别中，也可以将原有的数字图片进行任意旋转或者扭曲，或者增加一些 noise，如下图所示：</p>
<center><img src="/images/post_images/C2W1L8_2.jpg" width="300"></center>

<p><strong>还有另外一种防止过拟合的方法：<code>early stopping</code></strong>。一个神经网络模型随着迭代训练次数增加，train set error 一般是单调减小的，而 dev set error 先减小，之后又增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，<strong>迭代训练次数不是越多越好</strong>，可以通过 train set error 和 dev set error 随着迭代次数的变化趋势，选择合适的迭代次数，即 early stopping。</p>
<p>然而，Early stopping有其自身缺点。通常来说，机器学习训练模型有两个目标：一是优化cost function，尽量减小J；二是防止过拟合。这两个目标彼此对立的，即减小 J 的同时可能会造成过拟合，反之亦然。我们把这二者之间的关系称为正交化(orthogonalization)。在深度学习中，我们可以同时减小 Bias 和 Variance，构建最佳神经网络模型。但是，<strong>early stopping 的做法通过减少迭代训练次数来防止过拟合，这样 J 就不会足够小</strong>。也就是说，early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</p>
<p>与 early stopping 相比，<strong>L2 regularization 可以实现“分而治之”的效果：迭代训练足够多，减小 J，而且也能有效防止过拟合</strong>。而 L2 regularization 的缺点之一是最优的正则化参数 $$\lambda$$ 的选择比较复杂。对这一点来说，early stopping 比较简单。总的来说，L2 regularization 更加常用一些。</p>
<h1 id="Normalizing-inputs"><a href="#Normalizing-inputs" class="headerlink" title="Normalizing inputs"></a>Normalizing inputs</h1><p>在训练神经网络时，标准化输入可以提高训练的速度。<strong>标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值 $$\mu$$ 后，再除以其方差 $$\sigma^2$$</strong>：<br>$$$<br>\begin{align}<br>\mu&amp;=\frac1m\sum_{i=1}^mX^{(i)}  \<br>\sigma^2&amp;=\frac1m\sum_{i=1}^m(X^{(i)})^2    \<br>X&amp;:=\frac{X-\mu}{\sigma^2}   \<br>\end{align}<br>$$$</p>
<p>以二维平面为例，下图展示了其归一化过程：</p>
<p><img src="/images/post_images/C2W1L9_1.jpg" alt></p>
<p>值得注意的是，<strong>由于训练集进行了标准化处理，那么对于测试集或在实际应用时，应该使用同样的 $$\mu$$ 和 $$\sigma^2$$ 对其进行标准化处理</strong>。这样保证了训练集和测试集的标准化操作一致。 </p>
<p>之所以要对输入进行标准化操作，主要是为了<strong>让所有输入归一化同样的尺度上，方便进行梯度下降算法时能够更快更准确地找到全局最优解</strong>。假如输入特征是二维的，且 x1 的范围是[1,1000]，x2 的范围是[0,1]。如果不进行标准化处理，x1 与 x2 之间分布极不平衡，训练得到的 w1 和 w2 也会在数量级上差别很大。这样导致的结果是 cost function 与 w 和 b 的关系可能是一个非常细长的椭圆形碗。对其进行梯度下降算法时，由于 w1 和 w2 数值差异很大，只能选择很小的学习因子 $$\alpha$$ ，来避免J发生振荡。一旦 $$\alpha$$ 较大，必然发生振荡，J 不再单调下降。如下左图所示。</p>
<p>然而，如果进行了标准化操作，x1 与 x2 分布均匀，w1 和 w2 数值差别不大，得到的 cost function 与 w 和 b 的关系是类似圆形碗。对其进行梯度下降算法时， $$\alpha$$ 可以选择相对大一些，且 J 一般不会发生振荡，保证了 J 是单调下降的。如下右图所示。</p>
<p><img src="/images/post_images/C2W1L9_2.jpg" alt></p>
<p>另外一种情况，如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的。但是，标准化处理在大多数场合下还是值得推荐的。</p>
<h1 id="Vanishing-and-Exploding-gradients"><a href="#Vanishing-and-Exploding-gradients" class="headerlink" title="Vanishing and Exploding gradients"></a>Vanishing and Exploding gradients</h1><p>在神经网络尤其是深度神经网络中存在可能存在这样一个问题：梯度消失和梯度爆炸。意思是<strong>当训练一个层数非常多的神经网络时，计算得到的梯度可能非常小或非常大，甚至是指数级别的减小或增大</strong>。这样会让训练过程变得非常困难。</p>
<p>举个例子来说明，假设一个多层的每层只包含两个神经元的深度神经网络模型，如下图所示：</p>
<p><img src="/images/post_images/C2W1L10_1.jpg" alt></p>
<p>为了简化复杂度，便于分析，我们令各层的激活函数为线性函数，即 g(Z)=Z 。且忽略各层常数项 b 的影响，令 b 全部为零。那么，该网络的预测输出 $$\hat Y$$ 为：<br>$$ \hat Y=W^{[L]}W^{[L-1]}W^{[L-2]}\cdots W^{[3]}W^{[2]}W^{[1]}X $$</p>
<p>如果各层权重 $$W^{[l]}$$ 的元素都稍大于 1，例如 1.5，则预测输出 $$\hat Y$$ 将正比于 $$1.5^L$$ 。L越大， $$\hat Y$$ 越大，且呈指数型增长。我们称之为数值爆炸。相反，如果各层权重 $$W^{[l]}$$ 的元素都稍小于1，例如0.5，则预测输出 $$\hat Y$$ 将正比于 $$0.5^L$$ 。网络层数L越多， $$\hat Y$$ 呈指数型减小。我们称之为数值消失。</p>
<p>也就是说，如果各层权重 $$W^{[l]}$$ 都大于 1 或者都小于 1，那么各层激活函数的输出将随着层数 l 的增加，呈指数型增大或减小。当层数很大时，出现数值爆炸或消失。同样，这种情况也会引起梯度呈现同样的指数型增大或减小的变化。L 非常大时，例如 L=150，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，这让训练过程十分困难。</p>
<h1 id="Weight-Initialization-for-Deep-Networks"><a href="#Weight-Initialization-for-Deep-Networks" class="headerlink" title="Weight Initialization for Deep Networks"></a>Weight Initialization for Deep Networks</h1><p>下面介绍如何改善 Vanishing and Exploding gradients 这类问题，方法是对权重 w 进行一些初始化处理。</p>
<p>深度神经网络模型中，以单个神经元为例，该层 (l) 的输入个数为 n，其输出为：<br>$$$<br>\begin{align}<br>z&amp;=w_1x_1+w_2x_2+\cdots+w_nx_n   \<br>a&amp;=g(z)  \<br>\end{align}<br>$$$</p>
<center><img src="/images/post_images/C2W1L11_1.jpg" width="300"></center>

<p>这里忽略了常数项 b。为了让 z 不会过大或者过小，思路是<strong>让 w 与  n 有关，且 n 越大，w 应该越小才好</strong>。这样能够保证 z 不会过大。一种方法是在初始化 w 时，令其方差为 $$\frac1n$$ 。相应的 python 伪代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">1</span>/n[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<p>如果激活函数是 tanh，一般选择上面的初始化方法。如果激活函数是 ReLU，w 的初始化一般令其方差为 $$\frac2n$$ ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/n[l<span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<p>除此之外，Yoshua Bengio提出了另外一种初始化 w 的方法，令其方差为 $$\frac{2}{n^{[l-1]}n^{[l]}}$$ ：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/n[l<span class="number">-1</span>]*n[l])</span><br></pre></td></tr></table></figure>
<p>至于选择哪种初始化方法因人而异，可以根据不同的激活函数选择不同方法。另外，我们可以对这些初始化方法中设置某些参数，作为超参数，通过验证集进行验证，得到最优参数，来优化神经网络。</p>
<h1 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h1><p>Back Propagation神经网络有一项重要的测试是梯度检查（gradient checking），其目的是<strong>检查验证反向传播过程中梯度下降算法是否正确</strong>。先介绍如何近似求出梯度值。</p>
<center><img src="/images/post_images/C2W1L12_1.jpg" width="300"></center>

<p>利用微分思想，函数 f 在点 $$\theta$$ 处的梯度可以表示成：<br>$$ g(\theta)=\frac{f(\theta+\varepsilon)-f(\theta-\varepsilon)}{2\varepsilon} $$</p>
<p>其中， $$\varepsilon&gt;0$$ ，且足够小。</p>
<p>介绍完如何近似求出梯度值后，我们将介绍如何进行梯度检查，来验证训练过程中是否出现 bug。</p>
<ul>
<li>梯度检查首先要做的是分别将 $$ W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]}$$ 这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量 $$\theta$$ 。这样 cost function $$J(W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[L]})$$ 就可以表示成 $$J(\theta)$$ 。</li>
<li>然后将反向传播过程通过梯度下降算法得到的 $$ dW^{[1]},db^{[1]},\cdots,dW^{[L]},db^{[L]}$$ 按照一样的顺序构造成一个一维向量 $$d\theta$$ 。 $$d\theta$$ 的维度与 $$\theta$$ 一致。</li>
<li>接着利用 $$J(\theta)$$ 对每个 $$\theta_i$$ 计算近似梯度，其值与反向传播算法得到的 $$d\theta_i$$ 相比较，检查是否一致。例如，对于第i个元素，近似梯度为：<br>$$ d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,\cdots,\theta_i+\varepsilon,\cdots)-J(\theta_1,\theta_2,\cdots,\theta_i-\varepsilon,\cdots)}{2\varepsilon}$$</li>
<li>计算完所有 $$\theta_i$$ 的近似梯度后，可以计算 $$d\theta_{approx}$$ 与 $$d\theta$$ 的欧氏（Euclidean）距离来比较二者的相似度。公式如下：<br>$$ \frac{||d\theta_{approx}-d\theta||<em>2}{||d\theta</em>{approx}||_2+||d\theta||_2} $$</li>
</ul>
<p>一般来说，</p>
<ul>
<li>如果欧氏距离越小，例如 $$10^{-7}$$ ，甚至更小，则表明 $$d\theta_{approx}$$ 与 $$d\theta$$ 越接近，即反向梯度计算是正确的，没有 bug。</li>
<li>如果欧氏距离较大，例如 $$10^{-5}$$ ，则表明梯度计算可能出现问题，需要再次检查是否有 bug 存在。</li>
<li>如果欧氏距离很大，例如 $$10^{-3}$$ ，甚至更大，则表明 $$d\theta_{approx}$$ 与 $$d\theta$$ 差别很大，梯度下降计算过程有 bug，需要仔细检查。</li>
</ul>
<p>在进行梯度检查的过程中有几点需要注意的地方：</p>
<ul>
<li>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</li>
<li>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</li>
<li>注意不要忽略正则化项，计算近似梯度的时候要包括进去。</li>
<li>梯度检查时关闭dropout，检查完毕后再打开dropout。</li>
<li>随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。</li>
</ul>

        
      </div>
      
      
      
    </div>
    


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>深度学习之二《改善深层神经网络》笔记-1</li>
          
        
      
    
  </ul>


    
    
    
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/kenan.jpeg" alt="Aaron">
            
              <p class="site-author-name" itemprop="name">Aaron</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Train-Dev-Test-sets"><span class="nav-number">1.</span> <span class="nav-text">Train/Dev/Test sets</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#选择合适的参数"><span class="nav-number">1.1.</span> <span class="nav-text">选择合适的参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#选择合适的-Train-Dev-Test-sets"><span class="nav-number">1.2.</span> <span class="nav-text">选择合适的 Train/Dev/Test sets</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bias-Variance"><span class="nav-number">2.</span> <span class="nav-text">Bias/Variance</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Regularization"><span class="nav-number">3.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#L1-regularization-and-L2-regularization"><span class="nav-number">3.1.</span> <span class="nav-text">L1 regularization and L2 regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-regularization-reduces-overfitting"><span class="nav-number">3.2.</span> <span class="nav-text">Why regularization reduces overfitting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout-Regularization"><span class="nav-number">3.3.</span> <span class="nav-text">Dropout Regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Understanding-Dropout"><span class="nav-number">3.4.</span> <span class="nav-text">Understanding Dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Other-regularization-methods"><span class="nav-number">3.5.</span> <span class="nav-text">Other regularization methods</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Normalizing-inputs"><span class="nav-number">4.</span> <span class="nav-text">Normalizing inputs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vanishing-and-Exploding-gradients"><span class="nav-number">5.</span> <span class="nav-text">Vanishing and Exploding gradients</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Weight-Initialization-for-Deep-Networks"><span class="nav-number">6.</span> <span class="nav-text">Weight Initialization for Deep Networks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gradient-Checking"><span class="nav-number">7.</span> <span class="nav-text">Gradient Checking</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Aaron</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: 'fTeB9V5sfVjFhtRO94F2DNCL-gzGzoHsz',
    appKey: 'FygR9J8SjMjPb7QRF9Ld54NH',
    placeholder: '说点什么～',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: 'zh-cn' || 'zh-cn'
  });
</script>




  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
  


  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.loadBookmark();
  
  </script>


  

  

  

</body>
</html>
