<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="This is personal learning note for David Silver Lecture 5 - Model-free Control.">
<meta name="keywords" content="David Silver reinforcement learning,reinforcement learning">
<meta property="og:type" content="website">
<meta property="og:title" content="强化学习(5)Model-Free Control">
<meta property="og:url" content="http://kabibi.github.io/drafts/old_posts/强化学习-5-Model-Free-Control.html">
<meta property="og:site_name" content="∇ &gt; 0">
<meta property="og:description" content="This is personal learning note for David Silver Lecture 5 - Model-free Control.">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/RL_lecture5_first_visit_MC_control.png">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/RL_lecture5_off_policy_mc_control.png">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/RL_lecture5_saras.png">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/RL_lecture5_sarsa_algor.png">
<meta property="og:updated_time" content="2019-04-03T01:59:47.375Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习(5)Model-Free Control">
<meta name="twitter:description" content="This is personal learning note for David Silver Lecture 5 - Model-free Control.">
<meta name="twitter:image" content="http://kabibi.github.io/images/post_images/RL_lecture5_first_visit_MC_control.png">





  
  
  <link rel="canonical" href="http://kabibi.github.io/drafts/old_posts/强化学习-5-Model-Free-Control">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>强化学习(5)Model-Free Control | ∇ > 0</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">∇ > 0</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>

      
      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/kabibi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">强化学习(5)Model-Free Control

</h1>

<div class="post-meta">
  
  


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>强化学习-5-MODEL-FREE-CONTROL</li>
          
        
      
    
  </ul>


</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <p>This is personal learning note for <a href="https://www.bilibili.com/video/av9831889/" target="_blank" rel="noopener">David Silver Lecture 5 - Model-free Control</a>.</p>
<a id="more"></a>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>In last lecture, we introduced model-free prediction, which estimate the value function of an unknown MDP. In this lecture, we talk about model-free control, which tries to optimise the value function of an unknown MDP.</p>
<p>Some example problems that can be modelled as MDPs, e.g. Elevator、Robocup Soccer、Parallel Parking、Quake、Ship Steering(船舶操作)、Portfolio management(证券管理)、Bioreactor、Protein Folding、Helicopter、Robot walking、Aeroplane Logistics(航空物流)、Game of Go</p>
<p>For most of these problems, either:</p>
<ul>
<li>MDP model is unknown, but experience can be sampled</li>
<li>MDP model is known, but is too big to use, except by samples</li>
</ul>
<p>Model-free control can solve these problems.</p>
<table>
<thead>
<tr>
<th>On-policy learning</th>
<th>Off-policy learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>“Learn on the job”</td>
<td>“Look over someone’s shoulder”</td>
</tr>
<tr>
<td>learn about policy $\pi$ from experience sampled from $\pi$</td>
<td>Learn about policy $\pi$ from experience sampled from $\mu$</td>
</tr>
</tbody>
</table>
<h1 id="On-Policy-Monte-Carlo-Control"><a href="#On-Policy-Monte-Carlo-Control" class="headerlink" title="On-Policy Monte-Carlo Control"></a>On-Policy Monte-Carlo Control</h1><h2 id="Model-Free-Policy-Iteration-Using-Action-Value-Function"><a href="#Model-Free-Policy-Iteration-Using-Action-Value-Function" class="headerlink" title="Model-Free Policy Iteration Using Action-Value Function"></a>Model-Free Policy Iteration Using Action-Value Function</h2><p>Greedy policy improvement over $V(s)$ requires model of MDP because we need to know $$R_s^a$$ and $$P_{ss’}^a$$. That’s why we use $$Q(s,a)$$ instead of $$V(s)$$.<br>$$ \pi’(s) = \underset{a \in A}{argmax}\ R_s^a + P_{ss’}^a V(s’) $$</p>
<p>Greedy policy improvement over $$Q(s, a)$$ is model-free<br>$$ \pi’(s) = \underset{a \in A}{argmax}\ Q(s,a) $$</p>
<h3 id="epsilon-Greedy-Exploration"><a href="#epsilon-Greedy-Exploration" class="headerlink" title="$\epsilon$-Greedy Exploration"></a>$\epsilon$-Greedy Exploration</h3><p>If our policy is deterministic policy, the action we takes given the current state is determined. In order to ensure continual exploration, all m actions are tried with non-zero probability. With probability 1 − $\epsilon$ we choose the greedy action. With probability $\epsilon$ choose an action at random.<br>$$$<br>\begin{align}<br>\pi(a|s) = \left {<br>\begin{array}{lr}<br>\frac{\epsilon}{m}+1-\epsilon \quad &amp;if\ a^* = \underset{a \in A}{argmax}\ Q(s,a) \<br>\frac{\epsilon}{m} \quad &amp;otherwise<br>\end{array}<br>\right.<br>\end{align}<br>$$$</p>
<h2 id="GLIE-Monte-Carlo-Control"><a href="#GLIE-Monte-Carlo-Control" class="headerlink" title="GLIE Monte-Carlo Control"></a>GLIE Monte-Carlo Control</h2><blockquote>
<font color="red">[Definition - GLIE]</font>
</blockquote>
<blockquote>
<p>All state-action pairs are explored infinitely many times,<br>$$\underset{k \rightarrow \infty}{lim} N_k(s,a) = \infty $$</p>
</blockquote>
<blockquote>
<p>The policy converges on a greedy policy,<br>$$ \underset{k \rightarrow \infty}{lim} \pi_k(a|s) = {\bf 1}(a = \underset{a’ \in A}{argmax}\ Q_k(s,a’))$$</p>
</blockquote>
<blockquote>
<p>For example, $\epsilon$-greedy is GLIE if $\epsilon$ reduces to zero at $$\epsilon_k = \frac{1}{k}$$</p>
</blockquote>
<p>We sample k-th episode using $$\pi: {S_1, A_1 ,R_2, …, S_T} ∼ \pi$$. For each state $$S_t$$ and action $$A_t$$ in the episode,<br>$$$<br>\begin{align}<br>N(S_t, A_t) &amp;\leftarrow N(S_t, A_t) + 1  \<br>Q(S_t, A_t) &amp;\leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}(G_t - Q(S_t, A_t)) \<br>\end{align}<br>$$$</p>
<p>GLIE Monte-Carlo control converges to the optimal action-value function, $$Q(s, a) \rightarrow q∗(s, a)$$.</p>
<h2 id="Algorithm-and-Implementation"><a href="#Algorithm-and-Implementation" class="headerlink" title="Algorithm and Implementation"></a>Algorithm and Implementation</h2><p><img src="/images/post_images/RL_lecture5_first_visit_MC_control.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="string">"../"</span> <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</span><br><span class="line">    sys.path.append(<span class="string">"../"</span>)</span><br><span class="line"><span class="keyword">from</span> lib.envs.blackjack <span class="keyword">import</span> BlackjackEnv</span><br><span class="line"><span class="keyword">from</span> lib <span class="keyword">import</span> plotting</span><br><span class="line"></span><br><span class="line">matplotlib.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"></span><br><span class="line">env = BlackjackEnv()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_epsilon_greedy_policy</span><span class="params">(Q, epsilon, nA)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates an epsilon-greedy policy based on a given Q-function and epsilon.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Q: A dictionary that maps from state -&gt; action-values.</span></span><br><span class="line"><span class="string">            Each value is a numpy array of length nA (see below)</span></span><br><span class="line"><span class="string">        epsilon: The probability to select a random action . float between 0 and 1.</span></span><br><span class="line"><span class="string">        nA: Number of actions in the environment.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A function that takes the observation as an argument and returns</span></span><br><span class="line"><span class="string">        the probabilities for each action in the form of a numpy array of length nA.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></span><br><span class="line">        A = np.ones(nA) * epsilon / nA</span><br><span class="line">        best_action = np.argmax(Q[observation])</span><br><span class="line">        A[best_action] += <span class="number">1</span> - epsilon</span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_control_epsilon_greedy_me</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Monte Carlo Control using Epsilon-Greedy policies.</span></span><br><span class="line"><span class="string">    Finds an optimal epsilon-greedy policy.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        env: OpenAI gym environment.</span></span><br><span class="line"><span class="string">        num_episodes: Number of episodes to sample.</span></span><br><span class="line"><span class="string">        discount_factor: Gamma discount factor.</span></span><br><span class="line"><span class="string">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple (Q, policy).</span></span><br><span class="line"><span class="string">        Q is a dictionary mapping state -&gt; action values.</span></span><br><span class="line"><span class="string">        policy is a function that takes an observation as an argument and returns</span></span><br><span class="line"><span class="string">        action probabilities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Keeps track of sum and count of returns for each state</span></span><br><span class="line">    <span class="comment"># to calculate an average. We could use an array to save all</span></span><br><span class="line">    <span class="comment"># returns (like in the book) but that's memory inefficient.</span></span><br><span class="line">    returns_sum = defaultdict(float)</span><br><span class="line">    returns_count = defaultdict(float)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The final action-value function.</span></span><br><span class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></span><br><span class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The policy we're following</span></span><br><span class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line"></span><br><span class="line">        episode = []</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="comment"># 产生一条轨迹</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">            probs = policy(state)</span><br><span class="line">            action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            episode.append((state, action, reward))</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = next_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 Q[state][action]</span></span><br><span class="line">        sa_in_episode = set((x[<span class="number">0</span>], x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode)</span><br><span class="line">        <span class="keyword">for</span> state, action <span class="keyword">in</span> sa_in_episode:</span><br><span class="line">            first_occur_idx = next(i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(sa_in_episode) <span class="keyword">if</span> x[<span class="number">0</span>] == state <span class="keyword">and</span> x[<span class="number">1</span>] == action)</span><br><span class="line">            G = sum(x[<span class="number">2</span>] * (discount_factor ** i) <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(episode[first_occur_idx:]))</span><br><span class="line">            returns_sum[(state, action)] += G</span><br><span class="line">            returns_count[(state, action)] += <span class="number">1.0</span></span><br><span class="line">            Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Q, policy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_control_epsilon_greedy</span><span class="params">(env, num_episodes, discount_factor=<span class="number">1.0</span>, epsilon=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Monte Carlo Control using Epsilon-Greedy policies.</span></span><br><span class="line"><span class="string">    Finds an optimal epsilon-greedy policy.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        env: OpenAI gym environment.</span></span><br><span class="line"><span class="string">        num_episodes: Number of episodes to sample.</span></span><br><span class="line"><span class="string">        discount_factor: Gamma discount factor.</span></span><br><span class="line"><span class="string">        epsilon: Chance the sample a random action. Float betwen 0 and 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple (Q, policy).</span></span><br><span class="line"><span class="string">        Q is a dictionary mapping state -&gt; action values.</span></span><br><span class="line"><span class="string">        policy is a function that takes an observation as an argument and returns</span></span><br><span class="line"><span class="string">        action probabilities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Keeps track of sum and count of returns for each state</span></span><br><span class="line">    <span class="comment"># to calculate an average. We could use an array to save all</span></span><br><span class="line">    <span class="comment"># returns (like in the book) but that's memory inefficient.</span></span><br><span class="line">    returns_sum = defaultdict(float)</span><br><span class="line">    returns_count = defaultdict(float)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The final action-value function.</span></span><br><span class="line">    <span class="comment"># A nested dictionary that maps state -&gt; (action -&gt; action-value).</span></span><br><span class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The policy we're following</span></span><br><span class="line">    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate an episode.</span></span><br><span class="line">        <span class="comment"># An episode is an array of (state, action, reward) tuples</span></span><br><span class="line">        episode = []</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            probs = policy(state)</span><br><span class="line">            action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            episode.append((state, action, reward))</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = next_state</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Find all (state, action) pairs we've visited in this episode</span></span><br><span class="line">        <span class="comment"># We convert each state to a tuple so that we can use it as a dict key</span></span><br><span class="line">        sa_in_episode = set([(tuple(x[<span class="number">0</span>]), x[<span class="number">1</span>]) <span class="keyword">for</span> x <span class="keyword">in</span> episode])</span><br><span class="line">        <span class="keyword">for</span> state, action <span class="keyword">in</span> sa_in_episode:</span><br><span class="line">            sa_pair = (state, action)</span><br><span class="line">            <span class="comment"># Find the first occurance of the (state, action) pair in the episode</span></span><br><span class="line">            first_occurence_idx = next(i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(episode)</span><br><span class="line">                                       <span class="keyword">if</span> x[<span class="number">0</span>] == state <span class="keyword">and</span> x[<span class="number">1</span>] == action)</span><br><span class="line">            <span class="comment"># Sum up all rewards since the first occurance</span></span><br><span class="line">            G = sum([x[<span class="number">2</span>] * (discount_factor ** i) <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(episode[first_occurence_idx:])])</span><br><span class="line">            <span class="comment"># Calculate average return for this state over all sampled episodes</span></span><br><span class="line">            returns_sum[sa_pair] += G</span><br><span class="line">            returns_count[sa_pair] += <span class="number">1.0</span></span><br><span class="line">            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># The policy is improved implicitly by changing the Q dictionary</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Q, policy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Q, policy = mc_control_epsilon_greedy(env, num_episodes=<span class="number">50000</span>, epsilon=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For plotting: Create value function from action-value function</span></span><br><span class="line"><span class="comment"># by picking the best action at each state</span></span><br><span class="line">V = defaultdict(float)</span><br><span class="line"><span class="keyword">for</span> state, actions <span class="keyword">in</span> Q.items():</span><br><span class="line">    action_value = np.max(actions)</span><br><span class="line">    V[state] = action_value <span class="comment"># optimal value function</span></span><br><span class="line">plotting.plot_value_function(V, title=<span class="string">"Optimal Value Function"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Off-Policy-Monte-Carlo-Control-using-weighted-importance-sampling"><a href="#Off-Policy-Monte-Carlo-Control-using-weighted-importance-sampling" class="headerlink" title="Off-Policy Monte-Carlo Control using weighted importance sampling"></a>Off-Policy Monte-Carlo Control using weighted importance sampling</h1><p>First we introduce what is weighted importance sampling.</p>
<p>Suppose we have a sequence of returns $$G_1 , G_2, …, G_{n−1}$$, all starting in the same state and each with a corresponding random weight $$W_i$$. We wish to form the estimate<br>$$ V_n = \frac{\sum_{k=1}^{n-1} W_k G_k }{\sum_{k=1}^{n=1} W_k}, \quad n \geq 2$$</p>
<p>and keep it up-to-date as we obtain a single additional return $$G_n$$. In addition to keeping track of $$V_n$$, we must maintain for each state the cumulative sum $$C_n$$ of the weights given to the first n returns. The update rule for $$V_n$$ is<br>$$$<br>V_{n+1} = V_n + \frac{W_n}{C_n}[G_n - V_n], \quad n \geq 1<br>$$$</p>
<p>and $$C_{n+1} = C_n + W_{n+1}$$, where $$C_n=0$$.</p>
<p>Here is Off-policy MC control for estimating $$\pi \approx \pi_*$$,</p>
<p><img src="/images/post_images/RL_lecture5_off_policy_mc_control.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">if</span> <span class="string">"../"</span> <span class="keyword">not</span> <span class="keyword">in</span> sys.path:</span><br><span class="line">  sys.path.append(<span class="string">"../"</span>) </span><br><span class="line"><span class="keyword">from</span> lib.envs.blackjack <span class="keyword">import</span> BlackjackEnv</span><br><span class="line"><span class="keyword">from</span> lib <span class="keyword">import</span> plotting</span><br><span class="line"></span><br><span class="line">matplotlib.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">env = BlackjackEnv()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_random_policy</span><span class="params">(nA)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a random policy function.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        nA: Number of actions in the environment.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A function that takes an observation as input and returns a vector</span></span><br><span class="line"><span class="string">        of action probabilities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A = np.ones(nA, dtype=float) / nA</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(observation)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_greedy_policy</span><span class="params">(Q)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a greedy policy based on Q values.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Q: A dictionary that maps from state -&gt; action values</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A function that takes an observation as input and returns a vector</span></span><br><span class="line"><span class="string">        of action probabilities.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">policy_fn</span><span class="params">(state)</span>:</span></span><br><span class="line">        A = np.zeros_like(Q[state], dtype=float)</span><br><span class="line">        best_action = np.argmax(Q[state])</span><br><span class="line">        A[best_action] = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> A</span><br><span class="line">    <span class="keyword">return</span> policy_fn</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc_control_importance_sampling</span><span class="params">(env, num_episodes, behavior_policy, discount_factor=<span class="number">1.0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Monte Carlo Control Off-Policy Control using Weighted Importance Sampling.</span></span><br><span class="line"><span class="string">    Finds an optimal greedy policy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        env: OpenAI gym environment.</span></span><br><span class="line"><span class="string">        num_episodes: Number of episodes to sample.</span></span><br><span class="line"><span class="string">        behavior_policy: The behavior to follow while generating episodes.</span></span><br><span class="line"><span class="string">            A function that given an observation returns a vector of probabilities for each action.</span></span><br><span class="line"><span class="string">        discount_factor: Gamma discount factor.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        A tuple (Q, policy).</span></span><br><span class="line"><span class="string">        Q is a dictionary mapping state -&gt; action values.</span></span><br><span class="line"><span class="string">        policy is a function that takes an observation as an argument and returns</span></span><br><span class="line"><span class="string">        action probabilities. This is the optimal greedy policy.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># The final action-value function.</span></span><br><span class="line">    <span class="comment"># A dictionary that maps state -&gt; action values</span></span><br><span class="line">    Q = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    <span class="comment"># The cumulative denominator of the weighted importance sampling formula</span></span><br><span class="line">    <span class="comment"># (across all episodes)</span></span><br><span class="line">    C = defaultdict(<span class="keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Our greedily policy we want to learn</span></span><br><span class="line">    target_policy = create_greedy_policy(Q)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">1</span>, num_episodes + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># Print out which episode we're on, useful for debugging.</span></span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"\rEpisode &#123;&#125;/&#123;&#125;."</span>.format(i_episode, num_episodes), end=<span class="string">""</span>)</span><br><span class="line">            sys.stdout.flush()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Generate an episode.</span></span><br><span class="line">        <span class="comment"># An episode is an array of (state, action, reward) tuples</span></span><br><span class="line">        episode = []</span><br><span class="line">        state = env.reset()</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">            <span class="comment"># Sample an action from our policy</span></span><br><span class="line">            probs = behavior_policy(state)</span><br><span class="line">            action = np.random.choice(np.arange(len(probs)), p=probs)</span><br><span class="line">            next_state, reward, done, _ = env.step(action)</span><br><span class="line">            episode.append((state, action, reward))</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            state = next_state</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Sum of discounted returns</span></span><br><span class="line">        G = <span class="number">0.0</span></span><br><span class="line">        <span class="comment"># The importance sampling ratio (the weights of the returns)</span></span><br><span class="line">        W = <span class="number">1.0</span></span><br><span class="line">        <span class="comment"># For each step in the episode, backwards</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(len(episode))[::<span class="number">-1</span>]:</span><br><span class="line">            state, action, reward = episode[t]</span><br><span class="line">            <span class="comment"># Update the total reward since step t</span></span><br><span class="line">            G = discount_factor * G + reward</span><br><span class="line">            <span class="comment"># Update weighted importance sampling formula denominator</span></span><br><span class="line">            C[state][action] += W</span><br><span class="line">            <span class="comment"># Update the action-value function using the incremental update formula (5.7)</span></span><br><span class="line">            <span class="comment"># This also improves our target policy which holds a reference to Q</span></span><br><span class="line">            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])</span><br><span class="line">            <span class="comment"># If the action taken by the behavior policy is not the action </span></span><br><span class="line">            <span class="comment"># taken by the target policy the probability will be 0 and we can break</span></span><br><span class="line">            <span class="keyword">if</span> action !=  np.argmax(target_policy(state)):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            W = W * <span class="number">1.</span>/behavior_policy(state)[action]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> Q, target_policy</span><br><span class="line">    </span><br><span class="line">random_policy = create_random_policy(env.action_space.n)</span><br><span class="line">Q, policy = mc_control_importance_sampling(env, num_episodes=<span class="number">500000</span>, behavior_policy=random_policy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># For plotting: Create value function from action-value function</span></span><br><span class="line"><span class="comment"># by picking the best action at each state</span></span><br><span class="line">V = defaultdict(float)</span><br><span class="line"><span class="keyword">for</span> state, action_values <span class="keyword">in</span> Q.items():</span><br><span class="line">    action_value = np.max(action_values)</span><br><span class="line">    V[state] = action_value</span><br><span class="line">plotting.plot_value_function(V, title=<span class="string">"Optimal Value Function"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="On-Policy-Temporal-Difference-Learning"><a href="#On-Policy-Temporal-Difference-Learning" class="headerlink" title="On-Policy Temporal-Difference Learning"></a>On-Policy Temporal-Difference Learning</h1><h2 id="MC-vs-TD-Control"><a href="#MC-vs-TD-Control" class="headerlink" title="MC vs. TD Control"></a>MC vs. TD Control</h2><p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC)</p>
<ul>
<li>Lower variance</li>
<li>Online</li>
<li>Incomplete sequences</li>
</ul>
<p>Natural idea: use TD instead of MC in our control loop</p>
<ul>
<li>Apply TD to Q(S, A)</li>
<li>Use $\epsilon$-greedy policy improvement</li>
<li>Update every time-step</li>
</ul>
<h2 id="Sarsa-lambda"><a href="#Sarsa-lambda" class="headerlink" title="Sarsa($\lambda$)"></a>Sarsa($\lambda$)</h2><h3 id="Updating-Action-Value-Functions-with-Sarsa"><a href="#Updating-Action-Value-Functions-with-Sarsa" class="headerlink" title="Updating Action-Value Functions with Sarsa"></a>Updating Action-Value Functions with Sarsa</h3><p>$$Q(S,A) \leftarrow Q(S,A) + \alpha(R+\gamma Q(S’,A’)-Q(S,A))$$</p>
<h3 id="On-Policy-Control-With-Sarsa"><a href="#On-Policy-Control-With-Sarsa" class="headerlink" title="On-Policy Control With Sarsa"></a>On-Policy Control With Sarsa</h3><center><img src="/images/post_images/RL_lecture5_saras.png" width="300"></center>

<p>Every time-step:<br>Policy evaluation Saras, $$Q \approx q_\pi$$<br>Policy improvement $\epsilon$-greedy policy improvement</p>
<h3 id="Sarsa-Algorithm-for-On-Policy-Control"><a href="#Sarsa-Algorithm-for-On-Policy-Control" class="headerlink" title="Sarsa Algorithm for On-Policy Control"></a>Sarsa Algorithm for On-Policy Control</h3><center><img src="/images/post_images/RL_lecture5_sarsa_algor.png" width="300"></center>

<h3 id="Convergence-of-Sarsa"><a href="#Convergence-of-Sarsa" class="headerlink" title="Convergence of Sarsa"></a>Convergence of Sarsa</h3><font color="green">[Theorem]</font>

<p>Sarsa converges to the optimal action-value function, $$Q(s, a) \rightarrow q_∗(s, a)$$, under the following conditions:</p>
<ul>
<li>GLIE sequence of policies $$\pi_t (a|s)$$</li>
<li>Robbins-Monro sequence of step-sizes $$\alpha_t$$</li>
</ul>
<p>$$\sum_{t=1}^\infty \alpha_t = \infty$$<br>$$\sum_{t=1}^\infty \alpha^2 &lt; \infty$$</p>
<h3 id="Windy-Gridworld-Example"><a href="#Windy-Gridworld-Example" class="headerlink" title="Windy Gridworld Example"></a>Windy Gridworld Example</h3><h3 id="Sarsa-on-the-Windy-Gridworld"><a href="#Sarsa-on-the-Windy-Gridworld" class="headerlink" title="Sarsa on the Windy Gridworld"></a>Sarsa on the Windy Gridworld</h3><h1 id="Off-Policy-Learning"><a href="#Off-Policy-Learning" class="headerlink" title="Off-Policy Learning"></a>Off-Policy Learning</h1>
        
      </div>
      
      
      
    </div>
    


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>强化学习-5-MODEL-FREE-CONTROL</li>
          
        
      
    
  </ul>


    
    
    
  </div>


          </div>
          

  
    <div class="comments" id="gitalk-container">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Chao Wen</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#On-Policy-Monte-Carlo-Control"><span class="nav-number">2.</span> <span class="nav-text">On-Policy Monte-Carlo Control</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Free-Policy-Iteration-Using-Action-Value-Function"><span class="nav-number">2.1.</span> <span class="nav-text">Model-Free Policy Iteration Using Action-Value Function</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#epsilon-Greedy-Exploration"><span class="nav-number">2.1.1.</span> <span class="nav-text">$\epsilon$-Greedy Exploration</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GLIE-Monte-Carlo-Control"><span class="nav-number">2.2.</span> <span class="nav-text">GLIE Monte-Carlo Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Algorithm-and-Implementation"><span class="nav-number">2.3.</span> <span class="nav-text">Algorithm and Implementation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Off-Policy-Monte-Carlo-Control-using-weighted-importance-sampling"><span class="nav-number">3.</span> <span class="nav-text">Off-Policy Monte-Carlo Control using weighted importance sampling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#On-Policy-Temporal-Difference-Learning"><span class="nav-number">4.</span> <span class="nav-text">On-Policy Temporal-Difference Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MC-vs-TD-Control"><span class="nav-number">4.1.</span> <span class="nav-text">MC vs. TD Control</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sarsa-lambda"><span class="nav-number">4.2.</span> <span class="nav-text">Sarsa($\lambda$)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Updating-Action-Value-Functions-with-Sarsa"><span class="nav-number">4.2.1.</span> <span class="nav-text">Updating Action-Value Functions with Sarsa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#On-Policy-Control-With-Sarsa"><span class="nav-number">4.2.2.</span> <span class="nav-text">On-Policy Control With Sarsa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa-Algorithm-for-On-Policy-Control"><span class="nav-number">4.2.3.</span> <span class="nav-text">Sarsa Algorithm for On-Policy Control</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convergence-of-Sarsa"><span class="nav-number">4.2.4.</span> <span class="nav-text">Convergence of Sarsa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Windy-Gridworld-Example"><span class="nav-number">4.2.5.</span> <span class="nav-text">Windy Gridworld Example</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sarsa-on-the-Windy-Gridworld"><span class="nav-number">4.2.6.</span> <span class="nav-text">Sarsa on the Windy Gridworld</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Off-Policy-Learning"><span class="nav-number">5.</span> <span class="nav-text">Off-Policy Learning</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chao Wen</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  


  
    

<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">



<script src="//cdn.jsdelivr.net/npm/js-md5@0.7.3/src/md5.min.js"></script>

<script>
  var gitalk = new Gitalk({
    clientID: '682b527504e4475ef559',
    clientSecret: 'b9c751d03573533b9e6cf50704c917ecf035b822',
    repo: 'kabibi.github.io',
    owner: 'Kabibi',
    admin: ['Kabibi'],
    id: md5(location.pathname),
    
      language: 'zh-CN',
    
    distractionFreeMode: 'true'
  });
  gitalk.render('gitalk-container');
</script>

  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
  


  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.loadBookmark();
  
  </script>


  

  

  

</body>
</html>
