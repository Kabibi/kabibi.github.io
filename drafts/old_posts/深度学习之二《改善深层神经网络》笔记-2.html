<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="上节课我们主要介绍了如何建立一个实用的深度学习神经网络。包括Train/Dev/Test sets的比例选择，Bias和Variance的概念和区别：Bias对应欠拟合，Variance对应过拟合。接着，我们介绍了防止过拟合的两种方法：L2 regularization和Dropout。然后，介绍了如何进行规范化输入，以加快梯度下降速度和精度。然后，我们介绍了梯度消失和梯度爆炸的概念和危害，并提出">
<meta name="keywords" content="Andrew Ng deep learning">
<meta property="og:type" content="website">
<meta property="og:title" content="深度学习之二《改善深层神经网络》笔记(2)-梯度下降、滑动平均、优化算法以及学习率衰减">
<meta property="og:url" content="http://kabibi.github.io/drafts/old_posts/深度学习之二《改善深层神经网络》笔记-2.html">
<meta property="og:site_name" content="∇ &gt; 0">
<meta property="og:description" content="上节课我们主要介绍了如何建立一个实用的深度学习神经网络。包括Train/Dev/Test sets的比例选择，Bias和Variance的概念和区别：Bias对应欠拟合，Variance对应过拟合。接着，我们介绍了防止过拟合的两种方法：L2 regularization和Dropout。然后，介绍了如何进行规范化输入，以加快梯度下降速度和精度。然后，我们介绍了梯度消失和梯度爆炸的概念和危害，并提出">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L2_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L2_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L2_3.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L3_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L3_2.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L3_3.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L4_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L5_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L6_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L7_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2_optimization_update_rule.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L9_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L10_1.jpg">
<meta property="og:image" content="http://kabibi.github.io/images/post_images/C2W2L10_2.jpg">
<meta property="og:updated_time" content="2019-04-03T01:59:47.375Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深度学习之二《改善深层神经网络》笔记(2)-梯度下降、滑动平均、优化算法以及学习率衰减">
<meta name="twitter:description" content="上节课我们主要介绍了如何建立一个实用的深度学习神经网络。包括Train/Dev/Test sets的比例选择，Bias和Variance的概念和区别：Bias对应欠拟合，Variance对应过拟合。接着，我们介绍了防止过拟合的两种方法：L2 regularization和Dropout。然后，介绍了如何进行规范化输入，以加快梯度下降速度和精度。然后，我们介绍了梯度消失和梯度爆炸的概念和危害，并提出">
<meta name="twitter:image" content="http://kabibi.github.io/images/post_images/C2W2L2_1.jpg">





  
  
  <link rel="canonical" href="http://kabibi.github.io/drafts/old_posts/深度学习之二《改善深层神经网络》笔记-2">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>深度学习之二《改善深层神经网络》笔记(2)-梯度下降、滑动平均、优化算法以及学习率衰减 | ∇ > 0</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">∇ > 0</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Reinforcement Learning</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>

      
      
    </ul>
  

  
    

    
    
      
      
    
      
      
    
      
      
    
    

  


  

  
</nav>



  



</div>
    </header>

    
  
  

  

  <a href="https://github.com/kabibi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a>



    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    
    
    
    <div class="post-block page">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">深度学习之二《改善深层神经网络》笔记(2)-梯度下降、滑动平均、优化算法以及学习率衰减

</h1>

<div class="post-meta">
  
  


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>深度学习之二《改善深层神经网络》笔记-2</li>
          
        
      
    
  </ul>


</div>

</header>

      
      
      
      <div class="post-body">
        
        
          <p>上节课我们主要介绍了如何建立一个实用的深度学习神经网络。包括Train/Dev/Test sets的比例选择，Bias和Variance的概念和区别：Bias对应欠拟合，Variance对应过拟合。接着，我们介绍了防止过拟合的两种方法：L2 regularization和Dropout。然后，介绍了如何进行规范化输入，以加快梯度下降速度和精度。然后，我们介绍了梯度消失和梯度爆炸的概念和危害，并提出了如何使用梯度初始化来降低这种风险。最后，我们介绍了梯度检查，来验证梯度下降算法是否正确。本节课，我们将继续讨论深度神经网络中的一些优化算法，通过使用这些技巧和方法来提高神经网络的训练速度和精度。</p>
<a id="more"></a>
<h1 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h1><p>之前我们介绍的神经网络训练过程是对所有 m 个样本，称为 batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为<strong>每次迭代都要对所有样本进行进行求和运算和矩阵运算</strong>。我们将这种梯度下降算法称为 <code>Batch Gradient Descent</code>。</p>
<p>为了解决这一问题，我们可以把 m 个训练样本分成若干个子集，称为 <code>mini-batches</code>，这样每个子集包含的数据量就小了，例如只有 1000，然后<strong>每次在单一子集上进行神经网络训练</strong>，速度就会大大提高。这种梯度下降算法叫做 <code>Mini-batch Gradient Descent</code>。</p>
<p>假设总的训练样本个数 m=5000000，其维度为 $$(n_x,m)$$ 。将其分成 5000 个子集，每个 mini-batch 含有1000个样本。我们将每个 mini-batch 记为 $$X^$$ ，其维度为 $$(n_x,1000)$$ 。相应的每个 mini-batch 的输出记为 $$Y^$$ ，其维度为 (1,1000) ，且 $$t=1,2,\cdots,5000$$ 。</p>
<p>这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：</p>
<ul>
<li>$X^{(i)}$ ：第i个样本</li>
<li>$Z^{[l]}$ ：神经网络第l层网络的线性输出</li>
<li>$$X^{ { t }}$$, $$Y^{ { t } }$$ ：第 t 组 mini-batch</li>
</ul>
<p>Mini-batch Gradient Descent 的实现过程是先将总的训练样本分成 T 个子集（mini-batches），然后对每个 mini-batch 进行神经网络训练，包括 Forward Propagation，Compute Cost Function，Backward Propagation，循环至 T 个 mini-batch 都训练完毕。</p>
<p>$$$<br>\begin{align}<br>for\ \ t&amp;=1,\cdots,T\ \ {   \<br>&amp;Forward\ Propagation \<br>&amp;Compute\ Cost\ Function \<br>&amp;Backward\ Propagation \<br>&amp;W:=W-\alpha\cdot dW \<br>&amp;b:=b-\alpha\cdot db \<br>&amp;}<br>\end{align}<br>$$$</p>
<p>经过 T 次循环之后，所有 m 个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个 <code>epoch</code>。<strong>对于 Batch Gradient Descent 而言，一个 epoch 只进行一次梯度下降算法；而 Mini-Batch Gradient Descent，一个 epoch 会进行T次梯度下降算法。</strong></p>
<p>值得一提的是，对于 Mini-Batch Gradient Descent，可以进行多次 epoch 训练。而且，<strong>每次 epoch 最好是将总体训练数据重新打乱、重新分成 T 组 mini-batch，这样有利于训练出最佳的神经网络模型。</strong></p>
<hr>
<p>说完 mini-batch gradient descent，接下来我们扯一扯 mini-batch gradient descent 和其他 gradient descent 有哪些不同和优势以及如何设置 mini-batch 的大小。</p>
<p>Batch gradient descent 和 Mini-batch gradient descent 的 cost 曲线如下图所示：</p>
<center><img src="/images/post_images/C2W2L2_1.jpg" width="300"></center>

<p>对于一般的神经网络模型，使用 Batch gradient descent，随着迭代次数增加，cost 是不断减小的。然而，<strong>使用 Mini-batch gradient descent，随着在不同的 mini-batch 上迭代训练，其 cost 不是单调下降，而是受类似 noise 的影响，出现振荡</strong>。但整体的趋势是下降的，最终也能得到较低的 cost 值。之所以出现细微振荡的原因是不同的 mini-batch 之间是有差异的。例如可能第一个子集 $$(X^1,Y^1)$$ 是好的子集，而第二个子集 $$(X^2,Y^2)$$ 包含了一些噪声 noise。出现细微振荡是正常的。</p>
<p><font color="red">如何选择每个 mini-batch 的大小，即包含的样本个数呢？</font>有两个极端：如果 mini-batch size=m，即为 Batch gradient descent，只包含一个子集为 $$(X^1,Y^1)=(X,Y)$$ ；如果mini-batch size=1，即为Stachastic gradient descent，每个样本就是一个子集 $$(X^1,Y^1)=(x^{(i)},y^{(i)})$$ ，共有 m 个子集。</p>
<p>我们来<font color="red">比较一下 Batch gradient descent 和 Stochastic gradient descent 的梯度下降曲线</font>。如下图所示，蓝色的线代表 Batch gradient descent，紫色的线代表 Stochastic gradient descent。Batch gradient descent 会比较平稳地接近全局最小值，但是因为使用了所有 m 个样本，每次前进的速度有些慢。Stochastic gradient descent 每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。</p>
<center><img src="/images/post_images/C2W2L2_2.jpg" width="300"></center>

<p>实际使用中，<strong>mini-batch size 不能设置得太大（Batch gradient descent），也不能设置得太小（Stochastic gradient descent）。这样，相当于结合了 Batch gradient descent 和 Stochastic gradient descent 各自的优点，既能使用向量化优化算法，又能叫快速地找到最小值。</strong>mini-batch gradient descent 的梯度下降曲线如下图绿色所示，每次前进速度较快，且振荡较小，基本能接近全局最小值。</p>
<center><img src="/images/post_images/C2W2L2_3.jpg" width="300"></center>

<p>一般来说，如果总体样本数量 m 不太大时，例如 $$m\leq2000$$ ，建议直接使用 Batch gradient descent。如果总体样本数量 m 很大时，建议将样本分成许多 mini-batch。推荐<strong>常用的 mini-batch size 为 64, 128, 256, 512</strong>。这些都是 2 的幂。之所以这样设置的原因是计算机存储数据一般是 2 的幂，这样设置可以提高运算速度。</p>
<h1 id="Exponentially-weighted-averages"><a href="#Exponentially-weighted-averages" class="headerlink" title="Exponentially weighted averages"></a>Exponentially weighted averages</h1><p>该部分我们将介绍指数加权平均（Exponentially weighted averages）的概念。举个例子，记录半年内伦敦市的气温变化，并在二维平面上绘制出来，如下图所示：</p>
<center><img src="/images/post_images/C2W2L3_1.jpg" width="300"></center>

<p>看上去，温度数据似乎有 noise，而且抖动较大。如果我们希望看到半年内气温的整体变化趋势，可以通过<code>移动平均（moving average）</code>的方法来对每天气温进行平滑处理。</p>
<blockquote>
<p>例如我们可以设 $$V_0$$=0 ，当成第0天的气温值。第一天的气温与第 0 天的气温有关：<br>$$V_1=0.9V_0+0.1\theta_1$$</p>
</blockquote>
<blockquote>
<p>第二天的气温与第一天的气温有关：<br>$$\begin{eqnarray}V_2 &amp;=&amp;0.9V_1+0.1\theta_2\ &amp;=&amp;0.9(0.9V_0+0.1\theta_1)+0.1\theta_2\ &amp;=&amp;0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2 \end{eqnarray}$$</p>
</blockquote>
<blockquote>
<p>第三天的气温与第二天的气温有关：<br>$$\begin{eqnarray}V_3 &amp;=&amp;0.9V_2+0.1\theta_3\ &amp;=&amp;0.9(0.9^2V_0+0.9\cdot0.1\theta_1+0.1\theta_2)+0.1\theta_3\ &amp;=&amp;0.9^3V_0+0.9^2\cdot 0.1\theta_1+0.9\cdot 0.1\theta_2+0.1\theta_3 \end{eqnarray}$$</p>
</blockquote>
<blockquote>
<p>即第 t 天与第 t-1 天的气温迭代关系为：<br>$$$<br> \begin{eqnarray}<br> V_t &amp;=&amp; 0.9V_{t-1}+0.1\theta_t\<br> &amp;=&amp; \color{red}{0.9^tV_0}+ \color{blue}{0.9^{t-1}\cdot0.1\theta_1}+ \color{green}{0.9^{t-2}\cdot 0.1\theta_2}+ \cdots+0.9\cdot0.1\theta_{t-1}+<br> \color{purple}{0.1\theta_t}<br> \end{eqnarray}<br>$$$</p>
</blockquote>
<blockquote>
<p>经过移动平均处理得到的气温如下图红色曲线所示：</p>
</blockquote>
<blockquote>
<center><img src="/images/post_images/C2W2L3_2.jpg" width="300"></center>
</blockquote>
<p>这种滑动平均算法称为<code>指数加权平均（exponentially weighted average）</code>。根据之前的推导公式，其一般形式为：<br>$$V_t=\beta V_{t-1}+(1-\beta)\theta_t$$</p>
<p>上面的例子中， $$\beta$$=0.9 。 <strong>$$\beta$$ 值决定了指数加权平均的天数</strong>，近似表示为：</p>
<p>$$\frac{1}{1-\beta}$$</p>
<p>例如，当 $$\beta$$=0.9 ，则 $$\frac{1}{1-\beta}$$=10 ，表示将前10天进行指数加权平均。当 $$\beta$$=0.98 ，则 $$\frac{1}{1-\beta}$$=50 ，表示将前50天进行指数加权平均。 $$\beta$$ 值越大，则指数加权平均的天数越多，平均后的趋势线就越平缓，但是同时也会向右平移。下图绿色曲线和黄色曲线分别表示了 $$\beta$$=0.98 和 $$\beta$$=0.5 时，指数加权平均的结果。</p>
<center><img src="/images/post_images/C2W2L3_3.jpg" width="300"></center>

<p>这里简单解释一下公式 $$\frac{1}{1-\beta}$$ 是怎么来的。准确来说，指数加权平均算法跟之前所有天的数值都有关系，根据之前的推导公式就能看出。但是指数是衰减的，<strong>一般认为衰减到 $$\frac1e$$ 就可以忽略不计了</strong>。因此，根据之前的推导公式，我们只要证明<br>$$\beta^{\frac{1}{1-\beta}}=\frac1e$$</p>
<p>令 $$\frac{1}{1-\beta}=N ， N&gt;0 ，则 \beta=1-\frac{1}{N} ， \frac1N&lt;1$$ 。即证明转化为：<br>$$(1-\frac1N)^N=\frac1e$$</p>
<p>显然，当 N&gt;&gt;0 时，上述等式是近似成立的。至此，简单解释了为什么指数加权平均的天数的计算公式为 $$\frac{1}{1-\beta}$$ 。</p>
<p>我们将指数加权平均公式的一般形式写下来：<br>$$\begin{eqnarray}V_t &amp;=&amp;\beta V_{t-1}+(1-\beta)\theta_t\ &amp;=&amp;(1-\beta)\theta_t+(1-\beta)\cdot\beta\cdot\theta_{t-1}+(1-\beta)\cdot \beta^2\cdot\theta_{t-2}+\cdots\ &amp;&amp;+(1-\beta)\cdot \beta^{t-1}\cdot \theta_1+\beta^t\cdot V_0 \end{eqnarray}$$</p>
<p>观察上面这个式子， $$\theta_t,\theta_{t-1},\theta_{t-2},\cdots,\theta_1$$ 原始数据值， $$(1-\beta),(1-\beta)\beta,(1-\beta)\beta^2,\cdots,(1-\beta)\beta^{t-1}$$ 是类似指数曲线，从左向右，呈指数下降的。 $$V_t$$ 的值就是这两个子式的点乘，将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害。</p>
<center><img src="/images/post_images/C2W2L4_1.jpg" width="300"></center>

<p>我们已经知道了指数加权平均的递推公式。实际应用中，为了减少内存的使用，我们可以使用这样的语句来实现指数加权平均算法：<br>$$$<br>\begin{align}<br>V_{\theta}=0   \<br>Repeat{  \<br>&amp; Get\ next\ \theta_t \<br>&amp; V_{\theta}:=\beta V_{\theta}+(1-\beta)\theta_t  \<br>}<br>\end{align}<br>$$$</p>
<hr>
<p>在机器学习中使用 exponentially weighted average 还会遇到一个问题，就是偏移校正。上文中提到当 $$\beta$$=0.98 时，指数加权平均结果如下图绿色曲线所示。但是实际上，真实曲线如紫色曲线所示。</p>
<center><img src="/images/post_images/C2W2L5_1.jpg" width="300"></center>

<p>我们注意到，紫色曲线与绿色曲线的区别是，紫色曲线开始的时候相对较低一些。这是因为开始时我们设置 $$V_0$$=0 ，所以初始值会相对小一些，直到后面受前面的影响渐渐变小，趋于正常。修正这种问题的方法是进行<code>偏移校正（bias correction）</code>，即在每次计算完 $$V_t$$ 后，对 $$V_t$$ 进行下式处理：</p>
<p>$$\frac{V_t}{1-\beta^t}$$</p>
<p>在刚开始的时候，t 比较小， $$(1-\beta^t)&lt;1$$ ，这样就将 $$V_t$$ 修正得更大一些，效果是把紫色曲线开始部分向上提升一些，与绿色曲线接近重合。随着 t 增大， $$(1-\beta^t)\approx1$$ ， $$V_t$$ 基本不变，紫色曲线与绿色曲线依然重合。这样就实现了简单的偏移校正，得到我们希望的绿色曲线。</p>
<p>值得一提的是，<strong>机器学习中，偏移校正并不是必须的</strong>。因为，在迭代一次次数后（t 较大）， $$V_t$$ 受初始值影响微乎其微，紫色曲线与绿色曲线基本重合。所以，一般可以忽略初始迭代过程，等到一定迭代之后再取值，这样就不需要进行偏移校正了。</p>
<h1 id="Gradient-descent-with-momentum"><a href="#Gradient-descent-with-momentum" class="headerlink" title="Gradient descent with momentum"></a>Gradient descent with momentum</h1><p>该部分将介绍动量梯度下降算法，其速度要比传统的梯度下降算法快很多。做法是<strong>在每次训练时，对梯度进行指数加权平均处理，然后用得到的梯度值更新权重 W 和常数项 b</strong>。下面介绍具体的实现过程。</p>
<center><img src="/images/post_images/C2W2L6_1.jpg" width="300"></center>

<p>原始的梯度下降算法如上图蓝色折线所示。在梯度下降过程中，梯度下降的振荡较大，尤其对于W、b之间数值范围差别较大的情况。此时每一点处的梯度只与当前方向有关，产生类似折线的效果，前进缓慢。而如果<strong>对梯度进行指数加权平均，这样使当前梯度不仅与当前方向有关，还与之前的方向有关，这样处理让梯度前进方向更加平滑，减少振荡，能够更快地到达最小值处</strong>。</p>
<p>权重 W 和常数项 b 的指数加权平均表达式如下：<br>$$$<br>\begin{align}<br>V_{dW}&amp;=\beta\cdot V_{dW}+(1-\beta)\cdot dW  \<br>V_{db}&amp;=\beta\cdot V_{db}+(1-\beta)\cdot db  \<br>\end{align}<br>$$$</p>
<p>从动量的角度来看，以权重 W 为例， $$V_{dW}$$ 可以看成速度 V， dW 可以看成是加速度 a。指数加权平均实际上是计算当前的速度，当前速度由之前的速度和现在的加速度共同影响。而 $$\beta&lt;1$$ ，又能限制速度 $$V_{dW}$$ 过大。也就是说，当前的速度是渐变的，而不是瞬变的，是动量的过程。这保证了梯度下降的平稳性和准确性，减少振荡，较快地达到最小值处。动量梯度下降算法的过程如下：<br>$$$<br>\begin{align}<br>On\ iter&amp;ation\ t:   \<br>&amp;Compute\ dW,\ db\ on\ the\ current\ mini-batch  \<br>&amp;\color{red}{V_{dW}}=\beta \color{red}{V_{dW}}+(1-\beta)dW \<br>&amp;\color{blue}{V_{db}}=\beta \color{blue}{V_{db}}+(1-\beta)db \<br>&amp;W=W-\alpha \color{red}{V_{dW}},\ b=b-\alpha \color{blue}{V_{db}}   \<br>\end{align}<br>$$$<br>初始时，令 $$V_{dW}=0,V_{db}=0$$ 。一般设置 $$\beta$$=0.9 ，即指数加权平均前10天的数据，实际应用效果较好。另外，关于偏移校正，可以不使用。因为经过10次迭代后，随着滑动平均的过程，偏移情况会逐渐消失。补充一下，在其它文献资料中，动量梯度下降还有另外一种写法：<br>$$$<br>\begin{align}<br>V_{dW}&amp;=\beta V_{dW}+dW \<br>V_{db}&amp;=\beta V_{db}+db \<br>\end{align}<br>$$$</p>
<p>即消去了 dW 和 db 前的系数 $$(1-\beta)$$ 。这样简化了表达式，但是学习因子 $$\alpha$$ 相当于变成了 $$\frac{\alpha}{1-\beta}$$ ，表示 $$\alpha$$ 也受 $$\beta$$ 的影响。从效果上来说，这种写法也是可以的，但是不够直观，且调参涉及到 $$\alpha$$ ，不够方便。所以，实际应用中，推荐第一种动量梯度下降的表达式。</p>
<h1 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h1><p>RMSprop是另外一种优化梯度下降速度的算法。每次迭代训练过程中，其权重 W 和常数项 b 的更新表达式为：<br>$$$<br>S_W=\beta S_{dW}+(1-\beta)dW^2  \<br>S_b=\beta S_{db}+(1-\beta)db^2    \<br>W:=W-\alpha \frac{dW}{\sqrt{S_W}},\ b:=b-\alpha \frac{db}{\sqrt{S_b}}   \<br>$$$</p>
<p>下面简单解释一下RMSprop算法的原理，仍然以下图为例，为了便于分析，令水平方向为 W 的方向，垂直方向为 b 的方向。</p>
<center><img src="/images/post_images/C2W2L7_1.jpg" width="300"></center>

<p>从图中可以看出，梯度下降（蓝色折线）在垂直方向（b）上振荡较大，在水平方向（W）上振荡较小，表示在 b 方向上梯度较大，即 db 较大，而在 W 方向上梯度较小，即 dW 较小。因此，上述表达式中 $$S_b$$ 较大，而 $$S_W$$ 较小。在更新W和b的表达式中，变化值 $$\frac{dW}{\sqrt{S_W}}$$ 较大，而 $$\frac{db}{\sqrt{S_b}}$$ 较小。也就使得 W 变化得多一些，b 变化得少一些。<br>即加快了 W 方向的速度，减小了 b 方向的速度，减小振荡，实现快速梯度下降算法，其梯度下降过程如绿色折线所示。总得来说，就是如果哪个方向振荡大，就减小该方向的更新速度，从而减小振荡。</p>
<p>还有一点需要注意的是为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数 $$\varepsilon$$ ：</p>
<p>$$W:=W-\alpha \frac{dW}{\sqrt{S_W}+\varepsilon},\ b:=b-\alpha \frac{db}{\sqrt{S_b}+\varepsilon}$$</p>
<p>其中， $$\varepsilon=10^{-8}$$ ，或者其它较小值。</p>
<h1 id="Adam-optimization-algorithm"><a href="#Adam-optimization-algorithm" class="headerlink" title="Adam optimization algorithm"></a>Adam optimization algorithm</h1><p>Adam（Adaptive Moment Estimation）算法结合了动量梯度下降算法和RMSprop算法。其算法流程为：</p>
<p>$$$<br>\begin{align}<br>&amp;V_{dW}=0,\ S_{dW},\ V_{db}=0,\ S_{db}=0 \<br>On\ &amp;iteration\ t:  \<br>&amp;Compute\ dW,\ db   \<br>&amp;V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW,\ V_{db}=\beta_1V_{db}+(1-\beta_1)db    \<br>&amp;S_{dW}=\beta_2S_{dW}+(1-\beta_2)dW^2,\ S_{db}=\beta_2S_{db}+(1-\beta_2)db^2    \<br>&amp;V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta_1^t},\ V_{db}^{corrected}=\frac{V_{db}}{1-\beta_1^t}  \<br>&amp;S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta_2^t},\ S_{db}^{corrected}=\frac{S_{db}}{1-\beta_2^t}  \<br>&amp;W:=W-\alpha\frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\varepsilon},\ b:=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\varepsilon}  \<br>\end{align}<br>$$$</p>
<p>Adam算法包含了几个超参数，分别是： $$\alpha,\beta_1,\beta_2,\varepsilon$$ 。其中， $$\beta_1$$ 通常设置为0.9， $$\beta_2$$ 通常设置为0.999， $$\varepsilon$$ 通常设置为 $$10^{-8}$$ 。一般只需要对 $$\beta_1$$ 和 $$\beta_2$$ 进行调试。实际应用中，Adam算法结合了动量梯度下降和RMSprop各自的优点，使得神经网络训练速度大大提高。</p>
<p>下面总结几种优化算法的更新规则：</p>
<center><img src="/images/post_images/C2W2_optimization_update_rule.jpg" width="400"></center>


<h1 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h1><p>减小学习因子 $$\alpha$$ 也能有效提高神经网络训练速度，这种方法被称为<code>learning rate decay</code>。</p>
<p>Learning rate decay就是随着迭代次数增加，学习因子 $$\alpha$$ 逐渐减小。下面用图示的方式来解释这样做的好处。下图中，蓝色折线表示使用恒定的学习因子 $$ \alpha$$，由于每次训练 $$ \alpha$$  相同，步进长度不变，在接近最优值处的振荡也大，在最优值附近较大范围内振荡，与最优值距离就比较远。绿色折线表示使用不断减小的 $$ \alpha$$ ，随着训练次数增加， $$\alpha$$ 逐渐减小，步进长度减小，使得能够在最优值处较小范围内微弱振荡，不断逼近最优值。相比较恒定的 $$\alpha$$来说，learning rate decay 更接近最优值。</p>
<center><img src="/images/post_images/C2W2L9_1.jpg" width="300"></center>

<p>Learning rate decay中对 $$\alpha$$ 可由下列公式得到：</p>
<p>$$\alpha=\frac{1}{1+decay_rate*epoch}\alpha_0$$</p>
<p>其中，decay_rate 是参数（可调），epoch 是训练完所有样本的次数。随着 epoch 增加， $$\alpha$$ 会不断变小。</p>
<p>除了上面计算 $$\alpha$$ 的公式之外，还有其它可供选择的计算公式：</p>
<p>$$$<br>\begin{align}<br>\alpha&amp;=0.95^{epoch}\cdot \alpha_0   \<br>\alpha&amp;=\frac{k}{\sqrt{epoch}}\cdot \alpha_0     \<br>\alpha&amp;=\frac{k}{\sqrt{t}}\cdot \alpha_0 \<br>\end{align}<br>$$$</p>
<p>其中，k 为可调参数，t 为 mini-bach number。除此之外，<strong>还可以设置 $$\alpha$$ 为关于 t 的离散值，随着 t 增加， $$\alpha$$ 呈阶梯式减小</strong>。当然，也可以根据训练情况灵活调整当前的 $$\alpha$$ 值，但会比较耗时间。</p>
<h1 id="The-problem-of-local-optima"><a href="#The-problem-of-local-optima" class="headerlink" title="The problem of local optima"></a>The problem of local optima</h1><p>在使用梯度下降算法不断减小 cost function 时，可能会得到局部最优解（local optima）而不是全局最优解（global optima）。之前我们对局部最优解的理解是形如碗状的凹槽，如下图左边所示。但是在神经网络中，local optima 的概念发生了变化。准确地来说，<strong>大部分梯度为零的“最优点”并不是这些凹槽处，而是形如右边所示的马鞍状，称为 saddle point</strong>。也就是说，梯度为零并不能保证都是 convex（极小值），也有可能是 concave（极大值）。特别是在神经网络中参数很多的情况下，所有参数梯度为零的点很可能都是右边所示的马鞍状的 saddle point，而不是左边那样的 local optimum。</p>
<center><img src="/images/post_images/C2W2L10_1.jpg" width="400"></center>

<p><strong>类似马鞍状的 plateaus 会降低神经网络学习速度</strong>。Plateaus 是梯度接近于零的平缓区域，如下图所示。在 plateaus 上梯度很小，前进缓慢，到达 saddle point 需要很长时间。到达 saddle point 后，由于随机扰动，梯度一般能够沿着图中绿色箭头，离开 saddle point，继续前进，只是在 plateaus 上花费了太多时间。</p>
<center><img src="/images/post_images/C2W2L10_2.jpg" width="300"></center>

<p>总的来说，关于local optima，有两点总结：</p>
<ul>
<li>只要选择合理的强大的神经网络，一般不太可能陷入 local optima 而是 saddle point。</li>
<li>Plateaus可能会使梯度下降变慢，降低学习速度</li>
</ul>
<p>值得一提的是，上文介绍的动量梯度下降，RMSprop，Adam算法都能有效解决plateaus下降过慢的问题，大大提高神经网络的学习速度。</p>

        
      </div>
      
      
      
    </div>
    


  
  
  <ul class="breadcrumb">
    
      
      
        
          
            
          
          
            <li><a href="/drafts/">DRAFTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li><a href="/drafts/old_posts/">OLD_POSTS</a></li>
          
        
      
    
      
      
        
          
            
          
          
            <li>深度学习之二《改善深层神经网络》笔记-2</li>
          
        
      
    
  </ul>


    
    
    
  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/kenan.jpeg" alt="Aaron">
            
              <p class="site-author-name" itemprop="name">Aaron</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                    <span class="site-state-item-count">1</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Mini-batch-gradient-descent"><span class="nav-number">1.</span> <span class="nav-text">Mini-batch gradient descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Exponentially-weighted-averages"><span class="nav-number">2.</span> <span class="nav-text">Exponentially weighted averages</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gradient-descent-with-momentum"><span class="nav-number">3.</span> <span class="nav-text">Gradient descent with momentum</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RMSprop"><span class="nav-number">4.</span> <span class="nav-text">RMSprop</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adam-optimization-algorithm"><span class="nav-number">5.</span> <span class="nav-text">Adam optimization algorithm</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-rate-decay"><span class="nav-number">6.</span> <span class="nav-text">Learning rate decay</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-problem-of-local-optima"><span class="nav-number">7.</span> <span class="nav-text">The problem of local optima</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Aaron</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  



  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/reading_progress/reading_progress.js"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: true,
    notify: true,
    appId: 'fTeB9V5sfVjFhtRO94F2DNCL-gzGzoHsz',
    appKey: 'FygR9J8SjMjPb7QRF9Ld54NH',
    placeholder: '说点什么～',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: 'zh-cn' || 'zh-cn'
  });
</script>




  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
  


  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  

  

  

  
  
  
  <script src="/lib/bookmark/bookmark.min.js?v=1.0"></script>
  <script>
  
    bookmark.loadBookmark();
  
  </script>


  

  

  

</body>
</html>
