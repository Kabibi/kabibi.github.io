{"meta":{"title":"Aaron's blog","subtitle":"潇洒自在，任性逍遥。我就是我，不一样的烟火～","description":null,"author":"Aaron","url":"kabibi.github.io"},"pages":[{"title":"关于博主","date":"2018-01-08T07:53:44.000Z","updated":"2018-01-15T06:13:30.000Z","comments":true,"path":"about/index.html","permalink":"kabibi.github.io/about/index.html","excerpt":"","text":"热爱生活，喜爱书法，勤于健身，痴于阅读 本科在读 研究方向：机器学习，深度学习"},{"title":"分类","date":"2018-01-08T07:18:57.000Z","updated":"2018-01-15T06:13:30.000Z","comments":true,"path":"categories/index.html","permalink":"kabibi.github.io/categories/index.html","excerpt":"","text":""},{"title":"强化学习之四-动态规划","date":"2018-01-11T11:16:43.000Z","updated":"2018-01-15T06:13:30.000Z","comments":true,"path":"drafts/强化学习之四-动态规划.html","permalink":"kabibi.github.io/drafts/强化学习之四-动态规划.html","excerpt":"","text":"Dynamic Programming(DP): 给定一个关于环境的精确的MDP模型，动态规划是一系列能够用来计算这个环境中最优策略的一系列算法。 Classical DP: classical DP 方法使用范围有限，因为他们需要对环境的完美假设以及大量的计算开销，但经典DP算法在理论上依然有研究价值。其他的许多方法能够看做是经典DP方法的一个拓展：不需要对环境的完美假设以及大量的计算开销就能得到和经典DP同样的结果。 本章中，我们经常假设环境是 finite MDP，也就是假定状态空间 $S$ ，动作空间 $A$ 以及奖赏集 $R$ 均有限, 使用动态规划方法来计算值函数 v_* (s) = \\max_a E[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a ]Policy EvaluationPolicy ImprovementPolicy IterationValue IterationAsynchronous Dynamic ProgrammingGeneralized Policy IterationEfficiency of Dynamic Programming"},{"title":"《把时间当作朋友》读书笔记","date":"2018-01-08T10:22:04.000Z","updated":"2018-01-15T06:13:30.000Z","comments":true,"path":"drafts/把时间当作朋友-读书笔记.html","permalink":"kabibi.github.io/drafts/把时间当作朋友-读书笔记.html","excerpt":"去年年末的时候，我定了一个小目标：今年要读100本书。制定这个计划也并非心血来潮，去年保研之后，才第一次知道了没有事情是多么空虚的一件事，那几个月的时间，似乎突然醒悟了，没有人会永远推着你去走，总有一天，你要学会自己的人生制定计划，路怎样走，再也没有人会永远催着你前进。在那段空虚的阶段中，书似乎才第一次进入到我的世界中，我第一次发现读书是多么有乐趣的一件事，但我又有些许悔恨，过去的三年多时间里，我失去了一个多么精彩的世界呀。","text":"去年年末的时候，我定了一个小目标：今年要读100本书。制定这个计划也并非心血来潮，去年保研之后，才第一次知道了没有事情是多么空虚的一件事，那几个月的时间，似乎突然醒悟了，没有人会永远推着你去走，总有一天，你要学会自己的人生制定计划，路怎样走，再也没有人会永远催着你前进。在那段空虚的阶段中，书似乎才第一次进入到我的世界中，我第一次发现读书是多么有乐趣的一件事，但我又有些许悔恨，过去的三年多时间里，我失去了一个多么精彩的世界呀。 《把时间当作朋友》是我今年读的第二本书，作者李笑来，新东方的一名厨师，哦不，一名老师。如果不考托福的同学可能会对他很陌生，我第一次听到他还是这个人因为比特币成为了亿万富翁，我深知自己没有这样的远见和睿智，只能默默地又去屯了几本投资理财的书籍。 内容概述言归正传，这本书应该属于众多鸡汤中稍微苦涩一点的，因为书中对各种“成功学”，“人脉“，”时间管理”都有一些比较理性的分析。并不会像市面上大多数鸡汤一样，告诉你努力就可以成功，读的时候很爽，结果却没有什么卵用。 这本书中也充斥了许多大实话，比较难听却往往是事实。人们往往喜欢听悦耳的话，但是这种话往往是虚假的。 分章概述第1章: 心智的力量了解心智的力量首先，作者在本节中提出一个现象：老师重复千万遍的“真理”，尽管学生也点头同意，却有相当一部分人不会去实践。究其原因，是心智的差异在作怪。面对相同的问题，相同的理由，却由于心智力量的差异，每个人会做出不同的选择。用很糙的话来讲，就是：老师告诉你一定要努力学习，没有人会质疑这个陈述的正确性，但偏偏有人不努力。“道理都懂，就是做不到”，或许就是心智力量的不同所造成。 然后，作者通过了几个例子去阐述这样一个事实：心智力量的差异，最终会造成不可逾越的差异。 心智不同的人在面对蠢上司的态度和行为是不同的。抱怨上司蠢的人只是在为自己的懒惰寻找借口。 心智不同的人在被猎头找上后的而态度和行为是不同的。炫耀的人往往不知道猎头寻找的只是”二流人才”。 心智不同的人在做自己不喜欢的事情时心态是不同的，心智低的人会觉得自己做不好是因为不感兴趣，却没有意识到：往往不是有兴趣才能做好，而是做好了才有兴趣。 有的人会觉得学习方法是至关重要的，殊不知学习上的成功只靠两件事：策略和坚持，而坚持本身就是最好的策略。心智低的人永远在寻找学习方法，却从没有真正开始过。“语气不停地找更好的方法，还不如马上行动，省得虚度光阴。”，相比坚持来讲，方法的作用几乎可以忽略不计。 作者认为：心智低的人会因为懒惰拒绝学会盲打。 总结：心智出现问题，我们就会因为错误的理解而做出错误的判断，因此浪费的事件无法无量，可悲的是这些往往没有意识到。 心智力量的差异第2章: 开启自己的心智第3章: 提高心智，和时间做朋友第4章: 开拓我们的心智第5章: 小心作为的成功学第6章: 更多思考第7章: 从此时此刻开始改变综述笔记动机是一回事儿，预期是一回事儿，而结果往往又是另外一回事儿。 能够登上金字塔顶端的只有两种动物，一种是雄鹰，一种是蜗牛。雄鹰拥有矫健的翅膀，所以能够飞到金字塔的顶端，而蜗牛只能从底下一点点爬上去。雄鹰飞到顶端只要一瞬间，而蜗牛可能需要爬很久很久，也许需要坚持一辈子才能爬到顶端，也许爬到一半滚下来不得不从头爬起，但只要蜗牛爬到顶端，他所到达的高度和看到的世界就和雄鹰是一样的。我们大部分人也许不是雄鹰，但是我们每一个人都可以拥有蜗牛的精神，我们可以不断地攀登自己生命的高峰，终有一天，我们可以在无限风光的险峰俯视和欣赏这个美丽的世界。无论是雄鹰，还是蜗牛，都因为它们的勤奋和努力，就有了时间作为自己的朋友，每一分、每一秒，它们的生命都因此有了它们自己确定的意义，而非虚度。 仔细听清楚，无论我讲得多么有趣、多么有用或多么有道理，暂时还跟你一点关系都没有——当且只当你按我说的做了之后，对你来说，才算是真的有趣、真的有用、真的有道理。” “仔细听清楚，无论我讲得多么有趣、多么有用或多么有道理，暂时还跟你一点关系都没有——当且只当你按我说的做了之后，对你来说，才算是真的有趣、真的有用、真的有道理。” 制作一个“任务列表”，其实谁都会；分清楚“重要”与“次要”，或者“紧急”与“非紧急”也没有谁不会。因“不知道学习有什么用”而拒绝学习的人，会在接下来的日子里虚度无数光阴，哪怕他们天天“科学地”制定计划，编制“任务列表”。而与之相反，因“不知道学习有什么用”而选择 制作一个“任务列表”，其实谁都会；分清楚“重要”与“次要”，或者“紧急”与“非紧急”也没有谁不会。因“不知道学习有什么用”而拒绝学习的人，会在接下来的日子里虚度无数光阴，哪怕他们天天“科学地”制定计划，编制“任务列表”。而与之相反，因“不知道学习有什么用”而选择努力学习的人，每时每刻都充满了收获，并且会在将来的某一天获得更多的“意外”收获，哪怕他们可能显得“漫无目的”——结果真的是天壤之别。 这就是人与人之间的差异——除了看得见的相貌、身材、出身、财富之外，还有看不见摸不着的心智力量的差别。常常听人慨叹，“人与人之间怎么会有那么大的差异呢？”而事实上，我们应该对此毫不惊讶才对。 往往并不是有兴趣才能做好，而是做好了才有兴趣。 所有学习上的成功，都只靠两件事：策略和坚持，而坚持本身就是最重要的策略。 与其不停地找更好的方法，还不如马上开始行动，省得虚度更多的时间。 记得过去老师讲解《论语》中曾子说的“吾日三省吾身”，有两种解释，第一种是我每一天都自我反省三（多）次；第二种是我每一天都以下列三件事来自我反省；而根据上下文，貌似第二种解释更加合理。但是，在我那次被自己突然弄清楚的意识吓坏了之后的顿悟是，管他每天到底是“反省三次”，还是“列出三件事来反省”，都并不重要。 重要的是，他每天都在“反省”。 作为一个真正意义上的人，好像没有什么比这个更重要了。 那是一个14岁的男孩。8岁那年的11月，他的母亲突然去世。9岁那年的11月，他从梯子上掉下来，摔断了胳膊。10岁那年的11月，他骑自行车时发生车祸，造成头骨断裂，还伴有严重的脑震荡。11岁那年的11月，他从天窗跌了下来，造成臀部骨折。12岁那年的11月，他从滑板上摔下来，导致手腕骨骨折。13岁那年的11月，他被汽车撞伤，造成骨盆断裂…… 最痛苦事儿之一可能就是：“怎么道理全明白，但就是不行呢？”——谁都不愿意犯同样的错误，并且明白那是非常愚蠢的，但怎么就又在同样的地方跌倒了呢？为此痛苦，深夜难眠，一遍一遍地骂自己。可是，一觉醒来，其实只不过是几个小时之后，就再次回到从前的状态，并无任何变化。甚至下次在深夜里暗骂许多遍之后才想起来没多久之前也这样骂过自己，不禁长叹：“怎么就这么没出息，怎么会好了伤疤就忘了疼呢！” 有两种办法很简单却又非常有效。第一种办法是当你面临尴尬的时候，记得一定要拿出纸笔来，把你所遇到的尴尬记录下来——当然，最好是记录在同一个本子里。这样的记录是非常有意义的。因为它会提醒你，这是你曾经遇到过的尴尬。如果你不用纸笔记下来，那你就肯定会忘的。然后还要养成习惯，定期拿出这个本子回顾一下。这个习惯往往会使你很有成就感的，因为你知道，甚至可以清楚地看到你已经有进步了，因为那个本子里记录过的很多的错误你都不再犯了——当然，不再犯那些错误的原因是因为你在不停地提醒你自己！ 所以，背单词的时候，事实上，在做所有类似的必须记住大量信息的工作的时候，一定要想办法由衷地把这件事当作快乐的事情来做。 找来一批四岁孩子，给他们每人一块糖，并告诉他们若能等主持人回来再吃这块糖，则还能吃到第二块糖。戈尔曼悄悄观察，发现有的孩子只等了一会儿便不耐烦，迫不及待地把糖塞进了嘴里；而有的孩子则很有耐心，而且很有办法，想出做游戏、讲故事之类种种方式拖延时间，分散注意力，最终坚持到主持人回来，得到了第二块糖。戈尔曼又对这批孩子14岁时和进入工作岗位后的表现进行了跟踪调查，发现晚吃糖的孩子数学和语文成绩比早吃糖的平均高出20％，而且意志坚强，经得起困难和挫折，更容易取得成功。 大多数孩子没有等到主持人回来，已经把糖吃掉了”这样一个事实能够完全证明的或者说明的也许只有两件事儿：a．希望自己的欲望马上获得满足（Instant Gratification）是大多数人的天性；b．野心永远是少数人的天性。 所以，有时候成为高手需要愚钝，金庸小说里的郭靖成为一代宗师的根本原因更可能是因为他“傻”到一定程度，所以，很多人或事情对别人来讲是诱惑，对他来 所以，有时候成为高手需要愚钝，金庸小说里的郭靖成为一代宗师的根本原因更可能是因为他“傻”到一定程度，所以，很多人或事情对别人来讲是诱惑，对他来讲是干脆不存在的；于是，他可以用普通的智商长期只专注在一件最应该做的事情上，最终 所以，有时候成为高手需要愚钝，金庸小说里的郭靖成为一代宗师的根本原因更可能是因为他“傻”到一定程度，所以，很多人或事情对别人来讲是诱惑，对他来讲是干脆不存在的；于是，他可以用普通的智商长期只专注在一件最应该做的事情上，最终天下无敌。 一位朋友读完捷克作家米兰·昆德拉的小说《生命不能承受之轻》之后概括说，逃避责任就会带来轻松，可那恰恰就是“生命不能承受之轻”啊！ 掩卷之后只能长叹。于我这种普通人来讲，这种大师的境界，正是所谓的遥不可及，仰之弥高，望之弥艰，钻之亦不可得。知易行难啊。 这是一本很薄的册子，所以我在不到一个小时的时间里就读完了。掩卷之后只能长叹。于我这种普通人来讲，这种大师的境界，正是所谓的遥不可及，仰之弥高，望之弥艰，钻之亦不可得。知易行难啊。 基于过程记录自己的生活事件,从而精确感知时间。 精确感知时间 当我们决心改变的时候，“懒惰”便会指使我们的那个“大脑的自我保护功能”——“遗忘”——起作用，让我们不知不觉停止改变。 如果你已经习惯每天晚上睡觉之前记录一下当日的时间开销，那么，第二天早上就会很自然地开始在脑子里规划全天的时间了——不信你就试试看！ 作者从大师身上得到灵感, 鼓励我们记录每天的时间开销. 并说明了这样做的意义.帮助自己精确地感知时间,并且和时间做朋友。 按照作者的方式, 开始记录每天做了什么。尽管刚开始会大吃一惊, 但相信你并并不孤独。 强迫自己理智一些，就会知道，无用的事情，哪怕非常有趣都不应该去做；而有用的事情，哪怕非常无趣，你都应该做。但是，请你认真面对你自己，过去你一直是这样用理智指导你的行为的么？ 相信我，养成任何一个哪怕很小的习惯，都是要挣扎的。然而，貌似痛苦的挣扎过程，在将来的某个时刻终归会变得其乐无穷。 我总觉得一个人最终成功，并不是因为他曾经精确地计划自己的成功，而是关键在于他的坚持。走向成功的过程大抵上就像你的起点是南极，而成功路径的终点在北极。 我总觉得一个人最终成功，并不是因为他曾经精确地计划自己的成功，而是关键在于他的坚持。走向成功的过程大抵上就像你的起点是南极，而成功路径的终点在北极。那么，无论你往那个方向走，只要中途不改变方向，最终会到达北极。但是，如果你中途改变了方向，甚至经常改变方向，你就无法到达北极，甚至可能返回出发点。所以，先判断你的这个列表所代表的那个任务是不是现实的，如果你真的觉得你能够、也应该完成这个任务，那就开始去做；并且一定要做到底。 行动过程中，发现既定的目标确实是不现实的、不可行的，那么，半途而废不仅并不意味着失败，还意味着该决策者是无比理智的。 在行动过程中，发现既定的目标确实是不现实的、不可行的，那么，半途而废不仅并不意味着失败，还意味着该决策者是无比理智的。 时间的浪费，往往是因为a．目标不现实或者目前暂时尚不可行；b．为了达到目标而制定的实施策略有误。所以，为了不浪费时间，做任何事情的时候，要先仔细审视这两个条件。总是有人告诫我们凡事儿应该“三思而后行”，却很少有人告诉我们到底应该把什么东西思考起码三遍？我的经验是，行动之前，反复思考——要远远多于仅仅三遍——上述两个条件会大大减少时间的浪费。 计划是必需的，目标当然应该是确定的。一般来讲，越是短期的目标，越容易清晰。越是清晰的目标越容易实现。理想固然应该是有的，但是，理想这东西往往太过遥远，乃至于我们总是看不清楚。不过还好，所谓千里之行始于足下，我们要做的事情是把每一步都走好，踩得足够踏 计划是必需的，目标当然应该是确定的。一般来讲，越是短期的目标，越容易清晰。越是清晰的目标越容易实现。理想固然应该是有的，但是，理想这东西往往太过遥远，乃至于我们总是看不清楚。不过还好，所谓千里之行始于足下，我们要做的事情是把每一步都走好，踩得足够踏实。至于千里之外的终点，既然看都看不到，就不用花时间去想了，想了也没用。用各种方法保持乐观就好——乐观是靠努力和挣扎才可以获得的经验。 雾里看花，谁都看不清楚，上帝也没有在我们出生时给我们什么额外的特殊装备。然而，我相信，只要不停地往前走，总是可以走到一个鲜花盛开的地方，在那里，无论雾多大，你总是可以看到那些花的，因为已经足够近——足够近的时候，就算看不到花，还是可以闻到 不知所措。雾里看花，谁都看不清楚，上帝也没有在我们出生时给我们什么额外的特殊装备。然而，我相信， 雾里看花，谁都看不清楚，上帝也没有在我们出生时给我们什么额外的特殊装备。然而，我相信，只要不停地往前走，总是可以走到一个鲜花盛开的地方，在那里，无论雾多大，你总是可以看到那些花的，因为已经足够近——足够近的时候，就算看不到花，还是可以闻到 雾里看花，谁都看不清楚，上帝也没有在我们出生时给我们什么额外的特殊装备。然而，我相信，只要不停地往前走，总是可以走到一个鲜花盛开的地方，在那里，无论雾多大，你总是可以看到那些花的，因为已经足够近——足够近的时候，就算看不到花，还是可以闻到花香的。 很多的时候，没必要做计划的原因有两个，除了前面提到过的“大多数计划其实非常简单”之外，另外一个是“初始状态下，我们往往实际上并没有 很多的时候，没必要做计划的原因有两个，除了前面提到过的“大多数计划其实非常简单”之外，另外一个是“初始状态下，我们往往实际上并没有能力去制定合理有效的计划”。因为做任何事情，我们都可能要经历相同的过程：逐步熟悉，小心摸索，失败失败再失败，认真反思，卷土重来，直至成功。而在最初甚至连基本的认知都没有的时候，制定出来的计划十有八九只不过是空谈。 我们一生做的事，大多都是一个试错（Trial and Error）的过程，对于人生，没有人能像解释数学那样给出普适的公式。永远记住，马上行动是最重要的。 当然，有些时候，你必须拖延你的行动。比如，当你决定买个新潮手机的时候，故意拖延三个月，会让你享受更低的价格；如果你决定买一辆你非常中意的高级轿车，故意拖延上一年，也许就会让你意识到当初的审美观其实很有问题。我个人的经验是，对我个人来讲，所有的大额消费活动，乃至其他一切涉及金钱的活动，诸如投资之类，“马上行动”的建议肯定不适用。相反，这种情况下，一定要拖延，拖得越久越好——仅是我个人的经验。 我也有很多次这样的经历, 比如买平板, 换手机, 买电脑。买东西的时候一定要有拖延症。 每天早上花几分钟制定一天的时间预算, 并且标记其重要程度。 制作时间预算 强迫自己理智一些，就会知道，无用的事情，哪怕非常有趣都不应该去做；而有用的事情，哪怕非常无趣，你都应该做。但是，请你认真面对你自己，过去你一直是这样用理智指导你的行为的么？ 使用列表减少失误 制定check list 和tudo list。 试错，试过之后，知道错了，然后就不再犯错了。当然，也许在试过之后，发现不仅没错，还很正确，那么就多了一项新的知识。“试错”是如此重要，乃至于所谓的“教育”在最古老的年代，是要靠鞭子的——做对了，吝于奖励，但是做错了，就要加以惩罚。到今天，还是有很多的父母，依然把惩罚当作主要的教育手段之一。 试错, 发现错了, 就不会再犯。如果正确, 恭喜你又多了一条知识。 在“试错”这个手段的基础上，另外一个“聪明”一点的，也重要得多的获取知识的方式是“观察”。 获得知识的基本途径 作者提出, 试错, 观察和阅读, 是我们获取知识的三种手段 与科学一样，宗教也是人们用来“思考”这个世界、“解释”这个世界的工具，只不过，在解释物理世界方面（比如，生命的起源、天体运转的机理），现代科学已经逐步地替代了宗教。 与科学一样，宗教也是人们用来“思考”这个世界、“解释”这个世界的工具，只不过，在解释物理世界方面（比如，生命的起源、天体运转的机理），现代科学已经逐步地替代了宗教。今天的宗教势力依然庞大，只是宗教的重心已经转移到另外一个更需要它的方面去了——人文 与科学一样，宗教也是人们用来“思考”这个世界、“解释”这个世界的工具，只不过，在解释物理世界方面（比如，生命的起源、天体运转的机理），现代科学已经逐步地替代了宗教。今天的宗教势力依然庞大，只是宗教的重心已经转移到另外一个更需要它的方面去了——人文领域。 首先，个体的经验有限。 其次，群体的经验有限。 最后，不仅存在无法通过个体或者群体经验获得的知识，还存在与现有经验相悖的知识。 我们常说，“经验宝贵”，然而， 最后，不仅存在无法通过个体或者群体经验获得的知识，还存在与现有经验相悖的知识。 摆脱经验的局限, 理解与经验相悖的知识。 摆脱经验的局限 人类拥有了文字之后，拥有了最佳的知识积累经验共享手段，乃至于成为地球上最强大的物种——而其他的物种只能 人类拥有了文字之后，拥有了最佳的知识积累经验共享手段，乃至于成为地球上最强大的物种——而其他的物种只能依赖最落后但被称为神奇的方式：基因遗传。 我们的大脑有个运行机制叫做“选择性输入”。具体表现就是你在很多人身上看到的特征：他们只能听到自己喜欢听的，只能看到自己喜欢看的。其实这样的特征未必就是100％的缺陷，很多时候它对我们来说甚至属于“自我保护功能”。然而，对于一个挣扎着发展自己心智的人来讲，“选择性输入”就是个可怕的敌人了。 这个和我的经验很类似, 万事开头难, 所以一开始一定很缓慢, 如果你坚持了下去, 就会突飞猛进。当你的知识学得差不多的时候, 进步有会是缓慢的。 这个和我的经验很类似, 万事开头难, 所以一开始一定很缓慢, 如果你坚持了下去, 就会突飞猛进。当你的知识学得差不多的时候, 进步又会是缓慢的。 在任何一个阶段，总是有一段时间进展缓慢，许久过后，所谓量变到质变的效果才会出现，才可能有突飞猛进的感觉。 可是人们往往在行进一小段时间之后，就因为觉得进展“过分”缓慢而产生动摇。所以，从来都没有体会过突飞猛进的感觉。然而这样慢慢动摇直至最终放弃的人，在漫长的一生中总是会遇到身边的某些人在“突飞猛进”——于是，在不愿意承认自己曾经的错误的情况下，当然貌似“最合理”的解释就是“呀，他肯定有什么诀 在任何一个阶段，总是有一段时间进展缓慢，许久过后，所谓量变到质变的效果才会出现，才可能有突飞猛进的感觉。 可是人们往往在行进一小段时间之后，就因为觉得进展“过分”缓慢而产生动摇。所以，从来都没有体会过突飞猛进的感觉。然而这样慢慢动摇直至最终放弃的人，在漫长的一生中总是会遇到身边的某些人在“突飞猛进”——于是，在不愿意承认自己曾经的错误的情况下，当然貌似“最合理”的解释就是“呀，他肯定有什么诀窍！” 按照正态分布的规律来看，在某一个阶段里，你必然只能遇到一两位好的老师，以及许多平庸的老师，和那么一两位甚至可能令人生厌的老师——相信我，他们也恨你，如果你恨他们的话，每个人在这方面都非常敏感。 首先，那些老猴子、大猴子、还有注定永远是猴子的小猴子，如果在折磨你，实际上真的不是故意的。 其次，那些老猴子、大猴子、还有注定永远是猴子的小猴子，其实根本无法阻碍你进化，除非你愿意被他们阻拦，最终变得跟他们一样。 最后，如果不出意外的话，你这一生无论在哪里、哪个层面上，都必然会遇到无数老猴子、大猴子、还有注定永远是猴子的小猴子。 别害怕，最终你也会制造个小猴子出来，然后就看你自己究竟是什么了。 最后，如果不出意外的话，你这一生无论在哪里、哪个层面上，都必然会遇到无数老猴子、大猴子、还有注定永远是猴子的小猴子。 别害怕，最终你也会制造个小猴子出来，然后就看你自己究竟是什么了。 所以，遇到猴子并不可怕，可怕的是因为憎恨或者厌恶，放弃自我进化的权利和意愿，最终至死只不过是只猴子。另外，千万要记住，即便是猴子，也有非常正确的时候。 某个考试或许很愚蠢，但，某个考试很愚蠢的这个事实，有的时候会证明这样一个结论：如果你连这样愚蠢的考试都无法通过的话，那么愚蠢的不仅仅是考试本身。 1．要热爱考试，因为你喜欢通行证。 2．分辨考试的重要性。 3．提前很久开始准备重要的考试。 4．做题是最好的准备方法。 5．通过做题了解考试的重点、难点。 6．全面补习难点重点，并经常重新审视。 7．教是最好的学习方法。 · 笔记内容栏：尽可能完整而全面地记录演讲内容。 · 提示栏：当你做笔记的时候，让提示栏保留空白。演讲过后，简化你的笔记使之成为简明提示以供日后记忆、回顾和消化（思考）之用。 · 概要：用一个或两个句子总结你的每页笔记。 自我奋斗，勤奋努力，都是天经地义的事情，即便有些人（注意，只不过是“有些人”）貌似不需要奋斗，甚至懒惰到令人嫉妒的地步，也一样可以享受其他人即便奋斗、努力，甚至挣扎都不可能获得的一切。但这些少数人的存在并不能说明全部问题。 这种资源上的分布自然地“不均匀”，理解上貌似简单易懂，但古今中外都有很多的人拒绝理解，拒绝接受。他们甚至拒绝使用“不均匀”这个词，而是用“不公平”来取而代之。历史上，有无数次战争、无数次掠夺，本质上来看，只不过是因为把“均匀”理解成“公平”造成的。把“不均匀”理解成“不公平”，就可以理直气壮地打着“正义”的旗号为所欲为。 资源原本就是有限的，经济学上的措辞是“资源稀缺”（Scarcity）。在整体上资源稀缺的前提下，“资源并非均匀分布”体现在每个人 资源原本就是有限的，经济学上的措辞是“资源稀缺”（Scarcity）。在整体上资源稀缺的前提下，“资源并非均匀分布”体现在每个人身上，直接的结果是“绝大多数人都觉得自己拥有的不够多”。在我们生存的这个世界里，资源稀缺是客观现实，也恰恰因此，人们的主观愿望肯定不可能全部被满足。 资源原本就是有限的，经济学上的措辞是“资源稀缺”（Scarcity）。在整体上资源稀缺的前提下，“资源并非均匀分布”体现在每个人身上，直接的结果是“绝大多数人都觉得自己拥有的不够多”。在我们生存的这个世界里，资源稀缺是客观现实，也恰恰因此，人们的主观愿望肯定不可能全部被满足。 理解这种现象貌似并不困难，但是，清楚地理解之后平静地接受，就没那么容易了。到今天，也随处可见那些无法接受的人。对那些无法接受现实的人来讲， 资源原本就是有限的，经济学上的措辞是“资源稀缺”（Scarcity）。在整体上资源稀缺的前提下，“资源并非均匀分布”体现在每个人身上，直接的结果是“绝大多数人都觉得自己拥有的不够多”。在我们生存的这个世界里，资源稀缺是客观现实，也恰恰因此，人们的主观愿望肯定不可能全部被满足。 理解这种现象貌似并不困难，但是，清楚地理解之后平静地接受，就没那么容易了。到今天，也随处可见那些无法接受的人。对那些无法接受现实的人来讲，其实只剩下了一个选择，即，逃避。 人人都能成功，你是否相信 极度成功与极度失败的人都会非常的少, 近似服从正态分布, 因此, 即便有正确的方法, 也不一定能够成功。人人都能成功是个伪命题。 成功的定义——“成功学”的核心缺陷 关注于那些无需比较就可以获得的成功。 心理健康的人不仅应该有能力看清真实的世界，还应该有足够的能力感知真实的自我。 个案分析——“成功学”的方法缺陷 市面上大多数的成功学书籍可能存在的问题: 以偏概全, 单向成立, 逻辑混乱。 努力是应当的，无须强调 努力是应当的, 无需强调。 “我是独一无二的”——最浪费时间的错觉 自以为独一无二, 在生物学的角度上是正确的。但从心智发展的意义上来讲, 没有任何意义, 有时甚至会被用来当作逃避的借口。 但是，如果他们说的，你就当作全部；他们没有说的，你就当作没有，那你就跟寓言中那个掩耳盗铃的家伙属于同一个智商水平了。 在并非特殊的极端情况下，我们任何人都没有权利要求另外一个人讲述全部细节。但是，如果他们说的，你就当作全部；他们没有说的，你就当作没有，那你就跟寓言中那个掩耳盗铃的家伙属于同一个智商水平了。 一方面，大多数人一生都不会醒悟“平平淡淡才是真”的道理，另外一方面也有很多人在潜意识里希望所有的成功者都是通过艰苦奋斗才获得成功的，因为他们潜意识里觉得只有这样才能解释自己的“不成功”，才能更为自然地接受自己的“不成功”。他们潜意识里的想法如果表达出来可能是这样的：“看吧，成功多难呀？要付出那么多代价才可以呢！我还没付出那么多代价，没成功很正常么……”事实上，对那些渴望成功而又尚未成功的人来说，成功人士的“苦大仇深、血泪斑斑的经历”是一种多大的安慰啊！ 有些时候，“成功者”的经验根本没有用，因为那些经验根本就是错误的，但关键在于他们自己可能也并不知道。 有一个很实用的建议是：不妨反其道而行之——努力从失败者身上汲取经验。 不要试图从成功者身上找经验, 而要从失败者身上找原因。 留心成功者说的话 所以说，从理性角度出发，所谓我们能体会的运气，只不过是因小概率事件发生而产生的感受而已。尽管概率有些时候是可以计算出来的，但肯定不是你能控制的。欲望尽管并不总是可以 所以说，从理性角度出发，所谓我们能体会的运气，只不过是因小概率事件发生而产生的感受而已。尽管概率有些时候是可以计算出来的，但肯定不是你能控制的。欲望尽管并不总是可以被满足，却是你自己能控制，甚至可能完全控制的。浪费时间、虚度年华的人，有个共同的特征——他们拼命想控制自己完全不能控制的，却在自己真正能掌控的地方彻底失控。 不要相信“运气”，更不要相信“机不可失，时不再来” 好运气发生在你身上，你当然应该非常开心；坏运气降临在你身上，你应该平静接受——无论怎样你都要继续生活，当然就还要继续面对你不能控制的事物。其实，这是苏轼早就总结过的生活态度：“骤然临之而不惊，无故加之而不怒。” 所以，可以想象，资源多的人更喜欢，也更可能，与另外一个资源数量同样多或者资源质量对等的人进行交换。因为，在这种情况下，“公平交易”更容易产生。 而反过来，这些被公认为优秀的人，事实上往往并不“低调”，也并不“平易近人”。这并不是他们故意的。他们无意去惹恼身边那些在他们看来“平庸”的人，只不过无形中他们有这样的体会——“与这些人交流，沟通成本太高……”除非有一天，这些人终于意识到自己应该保护自己，因为有些误解根本没机会解释。于是，他们开始“谦虚”，他们学会“低调”，他们显得“平易近人”。 好多年前，我注意到 而反过来，这些被公认为优秀的人，事实上往往并不“低调”，也并不“平易近人”。这并不是他们故意的。他们无意去惹恼身边那些在他们看来“平庸”的人，只不过无形中他们有这样的体会——“与这些人交流，沟通成本太高……”除非有一天，这些人终于意识到自己应该保护自己，因为有些误解根本没机会解释。于是，他们开始“谦虚”，他们学会“低调”，他 而反过来，这些被公认为优秀的人，事实上往往并不“低调”，也并不“平易近人”。这并不是他们故意的。他们无意去惹恼身边那些在他们看来“平庸”的人，只不过无形中他们有这样的体会——“与这些人交流，沟通成本太高……”除非有一天，这些人终于意识到自己应该保护自己，因为有些误解根本没机会解释。于是，他们开始“谦虚”，他们学会“低调”，他们显得“平易近人”。 当一个人身边都是优秀的人的时候，没有人求他帮忙——因为身边这些优秀的人几乎无一 当一个人身边都是优秀的人的时候，没有人求他帮忙——因为身边这些优秀的人几乎无一例外都以耽误别人的时间为耻，同时，这些人恰好是因为遇到问题能够解决才被认为是优秀的。 专心打造自己，把自己打造成一个优秀的人，一个有用的人，一个独立的人，比什么都重要。 专心打造自己，把自己打造成一个优秀的人，一个有用的人，一个独立的人，比什么都重要。打造自己，就等于打造人脉——如果人脉真的像他们说的那么重要。事实上，我总觉得关于人脉导致成功的传说其实非常虚幻，只不过是不明真相的人臆造出来的幻象罢了。 善于与人交往也是一种需要学习，并且也需要耗费大量时间实践的技能。我只是提醒你，别高估自己，误以为自己有足够的时间可以妥善地处理好与身边所有人的关系。 事实上，真正的关心最终只有一个表现：为之心甘情愿地花费时间，哪怕“浪费”时间。 事实上，真正的关心最终只有一个表现：为之心甘情愿地花费时间，哪怕“浪费”时间。 这也很容易理解。因为，当你把时间花费到一个人身上的时候，相当于在他身上倾注了你生命的一段——哪管最终的结果如何，反正，那个人那件事都成了你生命中的一部分，不管最后你喜欢还是不喜欢。每个人的时间都是有限的。所以最终，“真正的好朋友”谁都只有几个而已。 · 专心做可以提升自己的事情，学习并拥有更多更好的技能，成为一个值得交往的人。 · 学会独善其身，以不给他人制造麻烦为美德，用你的独立赢得尊重。 · 除非有特殊原因，应该尽量回避那些连在物质生活上都不能独善其身的人；那些精神生活上不能独善其身的，就更应该回避了——尽管甄别起来比较困难。 · 真正关心一个朋友的意思是说，你情愿在他身上花费甚至浪费更多的时间。 · 记住，一个人的幸福程度，往往取决于他多大程度上可以脱离对外部世界的依附。 打造人脉不如打造自己 人脉没有你想象中的那么重要, 让自己成为一个更优秀的人, 是结交优秀人脉的前提。 很多人拒绝学习，本质上来看，就是在拒绝做人——因为几乎只有人类才有能力有机会“终生学习”。 我见过很多“拒绝学习”的人。我曾经多次尝试劝我的一个朋友花20分钟学习一下批处理命令，未果——他拒绝的理由是：现在谁还用Dos啊？早就是Windows时代了！ 作者在这里的观点有失偏颇, 时间有限, 并不是每一项技能都值得花时间学习 学会起码一种技能很重要，无论它多简单，多没什么大不了，学会它总是可以让习得者了解到习得之后与之前的大不同。 最节省时间的方法：学习 当初不肯花几分钟时间学习的人, 他们不知道为了要多花多少时间弥补曾经的拒绝学习。 每个人专注的能力是不一样的。少数情况下，一个人可以专注到无以复加的地步。看看那些几天几夜打麻将的人，或者看看那 每个人专注的能力是不一样的。少数情况下，一个人可以专注到无以复加的地步。看看那些几天几夜打麻将的人，或者看看那些每天除了吸毒什么都不做的人，抑或那些长时间坐在电脑面前打游戏的人就知道了。不过，这些都是负面的例子。通过前面讲的那些道理，你可以知道这些人本质上应该是被自己大脑控制的人，而不是那些控制自己大脑的人。某种意义上，我们不得不说，这样的人心智发育不是很健全，因为他们太容易满足于并仅仅满足于简单的感官刺激，而很少甚至无法感知那些需要通过复杂的劳动才可以获得的那种心灵上的愉悦。 其实，受到奖励之后，原本有两个选择：a．再次来过；b．见好就收。有意思的是，绝大多数人会自动忽视第二个选项（这就是“庞兹骗局”生生不息的根本原因）。受到惩罚之后，同样有两个选项：a．从此碰都不碰那件事情；b．挣扎着找一个出路。同样好玩的是，这次绝大多数人还是会自动忽视第二个选项。 这就解释了为什么那么多的人总是幻想有什么“速成”的方法——因为，他们前期松懈了太 这就解释了为什么那么多的人总是幻想有什么“速成”的方法——因为，他们前期松懈了太久，现在突然发现时间不够了，最后期限马上就到了，所以，只有寄希望于“速成”方法的存在。如果你曾经有过类似的幻觉，别奇怪，大多数人都是这样的。这几乎是所有人的本性。而这种本性使得人们不得不疲于奔命，于是，速食业也间接因此蓬勃发展； 在一个所有人都匆匆忙忙的世界里，想放慢节奏实在是一件非常难的事情。但是，应该记住的是：凡是 在一个所有人都匆匆忙忙的世界里，想放慢节奏实在是一件非常难的事情。但是，应该记住的是：凡是值得做的事情，都值得慢慢做——做很久很久。 其次，尽量不要减少与家庭成员和亲属交流的时间。 最后，最好不要放弃你的社交时间。 首先，不要盲目地试图减少睡眠时间。 临时抱佛脚的人却不知道自己早已陷入了荒谬的漩涡，每天忙忙碌碌而又实际上碌碌无为地度过，直至最终结果出现都不知道自己究竟错在哪里；只能在许多年后，对自己说：“年轻的时候，我也努力过……” 过去已经过去，无法更改。未来却可以更改——通过改变我们今天的起始条件。当我们真正运用心智的力量认真而又正确地反思我们生活中遇到的所有的尴尬时，最终会发现其中的大多数肯定是因为过去曾经做错过什么，或有意，或无意。那么，为了将来的正确，我们今天就要做到尽量不出错。今天任何一个小错误，都有可能在将来被无穷地放大。这样的认知，几乎会改变我们的一切。生活就是选择，而所谓的选择，只不过是一个人所拥有的观念对之衡量后的结果。一个人所拥有的观念，说穿了，就是心智力量的最终体现。"},{"title":"标签","date":"2018-01-08T07:20:45.000Z","updated":"2018-01-15T06:13:30.000Z","comments":true,"path":"tags/index.html","permalink":"kabibi.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"深度学习之四《卷积神经网络》笔记(2)","slug":"深度学习之四《卷积神经网络》笔记-2","date":"2018-01-18T09:23:51.000Z","updated":"2018-01-21T08:19:45.181Z","comments":true,"path":"2018/01/18/深度学习之四《卷积神经网络》笔记-2/","link":"","permalink":"kabibi.github.io/2018/01/18/深度学习之四《卷积神经网络》笔记-2/","excerpt":"本文为 Andrew NG 的深度学习课程四《卷积神经网络》第2周的对应学习笔记。","text":"本文为 Andrew NG 的深度学习课程四《卷积神经网络》第2周的对应学习笔记。 Why look at case studies 本周课程将主要介绍几个典型的CNN案例。通过对具体CNN模型及案例的研究，来帮助我们理解知识并训练实际的模型。 典型的CNN模型包括： LeNet-5 AlexNet VGG除了这些性能良好的CNN模型之外，我们还会介绍 Residual Network（ResNet）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。 另外，还会介绍Inception Neural Network。 Classic Network LeNet-5LeNet-5模型是第一个成功应用于数字识别问题的卷积神经网络。在MNIST数据中，它的准确率达到大约99.2%。典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是CONV layer-&gt;POOL layer-&gt;CONV layer-&gt;POOL layer-&gt;FC layer-&gt;FC layer-&gt;OUTPUT layer，即 \\hat y。下图所示的是一个数字识别的LeNet-5的模型结构： 该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。 AlexNetAlexNet模型是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示： AlexNet模型与LeNet-5模型类似，只是要复杂一些，总共包含了大约6千万个参数。同样可以根据实际情况使用激活函数ReLU。原作者还提到了一种优化技巧，叫做Local Response Normalization(LRN)。 而在实际应用中，LRN的效果并不突出。 VGG-16VGG-16模型更加复杂一些，一般情况下，其CONV layer和POOL layer设置如下： CONV = 3x3 filters, s = 1, same MAX-POOL = 2x2, s = 2 VGG-16结构如下所示: VGG-16的参数多达1亿3千万。 Residual Networks（ResNets） What’s ResNets?我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为Residual Networks(ResNets)。Residual Networks由许多隔层相连的神经元子模块组成，我们称之为Residual block。单个Residual block的结构如下图所示： 上图中红色部分就是skip connection，直接建立 a^{[l]} 与 a^{[l+2]} 之间的隔层联系。相应的表达式如下： a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$a^{[l]}$直接隔层与下一层的线性输出相连，与 z^{[l+2]} 共同通过激活函数（ReLU）输出a^{[l+2]} 。 该模型由Kaiming He, Xiangyu Zhang, Shaoqing Ren和Jian Sun共同提出。由多个Residual block组成的神经网络就是Residual Network。实验表明，这种模型结构对于训练非常深的神经网络，效果很好。另外，为了便于区分，我们把非Residual Networks称为Plain Network。 Residual Network的结构如上图所示。 Residual Network VS. Plain Network与Plain Network相比，Residual Network能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸。从下面两张图的对比中可以看出，随着神经网络层数增加，Plain Network实际性能会变差，training error甚至会变大。然而，Residual Network的训练效果却很好，training error一直呈下降趋势。 Why ResNets Work? Why work?下面用个例子来解释为什么ResNets能够训练更深层的神经网络。 如上图所示，输入x经过很多层神经网络后输出 a^{[l]}， a^{[l]} 经过一个Residual block输出 a^{[l+2]}。a^{[l+2]}的表达式为： a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=g(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})输入x经过Big NN后，若 W^{[l+2]}\\approx0，b^{[l+2]}\\approx0，则有： a^{[l+2]}=g(a^{[l]})=ReLU(a^{[l]})=a^{[l]}\\ \\ \\ \\ when\\ a^{[l]}\\geq0可以看出，即使发生了梯度消失，W^{[l+2]}\\approx0，b^{[l+2]}\\approx0，也能直接建立 a^{[l+2]}与a^{[l]}的线性关系，且a^{[l+2]}=a^{[l]} ，这其实就是identity function。a^{[l]} 直接连到 a^{[l+2]} ，从效果来说，相当于直接忽略了 a^{[l]} 之后的这两层神经层。这样， 看似很深的神经网络，其实由于许多Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。而且从性能上来说，这两层额外的Residual blocks也不会降低Big NN的性能。 当然，如果Residual blocks确实能训练得到非线性关系，那么也会忽略short cut，跟Plain Network起到同样的效果。 维度不同如何解决？有一点需要注意的是，如果Residual blocks中 a^{[l]} 和 a^{[l+2]}的维度不同，通常可以引入矩阵W_s，与 a^{[l]} 相乘，使得 W_s*a^{[l]} 的维度与 a^{[l+2]} 一致。参数矩阵 W_s 有来两种方法得到： 一种是将 W_s 作为学习参数，通过模型训练得到； 另一种是固定 W_s 值（类似单位矩阵），不需要训练， W_s 与 a^{[l]} 的乘积仅仅使得 a^{[l]} 截断或者补零。 这两种方法都可行。 下图所示的是CNN中ResNets的结构： ResNets同类型层之间，例如CONV layers，大多使用same类型，保持维度相同。如果是不同类型层之间的连接，例如CONV layer与POOL layer之间，如果维度不同，则引入矩阵 W_s。 Network in Network and 1×1 convolutions Min Lin, Qiang Chen等人提出了一种新的CNN结构，即1x1 Convolutions，也称Networks in Networks。这种结构的特点是滤波器算子filter的维度为1x1。对于单个filter，1x1的维度，意味着卷积操作等同于乘积操作。 那么，对于多个filters，1x1 Convolutions的作用实际上类似全连接层的神经网络结构。效果等同于Plain Network中 a^{[l]} 到 a^{[l+1]}的过程。这点还是比较好理解的。 1x1 Convolutions可以用来缩减输入图片的通道数目。方法如下图所示： Inception Network Motivation What’s Inception Network?如下图所示: 我们可以使用不同尺寸的filters进行same convolutions，或者进行max-pool。只要保证输出是28×28即可, 然后将不同layer的结果堆叠起来，得到最后的结果。 Convolution layer: 不同filter size的filters，比如1×1的filter(不需要padding), 或者3×3的fileter（需要padding）等等。 Max-Pool Layer: 我们甚至可以使用MAX-POOL layer. Inception Network 的优势使用Inception Network而非传统的单一尺寸和功能的filter不同，Inception Network使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块, 可以大大地提高性能。 The Problem of Computational CostInception Network在提升性能的同时，会带来计算量大的问题。例如下面这个例子： 此CONV layer需要的计算量为：28x28x32x5x5x192=120 million。可以看出但这一层的计算量都是很大的。为此，我们可以引入1x1 Convolutions来减少其计算量，结构如下图所示： 通常我们把该1x1 Convolution称为“瓶颈层”（bottleneck layer）。引入bottleneck layer之后，总共需要的计算量为：28x28x16x192+28x28x32x5x5x16=12.4 million。明显地，虽然多引入了1x1 Convolution层，但是总共的计算量减少了近90%，效果还是非常明显的。由此可见，1x1 Convolutions还可以有效减少CONV layer的计算量。 Summary总结一下：在构建神经网络的时候，不想决定Pooling layer的filter size大小，那么inception module就是最好的选择，我们可以使用各种类型的filters，只需要把输出连接起来。之后我们降到计算代价问题，我们可以使用1×1 ConV Layer构建bottleneck layer，从而大大降低计算量，通常，只要构建合理的bottleneck layer，既可以显著缩小表示层的规模，又不会降低网络的性能。 Inception Network 上一节我们使用1x1 Convolution来减少Inception Network计算量大的问题。引入1x1 Convolution后的Inception module如下图所示： 多个Inception modules组成Inception Network，效果如下图所示： 上述Inception Network除了由许多Inception modules组成之外，值得一提的是网络中间隐藏层也可以作为输出层Softmax，有利于防止发生过拟合。 Using Open-Source Implementation 介绍Github的使用，略。 Transfer learning 什么是transfer learning？Transfer learning: 深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如我们已经训练好一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学习方法被称为迁移学习（Transfer Learning）。 如果需要构建新模型的样本数量较少，可以只训练输出层的权重系数 $ W^{[L]} $, $ b^{[L]} $ 而保持其他层的权重参数保持不变。这种方法的优点在于做法比较简单，而且不需要很多的数据。而如果训练样本很多的话，那么也可以重新训练网络的权重系数，这样可以使得模型更加精确。采用那种方法通常根据数据量而定。 如果重新训练所有权重系数，初始 $ W^{[l]} $, $ b^{[l]} $ 由之前的模型训练得到，这一过程称为pre-training。 之后，不断调试、优化 $ W^{[l]} $,$b^{[l]}$ 的过程称为fine-tuning。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头。 transfer learning 的应用场合总的来说，transfer learning的应用场合主要为： Task A and B have the same input x. You have a lot more data for Task A than Task B. Low level features from A could be helpful for learning B. Why transfer learning works？一个从Quora上摘抄的回答：You have learned to differentiate between rotten potato and fresh potato. But now with your learning experience, you have to differentiate between rotten tomato and fresh tomato. During learning of rotten potato, you have extracted knowledge like- if any vegetable is rotten, liquid will come out from there body. So, if you can use this knowledge in your job of rotten tomato identification, you are using transfer learning technique. Data Augmentation 有几种data augmentation的方法： 常用的Data Augmentation方法是对已有的样本集进行Mirroring和Random Cropping。 另一种Data Augmentation的方法是color shifting。color shifting就是对图片的RGB通道数值进行随意增加或者减少，改变图片色调。 除了随意改变RGB通道数值外，还可以更有针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，对主要的通道颜色进行增加或减少，可以采用高斯扰动做法。这样也能增加有效的样本数量。具体的PCA color augmentation做法可以查阅AlexNet的相关论文。 最后提一下，在构建大型神经网络的时候，data augmentation和training可以由两个不同的线程来进行。 State of Computer Vision 神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的hand-engineering，对已有data进行处理，比如上一节介绍的data augmentation。模型算法也会相对要复杂一些。如果data很多，可以构建深层神经网络，不需要太多的hand-engineering，模型算法也就相对简单一些。 值得一提的是hand-engineering是一项非常重要也比较困难的工作。很多时候，hand-engineering对模型训练效果影响很大，特别是在数据量不多的情况下。 在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能： Ensembling: Train several networks independently and average their outputs. Multi-crop at test time: Run classifier on multiple versions of test images and average results. 最后，我们还要灵活使用开源代码： Use archittectures of networks published in the literature Use open source implementations if possible Use pretrained models and fine-tune on your dataset","categories":[{"name":"深度学习","slug":"深度学习","permalink":"kabibi.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"kabibi.github.io/tags/深度学习/"}]},{"title":"强化学习之Policy Gradient","slug":"强化学习之Policy-Gradient","date":"2018-01-14T12:23:20.000Z","updated":"2018-01-21T13:35:33.181Z","comments":true,"path":"2018/01/14/强化学习之Policy-Gradient/","link":"","permalink":"kabibi.github.io/2018/01/14/强化学习之Policy-Gradient/","excerpt":"Policy Gradient 是一种 Model-free 的方法。","text":"Policy Gradient 是一种 Model-free 的方法。 Policy Gradient 方法简介Policy-based Approach: Learning an Actor首先，我们知道对于深度学习问题，有三步： Define a set of functions. Evaluate the goodness of function. Pick the best function. 下面，我将从这三个角度分别进行说明。 Define a set of functions在Policy Gradient（PG）方法中，我们把Neural Network（NN）作为actor，这个actor就是我们的策略。这个NN的输入是机器所观察到的环境（比如游戏画面的pixels），而输出层的每一个neuron都对应于一个actor，neuron的值则是这个采取这个action的概率。如下图所示： 采取NN而不是传统的look-up table的好处在于： 具有更好的泛化能力，能够举一反三。 Evaluate the goodness of actor既然我们使用NN定义模型，那么如何评估这个模型的好坏呢？ 我们需要一个公式去量化这个模型的好坏。在supervised learning中，我们经常用Cross Entropy来评估。在PG中，我们这样定义： 首先，actor就是我们的策略，定义actor为： \\pi_\\theta(s)。其次，比如说用RL解决游戏问题，那么使用\\pi_\\theta(s) 打游戏，会得到一系列轨迹 \\tau = \\{s_1, a_1, r_1, s_2, a_2, r_2, ...,s_T, a_T, r_T\\}。我们定义 R_\\theta = \\sum_{t=1}^T r_t 由于我们没有任何关于环境的知识，所以actor是一个随机性策略：给定某个状态和相应的动作，输出的是一个概率值，即 \\pi_\\theta(s,a) 输出的是概率。因此 R_\\theta 其实是一个随机变量，我们的目的在于最大化其期望值 \\bar{R}_\\theta，因此，\\bar{R}_\\theta 是我们寻找的能够评估 actor 好坏的指标。 Pick the best function.既然我们已经找到了能够评估actor的指标了，那么下一步就是优化 \\overline{R}_\\theta，这需要一些数学上的推导。 我们定义一个episode就是一个轨迹 \\tau，其中\\tau = \\{s_1, a_1, r_1, s_2, a_2, r_2, ...,s_T, a_T, r_T\\}，如果使用actor打N场游戏，就会有N条轨迹，某一条轨迹都有一定的概率被取样出来，我们假定这个概率被参数\\theta控制，因此我们可以有下面的公式： \\overline{R}_\\theta = \\sum_\\tau R(\\tau)p(\\tau|\\theta) \\approx \\frac{1}{N} \\sum_{n=1}^N R(\\tau_n)接下来就是使用Policy Ascent 求解 \\overline{R}_\\theta 的最大值。 Gradient Ascent 优化Gradient Ascent的基本思路就是： \\theta_1\\leftarrow\\theta_0 + \\eta\\nabla\\overline{R}_{\\theta_0}\\theta_2\\leftarrow\\theta_1 + \\eta\\nabla\\overline{R}_{\\theta_1}...现在我们的目标是优化\\overline{R}_{\\theta}，即优化下式： \\theta^* = \\underset{\\theta}{argmax} \\ \\overline{R}_{\\theta}其中 \\theta = \\{ w_1, w_2, ..., b_1, ...\\} \\nabla \\overline{R_\\theta} = \\begin{bmatrix} \\partial \\overline{R}_\\theta / \\partial w_1 \\\\ \\partial \\overline{R}_\\theta / \\partial w_2 \\\\ \\vdots \\\\ \\partial \\overline{R}_\\theta / \\partial b_1 \\\\ \\vdots \\\\ \\end{bmatrix}由于: \\overline{R}_{\\theta} = \\sum_{n=1}^N R(\\tau_n)p(\\tau_n|\\theta)故： \\begin{align} \\nabla\\overline{R}_{\\theta} &= \\sum_{n=1}^N R(\\tau_n) \\nabla p(\\tau_n|\\theta) \\\\ &= \\sum_{n=1}^N R(\\tau_n)p(\\tau_n|\\theta) \\frac{\\nabla p(\\tau|\\theta)}{p(\\tau|\\theta)} \\\\ &= \\sum_{n=1}^N R(\\tau_n)p(\\tau_n|\\theta) \\nabla logp(\\tau|\\theta) \\qquad (注：p(\\tau|\\theta) = p(s_1)\\prod_{t=1}^T p(a_t|s_t,\\theta)p(r_t, s_{t+1}|s_t, a_t)) \\\\ &= \\sum_{n=1}^N [R(\\tau_n)p(\\tau_n|\\theta) \\nabla (log(p(s_1) \\prod_{t=1}^T p(a_t|s_t,\\theta)p(r_t, s_{t+1}|s_t, a_t)))] \\\\ &= \\sum_{n=1}^N [R(\\tau_n)p(\\tau_n|\\theta) \\nabla (logp(s_1) + \\sum_{t=1}^T (logp(a_t|s_t,\\theta) + logp(r_t, s_{t+1}|s_t, a_t))] \\\\ &= \\sum_{n=1}^N [R(\\tau_n)p(\\tau_n|\\theta) \\sum_{t=1}^T \\nabla logp(a_t|s_t,\\theta)] \\\\ &= \\sum_{n=1}^N \\sum_{t=1}^T R(\\tau_n)p(\\tau_n|\\theta) \\nabla logp(a_t|s_t,\\theta) \\\\ &\\approx \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^T R(\\tau_n) \\nabla log p(a_t|s_t, \\theta) \\end{align}有几点需要注意： 我们考虑的是累计奖赏R(\\tau^{(n)}) 而不是立即的奖赏r_t^{(n)} 如果在轨迹\\tau^{(n)} 中，机器在s_t^{(n)}采取动作a_t^{(n)}： 如果R(\\tau^{n}) > 0，调节\\theta 以增加 p(a_t^{(n)}|s_t^{(n)}) 如果R(\\tau^{n}) < 0，调节\\theta 以减少 p(a_t^{(n)}|s_t^{(n)}) Policy Gradient实现给定参数 \\theta，我们先用这个参数模拟出N条轨迹： \\tau^{(1)}: (s_1^{(1)}, a_1^{(1)}), (s_2^{(1)}, a_2^{(1)}), ...得到R(\\tau^{(1)}) \\\\ \\tau^{(2)}: (s_1^{(2)}, a_1^{(2)}), (s_2^{(2)}, a_2^{(2)}), ...得到R(\\tau^{(2)}) \\\\ ... \\\\ \\tau^{(N)}: (s_1^{(N)}, a_1^{(N)}), (s_2^{(N)}, a_2^{(N)}), ...得到R(\\tau^{(N)})然后通过 \\nabla\\overline{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^T R(\\tau_n) \\nabla log p(a_t|s_t, \\theta)计算出 \\nabla\\overline{R}_{\\theta}，再通过下式更新 \\theta： \\theta \\leftarrow \\theta + \\eta \\nabla \\overline{R}_{\\theta}接着再用新的 \\theta 再模拟出新的N条轨迹，用这N条轨迹得到新的\\theta，如此反复进行下去～ Some intuitionIntuition 1可是如何理解\\nabla\\overline{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^T R(\\tau_n) \\nabla log p(a_t|s_t, \\theta) 中的\\nabla log p(a_t|s_t, \\theta) 这个东西呢？ 我们可以通过 Classification 问题来理解，如下图所示。 给定状态s，假如a=”left”，那么NN的输出层y_i应该与 \\hat{y_i}越接近越好，我们通过Cross Entropy量化两者的接近程度并最小化，可以表示为： Minimize \\quad -\\sum_{i=1}^3 \\hat{y_i} log \\ y_i最小化上式也即最大化： Maximize \\quad log \\ y_i = log \\ P(\"left\"|s)于是我们对上式求梯度，就得到了 \\nabla log p(a_t|s_t, \\theta)。 Intuition 2但是\\nabla log p(a_t|s_t, \\theta)前的R(\\tau_n) 又当如何理解呢？这是在说每一笔训练数据都具有R(\\tau^{(n)})的权重，在实现的时候，比如R(\\tau^{(n)}) = 2，那我们需要把这笔数据用两次。 Add a baseline通常我们在实现时会在R(\\tau^{(n)}) 之后减去一个bias，即： \\nabla\\overline{R}_{\\theta} \\approx \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^T (R(\\tau_n)-b) \\nabla log p(a_t|s_t, \\theta)这是因为在理论上，每一个action都能够被取样到，然而在实现上却不一定，没有被取样到的action的概率就会下降，尽管采取这个action的奖赏很大。 Value-based Approach: Learning an Critic","categories":[{"name":"强化学习","slug":"强化学习","permalink":"kabibi.github.io/categories/强化学习/"}],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"kabibi.github.io/tags/强化学习/"}]},{"title":"深度学习之四《卷积神经网络》笔记(1)","slug":"深度学习之四《卷积神经网络》笔记-1","date":"2018-01-13T05:36:09.000Z","updated":"2018-01-21T10:20:00.806Z","comments":true,"path":"2018/01/13/深度学习之四《卷积神经网络》笔记-1/","link":"","permalink":"kabibi.github.io/2018/01/13/深度学习之四《卷积神经网络》笔记-1/","excerpt":"课程链接：http://mooc.study.163.com/course/2001281004 本文为 Andrew NG 的深度学习课程三第1周的对应学习笔记。","text":"课程链接：http://mooc.study.163.com/course/2001281004 本文为 Andrew NG 的深度学习课程三第1周的对应学习笔记。 Computer Vision 机器视觉（Computer Vision）是深度学习应用的主要方向之一。一般的CV问题包括以下三类： Image Classification(识别一个图片是否是猫) Object detection(检测图片中是否有车，是否有行人) Neural Style Transfer(可以把你的照片转换成某一种画的风格) 使用传统神经网络处理机器视觉的一个主要问题是输入层维度很大。例如一张64x64x3的图片，神经网络输入层的维度为12288。如果图片尺寸较大，例如一张1000x1000x3的图片，神经网络输入层的维度将达到3百万，使得网络权重W非常庞大。这样会造成两个后果，一是神经网络结构复杂，数据量相对不够，容易出现过拟合；二是所需内存、计算量较大。解决这一问题的方法就是使用卷积神经网络（CNN）。 Edge detection example 本节中介绍如何检测图片的边缘，图片的边缘分为两部分，一种是垂直边缘（vertical edges），另一种是水平边缘（horizontal edges）。 图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为6x6，滤波器filter尺寸为3x3，卷积后的图片尺寸为4x4，得到结果如下： 顺便提一下，* 表示卷积操作。python中，卷积用conv_forward()表示；tensorflow中，卷积用tf.nn.conv2d()表示；keras中，卷积用Conv2D()表示。 下图对应一个垂直边缘检测的例子，卷积的结果中，中间有一大片白色，说明在原图像中检测到了垂直的边缘，至于为什么白色区域很宽，是因为原图的尺寸太小。 More Edge Detection 图片边缘的渐变方式图片的渐变方式可以有两种，一种是由明变暗，另一种是由暗变明。实际应用中，这两种方式并不影响边缘检测的结果，可以对输出结果取绝对值，得到同样的结果。 几种滤波器算子垂直滤波器算子和水平滤波器算子如下所示： Sobel filter 和 Scharr filter如下图所示, 这两种滤波器的特点是增加图片中心区域的权重。 在深度学习中，如果我们想检测图片的各种边缘特征，而不仅限于垂直边缘和水平边缘，那么filter的数值一般需要通过模型训练得到，类似于标准神经网络中的权重W一样由梯度下降算法反复迭代求得。CNN的主要目的就是计算出这些filter的数值。确定得到了这些filter后，CNN浅层网络也就实现了对图片所有边缘特征的检测。 Padding 按照我们上面讲的图片卷积，如果原始图片尺寸为n*n，filter尺寸为f*f，则卷积后的图片尺寸为(n-f+1) * (n-f+1)，注意f一般为奇数。这样会带来两个问题： 卷积运算后，输出图片尺寸缩小 原始图片边缘信息对输出贡献得少，输出图片丢失边缘信息 为了解决图片缩小的问题，可以使用padding方法，即把原始图片尺寸进行扩展，扩展区域补零，用p来表示每个方向扩展的宽度。 经过padding之后，原始图片尺寸为(n+2p) x (n+2p)，filter尺寸为f x f，则卷积后的图片尺寸为(n+2p-f+1) x (n+2p-f+1)。若要保证卷积前后图片尺寸不变，则p应满足： p = \\frac{f-1}{2}没有padding操作， p=0 ，我们称之为Valid convolutions；有padding操作， $ p=\\frac{f-1}{2} $ ，我们称之为Same convolutions。 The main benefits of padding are the following: It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the “same” convolution, in which the height/width is exactly preserved after one layer. It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image. Strided Convolutions Stride表示filter在原图中每次移动的步长。之前我们默认stride=1。用s表示stride长度，p表示padding长度，如果原始图片尺寸为n x n，filter尺寸为f x f，则卷积后的图片尺寸为： \\lfloor{\\frac{n+2p-1}{s}+1}\\rfloor * \\lfloor{\\frac{n+2p-1}{s}+1}\\rfloorcross-correlation 与 convolutions 的区别真正的卷积运算会先将filter绕其中心旋转180度，然后再将旋转后的filter在原始图片上进行滑动计算。而相关系数的计算过程则不会对filter进行旋转，而是直接在原始图片上进行滑动计算。filter旋转如下所示： 其实，目前为止我们介绍的CNN卷积实际上计算的是相关系数，而不是数学意义上的卷积。但是，为了简化计算，我们一般把CNN中的这种“相关系数”就称作卷积运算。之所以可以这么等效，是因为滤波器算子一般是水平或垂直对称的，180度旋转影响不大；而且最终滤波器算子需要通过CNN网络梯度下降算法计算得到，旋转部分可以看作是包含在CNN模型算法中。总的来说，忽略旋转运算可以大大提高CNN网络运算速度，而且不影响模型性能。 卷积运算服从分配律： (A*B)*C = A*(B*C) Convolutions over volumns 3通道卷积运算对于3通道的RGB图片，其对应的滤波器算子同样也是3通道的。例如一个图片是6 x 6 x 3，分别表示图片的高度（height）、宽度（weight）和通道（#channel）。 3通道图片的卷积运算与单通道图片的卷积运算基本一致。过程是将每个单通道（R，G，B）与对应的filter进行卷积运算求和，然后再将3通道的和相加，得到输出图片的一个像素值。 不同通道的滤波算子可以不相同。例如R通道filter实现垂直边缘检测，G和B通道不进行边缘检测，全部置零，或者将R，G，B三通道filter全部设置为水平边缘检测。 实现多个卷积运算为了进行多个卷积运算，实现更多边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。这样，不同滤波器组卷积得到不同的输出，个数由滤波器组决定。 若输入图片的尺寸为n*n*n_c ，filter尺寸为f*f*n_c ，则卷积后的图片尺寸为(n-f+1) * (n-f+1) * n_c' 。其中， n_c 为图片通道数目， n_c' 为滤波器组个数。 One Layer of a Convolutional Network Structure of single layer CNN单层CNN结构如下图所示: 相比之前的卷积神经网络，CNN的单层结构多了激活函数Relu和偏移量b。整个过程与标准的神经网络单层结构非常类似。其中卷积操作相当于：Z^{[l]} = W^{l} A^{[l-1]} + b^{[l]}；滤波器组相当于W^{[l]}； 前面提到过，传统神经网络一个缺点就是：输入维度很高。而CNN的一个优点就是参数数目只由滤波器组决定，因此可以大大减少参数的个数。让我们看看怎么回事。 我们来计算一下上图中参数的数目：每个滤波器组有3x3x3=27个参数，还有1个偏移量b，则每个滤波器组有27+1=28个参数，两个滤波器组总共包含28x2=56个参数。我们发现，选定滤波器组后，参数数目与输入图片尺寸无关。所以，就不存在由于图片尺寸过大，造成参数过多的情况。例如一张1000x1000x3的图片，标准神经网络输入层的维度将达到3百万，而在CNN中，参数数目只由滤波器组决定，数目相对来说要少得多，这是CNN的优势之一。 Notations for CNN$ f^{[l]} $ = filter size$ p^{[l]} $ = padding$ s^{[l]} $ = stride$ n_c^{[l]} $ = number of filters 输入维度为： n_H^{[l-1]} * n_W^{[l-1]} * n_c^{[l-1]} 每个滤波器组维度为： f^{[l]} * f^{[l]} * n_c^{[l-1]} 权重维度为： f^{[l]} * f^{[l]} * n_c^{[l-1]} * n_c^{[l]} 偏置维度为：1 * 1 * 1 * n_c^{[l]} 输出维度为： n_H^{[l]} * n_W^{[l]} * n_c^{[l]} 其中，n_H^{[l]}=\\lfloor \\frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\rfloor, n_W^{[l]}=\\lfloor \\frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \\rfloor。如果有m个样本，进行向量化运算，相应的输出维度为：m * n_H^{[l]} * n_W^{[l]} * n_c^{[l]}。 Simple Convolutional Network Example 下图是一个典型的CNN结构示意图： 需要注意的是， a^{[3]} 的维度是 7 x 7 x 40，将 a^{[3]} 排列成1列，维度为1960 x 1，然后连接最后一级输出层。输出层可以是一个神经元，即二元分类（logistic）；也可以是多个神经元，即多元分类（softmax）。最后得到预测输出 \\hat y。 值得一提的是，随着CNN层数增加， n_H^{[l]} 和 n_W^{[l]} 一般逐渐减小，而 n_c^{[l]} 一般逐渐增大。 CNN有三种类型的layer： Convolution层（CONV） Pooling层（POOL） Fully connected层（FC） CONV最为常见也最重要，关于POOL和FC我们之后再介绍。 Pooling Layers Max poolingPooling layers是CNN中用来减小尺寸，提高运算速度的，同样能减小noise影响，让各特征更具有健壮性。 Pooling layers的做法比convolution layers简单许多，没有卷积运算，仅仅是在滤波器算子滑动区域内取最大值，即max pooling，这是最常用的做法。注意，超参数p很少在pooling layers中使用。 而且，max pooling需要的超参数仅为滤波器尺寸f和滤波器步进长度s，没有其他参数需要模型训练得到，计算量很小。如果是多个通道，那么就每个通道单独进行max pooling操作。 Average poolingaverage pooling就是在滤波器算子滑动区域计算平均值。实际应用中，max pooling比average pooling更为常用。 CNN Example 下面介绍一个简单的数字识别的CNN例子： 图中，CNN层后面紧接一个POOL层，CONV1和POOL1构成第一层，CONV2和POOL2构成第二层。特别注意的是FC3和FC4为全连接层FC，它跟标准的神经网络结构一致。最后的输出层（softmax）由10个神经元构成。构成了最终的输出：对应从0~9一共十个数字。 整个网络各层的尺寸和参数如下表格所示： 下面的视频演示了CNN的工作过程： Why Convolutions 相比标准神经网络，CNN的优势之一就是参数数目要少得多。参数数目少的原因有两个： 参数共享：一个特征检测器（例如垂直边缘检测）对图片某块区域有用，同时也可能作用在图片其它区域。 连接的稀疏性：因为滤波器算子尺寸限制，每一层的每个输出只与输入部分区域内有关。 除此之外，由于CNN参数数目较小，所需的训练样本就相对较少，从而一定程度上不容易发生过拟合现象。而且，CNN比较擅长捕捉区域位置偏移。也就是说CNN进行物体检测时，不太受物体所处图片位置的影响，增加检测的准确性和系统的健壮性。 Summary 本节中，我们首先提出CV领域的几个问题，分别是Image Classification，Object Detection以及 Neural Style Transfer;而这些问题都是与图像有关的，而如果图像的尺寸很大，那么需要解决的一个问题就是神经网络结构复杂与计算量大的这样一些问题。而我们之后将要提出的CNN，就是能够解决上述几个问题的方法。 接下来我们又讨论了边缘检测，边缘检测分为 vertical edge detection 和 horizontal edge detection。图片的边缘检测可以通过将图片与相应的 filter 进行卷积实现。接着又说明了图片的渐变方式并不影响边缘检测的结果。另外如果我们想检测图片的其他边缘特征，那么filter的数值一般需要通过模型训练得到。 另外一个问题就是，如果我们按照上面的方法进行卷积运算，卷积后图片的尺寸会缩小，而且图片边缘信息对输出的贡献很小，因此为了解决这个问题，我们需要对图片进行 padding，也就是在原图的周围打上一圈空白像素。如果要保证卷积后的图片尺寸，要保证 p=\\frac{f-1}{2}。 之前我们讨论的都是 filter 在原图中每次移动的步长都是1，然而实际的步长是可以改变的。另外需要注意的是，之前我们讨论的卷积操作并不是数学意义上的卷积。 随后，我们又讲到如何在RGB图像上进行卷积运算，以及同时进行多个卷积运算。为了在RGB图像上进行卷积操作，我们的filter也必须是3通道的，卷积后得到的是二维的像素值；而如果我们设置了滤波器组，那么把原图与每一个filter卷积后得到的结果堆叠起来，最终得到的就是在RGB图像上同时进行多个卷积运算的结果。如下图所示： 接着，我们给出了单层CNN的基本结构，在CNN中，滤波器组相当于权重W，这也是为什么参数数量被大量减少。于是我们给出了一个典型CNN结果示意图，除此之外，我们提出CNN有3种类型的layer： Convolution Layer Pooling Layer Fully Connected Layer 其中，Pooling Layer中经常采用的是Max Pooling，Pooling Layer能够有效减小尺寸，过滤噪声。 最后，我们介绍了一个简单数字识别的CNN例子并且给出了使用卷积神经网络的一些原因。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"kabibi.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"kabibi.github.io/tags/深度学习/"}]},{"title":"深度学习之三《结构化机器学习项目》笔记(2)","slug":"深度学习之三《结构化机器学习项目》笔记-2","date":"2018-01-12T02:38:14.000Z","updated":"2018-01-19T09:42:39.811Z","comments":true,"path":"2018/01/12/深度学习之三《结构化机器学习项目》笔记-2/","link":"","permalink":"kabibi.github.io/2018/01/12/深度学习之三《结构化机器学习项目》笔记-2/","excerpt":"本文为 Andrew NG 的深度学习课程三第二周的对应学习笔记。","text":"本文为 Andrew NG 的深度学习课程三第二周的对应学习笔记。 Carrying out error analysis 进行error analysis让我们改进模型更加有针对性，从而提高效率。我们把分类错误的样子拿出来进行分析，并使用表格进行分析，通过统计各个种类出现错误的比例来进行针对性的提升，这样可以更有针对性地提高模型的性能，而避免出现花了很长时间盲目地对某一方面进行改进，比如不断地收集更多的数据，结果却发现对性能的提升没有什么卵用。 这种error analysis虽然简单，但是能够避免花费大量的时间精力去做一些对提高模型性能收效甚微的工作，让我们专注解决影响模型正确率的最主要问题。 Cleaning up incorrectly labeled data 出现incorrectly labeled data的应对策略 Training set 中出现标记错误的数据：如果label标记错误是随机出现的，那么深度学习算法对其包容性是很强的，一般可以直接忽略。但是如果是系统性错误(Systematic errors), 则会对深度学习算法造成影响。 dev/test set 中出现标记错误的数据：利用上节内容介绍的error analysis，统计dev sets中所有分类错误的样本中incorrectly labeled data所占的比例。根据该比例的大小，决定是修正还是忽略incorrectly labeled data， 举例说明考虑下面的情况： Overall dev set error: 10% Errors due incorrect labels: 0.6% Errors due to other causes: 9.4% 这种情况下显然incorrect labels不是造成错误的主要原因，因此可以忽略incorrectly labeled data。 考虑另一种情况： Overall dev set error: 2% Errors due incorrect labels: 0.6% Errors due to other causes: 1.4% 由于incorrect labels的比例占到了30%，这种情况下不应该忽略incorrectly labeled data，需要手动修正。 修正incorrect dev/test set data 的建议 Apply same process to your dev and test sets to make sure they continue to come from the same distribution Consider examining examples your algorithm got right as well as ones it got wrong Train and dev/test data may now come from slightly different distributions Build your first system quickly then iterate 如何构建一个机器学习应用模型? Andrew 给出的建议是先快速构建第一个简单模型，然后再反复迭代优化。 Set up dev/test set and metric Build initial system quickly Use Bias/Variance analysis &amp; Error analysis to prioritize next steps Training and testing on different distribution 当train set与dev/test set不来自同一个分布的时候，我们应该如何解决这一问题，构建准确的机器学习模型呢？ 以猫类识别为例，train set来自于网络下载（webpages），图片比较清晰；dev/test set来自用户手机拍摄（mobile app），图片比较模糊。假如train set的大小为200000，而dev/test set的大小为10000，显然train set要远远大于dev/test set。 虽然dev/test set质量不高，但是模型最终主要应用在对这些模糊的照片的处理上。面对train set与dev/test set分布不同的情况，有两种解决方法。 第一种方法是将train set和dev/test set完全混合，然后在随机选择一部分作为train set，另一部分作为dev/test set。例如，混合210000例样本，然后随机选择205000例样本作为train set，2500例作为dev set，2500例作为test set。这种做法的优点是实现train set和dev/test set分布一致，缺点是dev/test set中webpages图片所占的比重比mobile app图片大得多。例如dev set包含2500例样本，大约有2381例来自webpages，只有119例来自mobile app。这样，dev set的算法模型对比验证，仍然主要由webpages决定，实际应用的mobile app图片所占比重很小，达不到验证效果。因此，这种方法并不是很好。 第二种方法是将原来的train set和一部分dev/test set组合当成train set，剩下的dev/test set分别作为dev set和test set。例如，200000例webpages图片和5000例mobile app图片组合成train set，剩下的2500例mobile app图片作为dev set，2500例mobile app图片作为test set。其关键在于dev/test set全部来自于mobile app。这样保证了验证集最接近实际应用场合。这种方法较为常用，而且性能表现比较好。 Bias and Variance with mismatched data distributions 根据human-level error，training error以及dev-error可以判断到底是出现了bias还是variance。 human-level error和training error相差较大，则出现了bias training error和dev error相差较大，则出现了variance但是上述结论的前提是：你的training error和dev error必须来自同一个分布。否则无法判断到底是否真的出现了variance。比如training error和dev error相差较大的原因可能是由于两种数据集来自不同的分布。 那么如果training set和dev set分布不一致，应该怎么办呢？一个方法就是使用：train-dev set——“Same distribution as training set, but not used for training.” 这样我们就有了training error，train-dev error以及dev error。 training error和train-dev error的差值表现variance有多大（因为显然两者来自同一分布）。 train-dev error和dev error的差值表现两者是否来自统一分布（data mismatch）。 Addressing data mismatch 关于如何解决train set和dev/test set 样本分布不一致的问题，可以从以下角度： Carry out manual error analysis to try to understand difference between training and dev/test sets. Make training data more similar; or collect more data similar to dev/test sets.为了让train set与dev/test set类似，我们可以使用人工数据合成的方法（artificial data synthesis）。例如说话人识别问题，实际应用场合（dev/test set）是包含背景噪声的，而训练样本train set很可能没有背景噪声。为了让train set与dev/test set分布一致，我们可以在train set上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。但是，需要注意的是，我们不能给每段语音都增加同一段背景噪声，这样会出现对背景噪音的过拟合，效果不佳。这就是人工数据合成需要注意的地方。 Transfer learning 什么是transfer learning？Transfer learning: 深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如我们已经训练好一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学习方法被称为迁移学习（Transfer Learning）。 如果需要构建新模型的样本数量较少，可以只训练输出层的权重系数 $ W^{[L]} $, $ b^{[L]} $ 而保持其他层的权重参数保持不变。这种方法的优点在于做法比较简单，而且不需要很多的数据。而如果训练样本很多的话，那么也可以重新训练网络的权重系数，这样可以使得模型更加精确。采用那种方法通常根据数据量而定。 如果重新训练所有权重系数，初始 $ W^{[l]} $, $ b^{[l]} $ 由之前的模型训练得到，这一过程称为pre-training。 之后，不断调试、优化 $ W^{[l]} $,$b^{[l]}$ 的过程称为fine-tuning。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头。 transfer learning 的应用场合总的来说，transfer learning的应用场合主要为： Task A and B have the same input x. You have a lot more data for Task A than Task B. Low level features from A could be helpful for learning B. Why transfer learning works？一个从Quora上摘抄的回答：You have learned to differentiate between rotten potato and fresh potato. But now with your learning experience, you have to differentiate between rotten tomato and fresh tomato. During learning of rotten potato, you have extracted knowledge like- if any vegetable is rotten, liquid will come out from there body. So, if you can use this knowledge in your job of rotten tomato identification, you are using transfer learning technique. Multi-task learning 什么是 MTL?顾名思义，多任务学习（multi-task learning）就是构建神经网络同时执行多个任务。这跟二元分类或者多元分类都不同，多任务学习类似将多个神经网络融合在一起，用一个网络模型来实现多种分类效果。如果有C个，那么输出y的维度是(C,1)。例如汽车自动驾驶中，需要实现的多任务为行人、车辆、交通标志和信号灯。如果检测出汽车和交通标志，则y为： y = \\quad \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}MTL 与 softmax 的不同Multi-task learning与Softmax regression的区别在于Softmax regression是single label的，即输出向量y只有一个元素为1；而Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1。 MTL 的应用场合多任务学习是使用单个神经网络模型来实现多个任务。实际上，也可以分别构建多个神经网络来实现。但是，如果各个任务之间是相似问题（例如都是图片类别检测），则可以使用多任务学习模型。另外，多任务学习中，可能存在训练样本Y某些label空白的情况，这并不影响多任务模型的训练。 总体来说，多任务学习的应用场合主要包括三点： Training on a set of tasks that could benefit from having shared lower-level features. Usually: Amount of data you have for each task is quite similar. Can train a big enough neural network to do well on all the tasks. What is end-to-end deep learning 什么是 end-to-end deep learning?端到端（end-to-end）深度学习就是将所有不同阶段的数据处理系统或学习系统模块组合在一起，用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。 如果训练样本足够大，神经网络模型足够复杂，那么end-to-end模型性能比传统机器学习分块模型更好。实际上，end-to-end让神经网络模型内部去自我训练模型特征，自我调节，增加了模型整体契合度。end-to-end deep learning 在机器翻译领域工作的很好，因为可以获得大量的数据。 Whether to use end-to-end deep learning 优点： Let the data speak: 让机器自己去根据数据学习更没有加入人为的干扰。 Less hand-designing of components needed: 简化设计流程。 缺点： May need large amount of data Excludes potentially useful hand-designed: 可能无法把有用的人类知识注入到算法中。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"kabibi.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"kabibi.github.io/tags/深度学习/"}]},{"title":"《小狗钱钱》读书笔记","slug":"小狗钱钱-读书笔记","date":"2018-01-04T01:40:22.000Z","updated":"2018-01-16T05:52:38.051Z","comments":true,"path":"2018/01/04/小狗钱钱-读书笔记/","link":"","permalink":"kabibi.github.io/2018/01/04/小狗钱钱-读书笔记/","excerpt":"这本书讲的主要是一只名叫钱钱的小狗，它和其他的小狗有一点点的不同，他会说人话。并且借着会说话的本事带领本书的主人公——一个12岁的小女孩以及她全家共同致富的故事。","text":"这本书讲的主要是一只名叫钱钱的小狗，它和其他的小狗有一点点的不同，他会说人话。并且借着会说话的本事带领本书的主人公——一个12岁的小女孩以及她全家共同致富的故事。 本书从名字上看起来像一个略显幼稚的童话故事，但实际上这是一本通俗易懂，并且将难以理解的大道理融汇在故事中的理财书籍。 要说我去读这本书的原由，前段时间听到过一句话，“你不理财，财不理你”。于是便心生理财的念头，在知乎上偶然看到有人推荐这本书，于是便借之一读，于是这本书便成了我第一本理财书籍。这本书文字简单通俗易懂，前前后后读完花了3～4天的时间，读完也算略有收获，这本书也算开启了我的理财人生，希望能够透过这本书，开始我的开挂人生。 现在想总结我从这本书中得到的几个收获： 理财之前，首先要建立起自信 实现财务自由是每个人都要尽快实现的目标。财务自由是今天每个人都可能实现的目标，当然这需要我们拥有追求自己真正想要的生活的勇气。正如一句名言所说：“并非困难使我们放弃，而是因为我们放弃，才显得如此困难。”以前准备考研的时候，看过朱伟老师的视频，他给我最大的印象不是那些黄段子，而是一个很有个性和想法的人。他在视频里经常提到，“先有财务自由，再有人生自由”。一个人真正自由成熟的时候，是能够自己挣钱养家，许多人喜欢啃老，即便有能力实现财务自由。他们究是一个肢体上的成年人，思想上的巨婴。 知道自己想要什么，不要模糊了事。经常和朋友一起吃饭，问道吃什么，我说随便，可是到了饭店才发现桌上都是自己不喜欢吃的东西，于是度过了一个难过的中午。谈到自己想要怎样的一个女朋友，我说随便，是女的就行，结果别人介绍的又让自己又难过了一整天。我相信这些不是我自己一个人才有过的经历。大多数人不知道想要什么，他们只知道，自己还想要更多的东西。书中提供的一种方法就是：写一张愿望清单，每天都把这张写着自己的愿望的单子从头到尾看一遍，它会不断地提醒你自己想得到什么，那么你就会密切关注一切可以帮助你实现这些愿望的机遇了。 当你决定做一件事情的时候，你必须在72小时之内完成，否则你很可能永远不会再做了。 小狗钱钱鼓励吉娅建立自信的方法就是：建立成功笔记。把自己曾经做过的每一件成功的事情都写在成功日记中，当你觉得自卑的时候，就不断地翻开这本日记，你就会获得莫大的成就感。 理财中的一些建议 一个金蛋的故事：有一个农家小伙儿，他每天的愿望就是从鹅笼里拣一个鹅蛋当早饭。有一天，他竟然在鹅笼里发现了一只金蛋。一开始他当然不敢相信自己的眼睛。他想，也许是有人在捉弄他。为了谨慎起见，他把金蛋拿去让金匠辨别，可是金匠向他保证说，这只金蛋完完全全是金子铸成的。于是，这个农家小伙儿就卖了这只金蛋，举行了一个盛大的庆祝会。第二天清晨，他起了一个大早，赶到鹅笼里一看，那里果真又放着一个金蛋，这样的情况延续了好几天。可是这个小伙儿是一个贪婪的人，他抱怨自己的鹅，因为鹅没办法向他解释是怎么下出这个蛋的，否则他也许可以制造金蛋。他还气呼呼的想，这只懒惰的鹅每天至少应该下两只金蛋。他觉得这样的速度太慢了。他的怒火越来越大，最后，他终于怒不可遏地把鹅揪出笼子劈成了两半。自那以后，他再也得不到金蛋了。 基金和股票是很好的理财方式，保险起见，选择基金。挑选基金时的注意事项： 1. 基金应该至少有10年历史。假如它在这么长时间内一直有丰厚的利润，那我们可以认为，未来它也会运作良好。2. 应该选择大型的跨国股票基金。这种基金在世界各地购买股票，以此分散风险，所以十分安全。 3. 对基金的走势图进行比较。我们应该观察在过去10年间哪些基金的年终利润最好。 保证基金收益的安全至少要存入五年或者更多，所以只能把不是马上要用的钱投进基金里。 72定理：用72除以投资的年收益率的百分比，得出的数就是这笔钱翻倍所要的年数。 不要在意暂时的亏损，不要忘了你有五年甚至十年的时间，在这么多年之后，它一定会保证你的收益。暂时的亏损只是“冬天”的来临，春天是早晚都要来的，所以不要急于卖出。 在暂时亏损，也就是相对于之前的涨幅有所跌落的时候，或许是买进的时机，但不要投入大笔钱，因为没有人能够确定这就是最低的时候。 不要把鸡蛋放入同一个篮子”，同样道理，不要把你的鹅放入同一家基金或者股票。 笔记摘抄所有人都希望自己变得富有一些，只是有些人的这一愿望更为强烈，而有些人却假装自己满足于现在的生活。事实上，大多数人都希望自己更幸福、更成功，也想拥有更多的钱。 不是努力去挖掘故事情节之后隐藏的生活原则，而是试图模仿书中主人公获取成功的举动。成功的故事很少有精彩的翻版，但是故事中包含的道理却可以帮助我们在遭遇困境时找到最佳的出路。 一旦丰厚的资产开始流动，其速度之快和数量之大都可能让你吃惊：以前它们都藏到哪里去了？ 并非困难使我们放弃，而是因为我们放弃，才显得如此困难。 财务自由是今天每个人都可能实现的目标，当然这需要我们拥有追求自己真正想要的生活的勇气。正如一句名言所说：“并非困难使我们放弃，而是因为我们放弃，才显得如此困难。” 大多数人并不清楚自己想要的是什么，他们只知道，自己想得到更多的东西。你可以把自己的生活想象成一家很大的邮购公司。如果你给一家邮购公司写信说‘请给我寄一些好东西来’，你肯定什么都得不到。我们的愿望也是一样。我们必须确切地知道自己心里渴望的是什么才行。” 从现在开始，你必须每天都把这张写着自己的愿望的单子从头到尾看一遍，它会不断地提醒你自己想得到什么，那么你就会密切关注一切可以帮助你实现这些愿望的机遇了。 好奇是好的，但是绝不能让好奇阻碍你做事。太多的人做事犹豫不决，就是因为他们觉得没有完全弄懂这件事。真正付诸实践要比纯粹的思考有用多了。 如果你只是抱着试试看的心态，那么你只会以失败告终，你会一事无成。‘尝试’纯粹是一种借口，你还没有做，就已经给自己想好了退路。不能试验，你只有两个选择—做或者不做。” 我们对一件事投入的精力越多，成功的可能性也越大。可是大多数人把精力放在自己并不喜欢的事情上，而不去想象自己希望得到的东西。 有一只海鸥曾经对我说过：‘在你展翅飞翔之前，你就必须相信自己能到达目的地。’你必须设想自己已经拥有了这些东西，这样你的一个小愿望才会变成一种强烈的渴望。你想象得越多，你的愿望就越强烈，那么你就会开始寻找机会来实现自己的梦想。 知道如何去实现并不是目前最重要的事情。最重要的是，你真的有这样的愿望，否则你一遇到困难就会放弃了。 不去寻找机会的人，最多不过是在走运的时候捡到天上掉下来的馅饼。 首先，在遇到困难的时候，仍然要坚持自己的想法。一切正常的时候，每个人都能做到这一点。只有当真正的困难出现时才能见分晓。只有少数人能坚定不移地贯彻自己的计划。那些非常成功的人，甚至有能力在他们最困难的时候作出最杰出的表现。 你看，有成千上万件事情可能让你分心，因此你每天应该在固定的时间里，有规律地做这些事情。 当你得到带拿破仑散步的工作时，你的喜悦让你忘记了该做的事。你看，有成千上万件事情可能让你分心，因此你每天应该在固定的时间里，有规律地做这些事情。” 当你决定做一件事情的时候，你必须在72小时之内完成，否则你很可能永远不会再做了。 你干的活最多只值报酬的一半，另一半报酬源于你的想法和实施这个想法的勇气。 我生命中出现了最美好的东西，是因为我做了原本不敢做的。 最珍贵的礼物是我们自己争取来的。克服了丢面子的恐惧，世界就会向你敞开大门！” 可是还有一点，你不能在困难面前逃跑。困难、犯错误和丢面子引起的恐惧已经破坏了无数人的生活。” 说：“恐惧总是出现在我们设想事情会如何不顺的时候。我们对恐惧总是出现在我们设想事情会如何不顺的时候。我们对失败的可能性想得越多，就会越害怕。而当你看着自己的成功日记时，你就会注意到那些成功的事情，自然而然也就会想到应该怎样去做。” 当你朝着积极的目标去思考的时候，就不会心生畏惧。 挑选基金时的注意事项： 1. 基金应该至少有10年历史。假如它在这么长时间内一直有丰厚的利润，那我们可以认为，未来它也会运作良好。2. 应该选择大型的跨国股票基金。这种基金在世界各地购买股票，以此分散风险，所以十分安全。 3. 对基金的走势图进行比较。我们应该观察在过去10年间哪些基金的年终利润最好。 不要为失去的东西而忧伤，而要对拥有它的时光心存感激。 从小开始赚钱的人拥有对依赖思想的抵抗力，不会乐意让别人来养活自己。而且自己赚钱的人不会成为不断膨胀的消费欲望的奴隶。从事商业活动有助于人们以理智的态度珍惜和使用有限的资源。 我们推崇一种聪明的、简朴的生活方式。也就是说，宁愿购买一件一流产品，也不要不停地买许多的二流产品。而且，不要仅仅因为一件产品的外观不再时髦而新产品正在流行，就不断追逐新鲜的东西。 分析企业家的传记，有一点很引人注目。绝大多数敢于投身商业活动的人，在童年时期就已经拥有了一些自己的想法，具有某种“怪癖”，而且善于思考。他们之中的大多数人有过小规模的经营管理的实践经验，参与过小型的经济活动。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"kabibi.github.io/categories/读书笔记/"}],"tags":[{"name":"读书","slug":"读书","permalink":"kabibi.github.io/tags/读书/"}]},{"title":"Python实现的图片按时间归类","slug":"Python实现的图片按时间归类","date":"2016-04-28T06:10:00.000Z","updated":"2018-01-15T06:13:30.000Z","comments":true,"path":"2016/04/28/Python实现的图片按时间归类/","link":"","permalink":"kabibi.github.io/2016/04/28/Python实现的图片按时间归类/","excerpt":"來到臺灣之後四處遊學，照片真是越來越多了，怎麼也有幾千張了吧。再加上和前幾年的照片混雜在一起，有時候想看看5年前自己長什麼蠢樣，無奈卻找不到照片～","text":"來到臺灣之後四處遊學，照片真是越來越多了，怎麼也有幾千張了吧。再加上和前幾年的照片混雜在一起，有時候想看看5年前自己長什麼蠢樣，無奈卻找不到照片～ 於是就像不如用Python寫一個腳本，把所有的照片按照時間進行分類。 一、背景安裝exifread模塊我們將會使用到exifread模塊，在Ubuntu下，它的安裝命令如下： 1sudo pip install exifread 什麼是Exif?現在我們用手機/數碼相機進行拍照，一般可以直接從文件的名稱就可以看出這張照片的拍照時間，但是如果我們修改了文件的名稱，是不是就不知道這張照片什麼時候拍的了？ 顯然不是。通常我們查看照片的屬性，就可以直接看到Width, Height, Maker, Flash, ISO, White balance, Focal length這些參數信息。這些信息顯然一定存儲在照片的某個位置。我們實現對照片進行分類，就要通過某種方法，獲得這些信息。 這些信息就存儲在Exif中，至於Exif是什麼，我們來看Wiki的解釋。 可交换图像文件格式常被简称为Exif（Exchangeable image file format），是专门为数码相机的照片设定的，可以记录数码照片的属性信&gt;息和拍摄数据。 Exif可以附加于JPEG、TIFF、RIFF等文件之中，为其增加有关数码相机&gt;拍摄信息的内容和索引图或图像处理软件的版本信息。 Windows7操作系统具备对Exif的原生支持，通过鼠标右键点击图片打开&gt;菜单，点击属性并切换到详细信息标签下即可直接查看Exif信息。 Exif信息是可以被任意编辑的，因此只有参考的功能。 Exif信息以0xFFE1作为开头标记，后两个字节表示Exif信息的长度。所以Exif信息最大为64 kB，而内部采用TIFF格式。 二、其他的一些小問題編程之前，我們還有一些小問題需要解決～ 1. 如何獲得照片的屬性？1234import exifreadfile = open('1.jpg')tags = exifread.process_file(file)print tags.keys() 輸出如下： [‘EXIF ApertureValue’, ‘Image ExifOffset’, ‘EXIF ComponentsConfiguration’, ‘EXIF FlashPixVersion’, ‘EXIF ColorSpace’, ‘EXIF ExifVersion’, ‘Thumbnail ResolutionUnit’, ‘Thumbnail YResolution’, ‘Interoperability InteroperabilityVersion’, ‘GPS GPSDate’, ‘EXIF DateTimeOriginal’, ‘Image YCbCrPositioning’, ‘EXIF InteroperabilityOffset’, ‘Thumbnail JPEGInterchangeFormat’, ‘EXIF ExifImageLength’, ‘Image ResolutionUnit’, ‘EXIF ExposureTime’, ‘Thumbnail XResolution’, ‘Image GPSInfo’, ‘Thumbnail JPEGInterchangeFormatLength’, ‘Thumbnail Compression’, ‘EXIF ExifImageWidth’, ‘JPEGThumbnail’, ‘GPS GPSTimeStamp’, ‘EXIF DateTimeDigitized’, ‘EXIF FocalLength’, ‘Image Model’, ‘Image XResolution’, ‘Image Make’, ‘EXIF ISOSpeedRatings’, ‘Image YResolution’, ‘Interoperability InteroperabilityIndex’] 這個list就是照片中所有的參數，他們的值我們都可以通過獲得，比如我們想知道這張照片是什麼時候拍的，就可以下面的代碼獲得： 1print tags['EXIF DateTimeOriginal'] 2. 文件操作複製1shutuil.copy(src, dst) 剪切1shutuil.move(src, dst) 新建目錄1234567os.mkdir(path, mode)# Super-mkdir;# create a leaf directory and all intermediate ones.# Works like mkdir, except that any intermediate path# segment (not just the rightmost) will be created if it does not exist.# This is recursive.os.makedirs(path, mode) 重命名1os.rename(src, dst) 使用os.system實現複製，剪切新建，重命名可以直接調用os提供的函數實現文件的操作，比如下面的： 1234567import osos.system('mkdir /home/code') # 新建文件夾os.system('cp 1.jpg 2.jpg') # 複製os.system('mv 1.jpg 2.jpg') # 重命名os.system('mv 1.jpg /home/2.jpg') # 剪切os.path.isdir('/home') # 判斷是否是目錄os.path.isfile('/home/1.jpg') # 判斷是否是文件 遍歷目錄中的文件1234567# os.walk(top[, topdown=True[, onerror=None[, followlinks=False]]])# root, dirs, files三者的順序不能改變！for root, dirs, files in os.walk(\"/home/vinater/DATA/Pictures/\"): for name in files: # 遍歷文件 print (os.path.join(root, name)) for name in dirs: # 遍歷目錄中的文件 print (os.path.join(root, name)) os.walk返回一個三元組，分別是根目錄root，文件files和文件夾dirs； 分離路徑名和基本名1234567# 第一種方法path = '/home/vinater/DATA/Pictures/background'basename = os.path.basename(path)dirname = os.path.dirname(path)# 第二種方法dirname, basename = os.path.split(path) 分離基本名和擴展名第一種方法直接使用splitext: 1root, extension = os.path.splitext(fname) 第二種方法使用split 123456for root,dirs,files in os.walk('/home/vinater/DATA/Pictures/background'): for name in files: # 分別是前綴名和後綴名 print str.split(name,'.')[0], str.split(name,'.')[1] for name in dirs: print str.split(name,'.')[0], str.split(name,'.')[1] 三、開始編程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# coding: utf-8import exifreadimport osimport shutilimport hashlibclass Photo: def __init__(self, fromPath, toPath): # source path where your photos are self.fromPath = fromPath # destination path where put your photos to self.toPath = toPath # counter self.totalPhotos = 1 self.md5List = [] self.samePhotos = 0 self.picFormats = ['.jpg','.JPG','.jpeg','.JPEG','.png','.PNG'] def getYMD(self, filename): \"\"\" Get year, month, day, hour, minute, second of the image :param filename: file name of the image :return: year, month, day, hour, minute, second \"\"\" file = open(filename) r,e = os.path.split(filename) if e in self.picFormats: md5 = hashlib.md5(open(filename).read()).hexdigest() if(md5 not in self.md5List): self.md5List.append(md5) else: print '!!Find the same files!!' self.samePhotos += 1 return None,None,None,None,None,None # tags is a dictionary stores image's exif information tags = exifread.process_file(file) # if do not find exif information, set default date if len(tags) == 0: date = '1970:01:01' time = '00:00:00' else: # temp has the format \"1970:01:01 00:00:00\" if tags.has_key('EXIF DateTimeOriginal'): temp = str(tags['EXIF DateTimeOriginal']) elif tags.has_key('Image DateTime'): temp = str(tags['Image DateTime']) else: temp = '1970:01:01 00:00:00' date, time = str.split(temp, ' ') year, month, day = date.split(':') hour, minute, second = time.split(':') return year, month, day, hour, minute, second def renameAllFiles(self): \"\"\" Rename all images and move to property location :return: none \"\"\" if os.path.isdir(self.fromPath): # traversal all file in source path for root, dirs, files in os.walk(self.fromPath): for name in files: print root, name if (root[-1] != '/'): filename = str(root) + '/' + str(name) else: filename = str(root) + str(name) # get root and extension name r, extension = os.path.splitext(filename) # get date year, month, day, hour, min, sec = self.getYMD(filename) if year == None: continue # get new file name newFileName = str(year) + '-' + str(month) + '-' + str(day) + '-' + str(hour) + ':' + str( min) + '-' + str(self.totalPhotos) + extension # judge whether the folder exists, if not, make directory save2Path = self.toPath + '/' + year + '/' + year + '-' + month if (not os.path.isdir(save2Path)): os.makedirs(save2Path) # counter plus 1 self.totalPhotos += 1 # move file to property place shutil.move(filename, save2Path + '/' + newFileName) def removeEmptyFolders(self, path): \"\"\" remove empty folders :param path: remove empty folder under path :return: none \"\"\" for root, dirs, files in os.walk(path): for dir in dirs: if os.listdir(root + '/' + dir) == []: os.rmdir(root + '/' + dir) def getMd5(self,filename): return hashlib.md5(open(filename,'rb').read()).hexdigest()src = raw_input('Enter source path:').encode('utf-8')dst = raw_input('Enter destination path:').encode('utf-8')p = Photo(src, dst)p.renameAllFiles()p.removeEmptyFolders(p.fromPath)print 'Total images: %d \\nTotal same images: %d' % (p.totalPhotos, p.samePhotos)","categories":[{"name":"技术杂谈","slug":"技术杂谈","permalink":"kabibi.github.io/categories/技术杂谈/"}],"tags":[{"name":"编程","slug":"编程","permalink":"kabibi.github.io/tags/编程/"},{"name":"Python","slug":"Python","permalink":"kabibi.github.io/tags/Python/"}]}]}